{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PP_MM_BaseModel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rishi15-t/PP-MM/blob/master/PP_MM_BaseModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZyaSvq7Poeg",
        "colab_type": "code",
        "outputId": "a07021ae-a878-4514-a258-9dcdbcd1c791",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daviCRfrPsDK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "dataset = pd.read_pickle('/content/drive/My Drive/dataset/w2v_vgg_embeddings.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lq5KYZXPtiv",
        "colab_type": "code",
        "outputId": "8191fcbd-24d4-401c-e308-28a014a719fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "from torch.nn.functional import softplus\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.optim import AdamW\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from tqdm import tqdm, trange\n",
        "from sklearn import metrics\n",
        "#!pip install transformers\n",
        "#from transformers import get_linear_schedule_with_warmup\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "#!pip install git+https://github.com/uber/pyro.git\n",
        "import pyro\n",
        "from pyro import poutine\n",
        "from pyro.distributions import Normal, Categorical, Laplace\n",
        "from pyro.infer import SVI, Trace_ELBO, TraceEnum_ELBO\n",
        "from pyro.optim import ClippedAdam\n",
        "\n",
        "\n",
        "def Train_Test_Val_Split(data , test_data_fraction = 0.3, val_data_fraction = 0.1) :\n",
        "    \n",
        "  \n",
        "    data_genres_one_hot_encoding = pd.DataFrame.from_items(zip(data['genres'].index, data['genres'].values)).T\n",
        "    Label_names = np.array(['Action', 'Adventure', 'Animation', 'Biography', 'Comedy', 'Crime',\n",
        "       'Documentary', 'Drama', 'Family', 'Fantasy', 'Film-Noir',\n",
        "       'History', 'Horror', 'Music', 'Musical', 'Mystery', 'Romance',\n",
        "       'Sci-Fi', 'Short', 'Sport', 'Thriller', 'War', 'Western'])\n",
        "    data_genres_one_hot_encoding.columns = Label_names\n",
        "    Data_train, Data_test, Labels_train, Labels_test = train_test_split(data, data_genres_one_hot_encoding, test_size = test_data_fraction)\n",
        "\n",
        "    Data_train, Data_val, Labels_train, Labels_val = train_test_split(Data_train, Labels_train, test_size = val_data_fraction)\n",
        "\n",
        "    Data_train = Data_train.reset_index(drop=True)\n",
        "    Data_test = Data_test.reset_index(drop=True)\n",
        "    Data_val = Data_val.reset_index(drop=True)\n",
        "    \n",
        "    Labels_train = torch.tensor(Labels_train.values)\n",
        "    Labels_test = torch.tensor(Labels_test.values)\n",
        "    Labels_val = torch.tensor(Labels_val.values)\n",
        "    \n",
        "    return (Data_train, Data_test, Data_val, Labels_train, Labels_test, Labels_val, Label_names)\n",
        "    \n",
        "Data_train, Data_test, Data_val, Labels_train_tensor, Labels_test_tensor, Labels_val_tensor, Label_names = Train_Test_Val_Split(dataset)\n",
        "\n",
        "Data_train_tensor_text = torch.tensor(Data_train['w2v_embeddings'])\n",
        "Data_test_tensor_text = torch.tensor(Data_test['w2v_embeddings'])\n",
        "Data_val_tensor_text = torch.tensor(Data_val['w2v_embeddings'])\n",
        "\n",
        "Data_train_tensor_image = torch.tensor(Data_train['vgg16_embeddings'])\n",
        "Data_test_tensor_image = torch.tensor(Data_test['vgg16_embeddings'])\n",
        "Data_val_tensor_image = torch.tensor(Data_val['vgg16_embeddings'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: from_items is deprecated. Please use DataFrame.from_dict(dict(items), ...) instead. DataFrame.from_dict(OrderedDict(items)) may be used to preserve the key order.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6TKgAUIPwu0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Neural networks\n",
        "\n",
        "\n",
        "#source: https://github.com/Duncanswilson/maxout-pytorch/blob/master/maxout_pytorch.ipynb\n",
        "class ListModule(object):\n",
        "    def __init__(self, module, prefix, *args):\n",
        "        self.module = module\n",
        "        self.prefix = prefix\n",
        "        self.num_module = 0\n",
        "        for new_module in args:\n",
        "            self.append(new_module)\n",
        "\n",
        "    def append(self, new_module):\n",
        "        if not isinstance(new_module, nn.Module):\n",
        "            raise ValueError('Not a Module')\n",
        "        else:\n",
        "            self.module.add_module(self.prefix + str(self.num_module), new_module)\n",
        "            self.num_module += 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_module\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        if i < 0 or i >= self.num_module:\n",
        "            raise IndexError('Out of bound')\n",
        "        return getattr(self.module, self.prefix + str(i))\n",
        "\n",
        "\n",
        "class Maxout_MLP(nn.Module):\n",
        "    \n",
        "    def __init__(self, hidden_layer_size1, hidden_layer_size2, dropout, num_labels = 23, num_maxout_units=2):\n",
        "        \n",
        "        super(Maxout_MLP, self).__init__()\n",
        "        self.fc1_list = ListModule(self, \"fc1_\")\n",
        "        self.fc2_list = ListModule(self, \"fc2_\")\n",
        "        self.hidden_layer_size1 = hidden_layer_size1\n",
        "        self.hidden_layer_size2 = hidden_layer_size2\n",
        "        for _ in range(num_maxout_units):\n",
        "            self.fc1_list.append(nn.Linear(self.hidden_layer_size1, self.hidden_layer_size2))\n",
        "            self.fc2_list.append(nn.Linear(self.hidden_layer_size2, self.hidden_layer_size2))\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "        self.linear = torch.nn.Linear(self.hidden_layer_size2, num_labels)  #PP_MM_pattern 4\n",
        "\n",
        "        self.bn0 = nn.BatchNorm1d(self.hidden_layer_size1)\n",
        "        self.bn1 = nn.BatchNorm1d(self.hidden_layer_size2)\n",
        "        self.bn2 = nn.BatchNorm1d(self.hidden_layer_size2)\n",
        "\n",
        "    def forward(self, x): \n",
        "        \n",
        "        x = x.view(-1, self.hidden_layer_size1)\n",
        "        x = self.bn0(x)\n",
        "        x = self.maxout(x, self.fc1_list)\n",
        "        x = self.bn1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.maxout(x, self.fc2_list)\n",
        "        x = self.bn2(x)\n",
        "        '''\n",
        "        PP_MM_pattern 4\n",
        "        '''\n",
        "        logits = self.linear(x)  \n",
        "        if(self.training) :     \n",
        "            return logits\n",
        "        else :\n",
        "            output = self.sigmoid(logits)\n",
        "            return output\n",
        "\n",
        "    def maxout(self, x, layer_list):\n",
        "        \n",
        "        max_output = layer_list[0](x)\n",
        "        for _, layer in enumerate(layer_list, start=1):\n",
        "            max_output = torch.max(max_output, layer(x))\n",
        "        return max_output\n",
        "\n",
        "\n",
        "class GMU(nn.Module):\n",
        "\n",
        "    def __init__(self, num_maxout_units = 2, hidden_layer_size = 512, text_embeddings_size = 300, img_embeddings_size = 4096, num_labels = 23, hidden_activation = None, dropout = 0.1):\n",
        "\n",
        "        super(GMU, self).__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.hidden_layer_size = hidden_layer_size\n",
        "\n",
        "        self.linear_h_text = torch.nn.Linear(text_embeddings_size, self.hidden_layer_size)\n",
        "        self.linear_h_image = torch.nn.Linear(img_embeddings_size, self.hidden_layer_size)\n",
        "        self.linear_z = torch.nn.Linear(text_embeddings_size + img_embeddings_size, self.hidden_layer_size)\n",
        "\n",
        "        self.mm0 = nn.Linear(self.hidden_layer_size, self.hidden_layer_size) #PP_MM_pattern 4\n",
        "        self.mm1 = nn.Linear(self.hidden_layer_size, self.hidden_layer_size) #PP_MM_pattern 4\n",
        "\n",
        "        \n",
        "        self.tanh = torch.nn.Tanh()\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        self.bn0 = nn.BatchNorm1d(img_embeddings_size)\n",
        "        self.bn1 = nn.BatchNorm1d(text_embeddings_size)\n",
        "        self.bn2 = nn.BatchNorm1d(text_embeddings_size + img_embeddings_size)\n",
        "\n",
        "    def forward(self, text_embeddings, image_embeddings):\n",
        "        \n",
        "        image_embeddings = self.bn0(image_embeddings)\n",
        "        image_h = self.linear_h_image(image_embeddings)\n",
        "        image_h = self.tanh(image_h)\n",
        "\n",
        "        text_embeddings = self.bn1(text_embeddings)\n",
        "        text_h = self.linear_h_text(text_embeddings)\n",
        "        text_h = self.tanh(text_h)\n",
        "\n",
        "        concat = torch.cat((image_embeddings, text_embeddings), 1)\n",
        "        concat = self.bn2(concat)\n",
        "        z = self.linear_z(concat)\n",
        "        z = self.sigmoid(z)\n",
        "        gmu_output = z*image_h + (1-z)*text_h\n",
        "        \n",
        "        gmu_output = self.mm0(gmu_output)  #PP_MM_pattern 4\n",
        "        gmu_output = self.tanh(gmu_output)  #PP_MM_pattern 4\n",
        "        gmu_output = self.mm1(gmu_output)  #PP_MM_pattern 4\n",
        "        attendout = self.tanh(gmu_output)  #PP_MM_pattern 4\n",
        "\n",
        "        return attendout  #PP_MM_pattern 4   \n",
        "\n",
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_T5rrAgfJE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pyro module defined only over the GMU NN\n",
        "\n",
        "class GMU_Pyro():\n",
        "\n",
        "    def __init__(self, model):\n",
        "\n",
        "        self.model = model \n",
        "    \n",
        "    def gmu_model(self, text_embeddings, image_embeddings, labels):\n",
        "\n",
        "        #pyro.module(\"self.model\", self)\n",
        "        #global x_label\n",
        "\n",
        "        linear_h_text_prior_w = Laplace(loc=torch.ones_like(self.model.linear_h_text.weight), scale=torch.ones_like(self.model.linear_h_text.weight))\n",
        "        linear_h_text_prior_b = Laplace(loc=torch.ones_like(self.model.linear_h_text.bias), scale=torch.ones_like(self.model.linear_h_text.bias))\n",
        "        linear_h_image_prior_w = Laplace(loc=torch.ones_like(self.model.linear_h_image.weight), scale=torch.ones_like(self.model.linear_h_image.weight))\n",
        "        linear_h_image_prior_b = Laplace(loc=torch.ones_like(self.model.linear_h_image.bias), scale=torch.ones_like(self.model.linear_h_image.bias))\n",
        "        linear_z_prior_w = Laplace(loc=torch.ones_like(self.model.linear_z.weight), scale=torch.ones_like(self.model.linear_z.weight))\n",
        "        linear_z_prior_b = Laplace(loc=torch.ones_like(self.model.linear_z.bias), scale=torch.ones_like(self.model.linear_z.bias))\n",
        "\n",
        "        mm0_prior_w = Laplace(loc=torch.ones_like(self.model.mm0.weight), scale=torch.ones_like(self.model.mm0.weight))\n",
        "        mm0_prior_b = Laplace(loc=torch.ones_like(self.model.mm0.bias), scale=torch.ones_like(self.model.mm0.bias))\n",
        "        mm1_prior_w = Laplace(loc=torch.ones_like(self.model.mm1.weight), scale=torch.ones_like(self.model.mm1.weight))\n",
        "        mm1_prior_b = Laplace(loc=torch.ones_like(self.model.mm1.bias), scale=torch.ones_like(self.model.mm1.bias))\n",
        "\n",
        "        priors = {\n",
        "            'linear_h_text.weight': linear_h_text_prior_w, 'linear_h_text.bias': linear_h_text_prior_b,\n",
        "            'linear_h_image.weight': linear_h_image_prior_w, 'linear_h_image.bias': linear_h_image_prior_b,\n",
        "            'linear_z.weight': linear_z_prior_w, 'linear_z.bias': linear_z_prior_b,\n",
        "            'mm0.weight': mm0_prior_w, 'mm0.bias': mm0_prior_b,\n",
        "            'mm1.weight': mm1_prior_w, 'mm1.bias': mm1_prior_b}\n",
        "\n",
        "        lifted_module = pyro.random_module(\"module\", self.model, priors)\n",
        "        lifted_reg_model = lifted_module()\n",
        "\n",
        "        lhat = torch.sigmoid(lifted_reg_model(text_embeddings, image_embeddings))\n",
        "        pyro.sample(\"obs\", Categorical(logits=lhat), obs=labels)\n",
        "\n",
        "    \n",
        "    def gmu_guide(self, text_embeddings, image_embeddings, labels):\n",
        "\n",
        "        #pyro.module(\"self.model\", self)\n",
        "\n",
        "        # linear_h_text\n",
        "        # weight\n",
        "        lwlinear_h_text = torch.empty_like(self.model.linear_h_text.weight)\n",
        "        swlinear_h_text = torch.empty_like(self.model.linear_h_text.weight)\n",
        "        torch.nn.init.normal_(lwlinear_h_text, std=0.01)\n",
        "        torch.nn.init.normal_(swlinear_h_text, std=0.1)\n",
        "        linear_h_text_loc_param_w_l = pyro.param(\"linear_h_text_loc_w_l\", lwlinear_h_text)\n",
        "        linear_h_text_loc_param_w_s = softplus(pyro.param(\"linear_h_text_loc_w_s\", swlinear_h_text))\n",
        "        linear_h_text_prior_w = Laplace(loc=linear_h_text_loc_param_w_l, scale=linear_h_text_loc_param_w_s)\n",
        "        # bias\n",
        "        lblinear_h_text = torch.empty_like(self.model.linear_h_text.bias)\n",
        "        sblinear_h_text = torch.empty_like(self.model.linear_h_text.bias)\n",
        "        torch.nn.init.normal_(lblinear_h_text, std=0.01)\n",
        "        torch.nn.init.normal_(sblinear_h_text, std=0.1)\n",
        "        linear_h_text_loc_param_b_l = pyro.param(\"linear_h_text_loc_b_l\", lblinear_h_text)\n",
        "        linear_h_text_loc_param_b_s = softplus(pyro.param(\"linear_h_text_loc_b_s\", sblinear_h_text))\n",
        "        linear_h_text_prior_b = Laplace(loc=linear_h_text_loc_param_b_l, scale=linear_h_text_loc_param_b_s)\n",
        "\n",
        "        # linear_h_image\n",
        "        # weight\n",
        "        lwlinear_h_image = torch.empty_like(self.model.linear_h_image.weight)\n",
        "        swlinear_h_image = torch.empty_like(self.model.linear_h_image.weight)\n",
        "        torch.nn.init.normal_(lwlinear_h_image, std=0.01)\n",
        "        torch.nn.init.normal_(swlinear_h_image, std=0.1)\n",
        "        linear_h_image_loc_param_w_l = pyro.param(\"linear_h_image_loc_w_l\", lwlinear_h_image)\n",
        "        linear_h_image_loc_param_w_s = softplus(pyro.param(\"linear_h_image_loc_w_s\", swlinear_h_image))\n",
        "        linear_h_image_prior_w = Laplace(loc=linear_h_image_loc_param_w_l, scale=linear_h_image_loc_param_w_s)\n",
        "        # bias\n",
        "        lblinear_h_image = torch.empty_like(self.model.linear_h_image.bias)\n",
        "        sblinear_h_image = torch.empty_like(self.model.linear_h_image.bias)\n",
        "        torch.nn.init.normal_(lblinear_h_image, std=0.01)\n",
        "        torch.nn.init.normal_(sblinear_h_image, std=0.1)\n",
        "        linear_h_image_loc_param_b_l = pyro.param(\"linear_h_image_loc_b_l\", lblinear_h_image)\n",
        "        linear_h_image_loc_param_b_s = softplus(pyro.param(\"linear_h_image_loc_b_s\", sblinear_h_image))\n",
        "        linear_h_image_prior_b = Laplace(loc=linear_h_image_loc_param_b_l, scale=linear_h_image_loc_param_b_s)\n",
        "\n",
        "        # linear_z\n",
        "        # weight\n",
        "        lwlinear_z = torch.empty_like(self.model.linear_z.weight)\n",
        "        swlinear_z = torch.empty_like(self.model.linear_z.weight)\n",
        "        torch.nn.init.normal_(lwlinear_z, std=0.01)\n",
        "        torch.nn.init.normal_(swlinear_z, std=0.1)\n",
        "        linear_z_loc_param_w_l = pyro.param(\"linear_z_loc_w_l\", lwlinear_z)\n",
        "        linear_z_loc_param_w_s = softplus(pyro.param(\"linear_z_loc_w_s\", swlinear_z))\n",
        "        linear_z_prior_w = Laplace(loc=linear_z_loc_param_w_l, scale=linear_z_loc_param_w_s)\n",
        "        # bias\n",
        "        lblinear_z = torch.empty_like(self.model.linear_z.bias)\n",
        "        sblinear_z = torch.empty_like(self.model.linear_z.bias)\n",
        "        torch.nn.init.normal_(lblinear_z, std=0.01)\n",
        "        torch.nn.init.normal_(sblinear_z, std=0.1)\n",
        "        linear_z_loc_param_b_l = pyro.param(\"linear_z_loc_b_l\", lblinear_z)\n",
        "        linear_z_loc_param_b_s = softplus(pyro.param(\"linear_z_loc_b_s\", sblinear_z))\n",
        "        linear_z_prior_b = Laplace(loc=linear_z_loc_param_b_l, scale=linear_z_loc_param_b_s)\n",
        "\n",
        "        # mm0\n",
        "        # weight\n",
        "        lwmm0 = torch.empty_like(self.model.mm0.weight)\n",
        "        swmm0 = torch.empty_like(self.model.mm0.weight)\n",
        "        torch.nn.init.normal_(lwmm0, std=0.01)\n",
        "        torch.nn.init.normal_(swmm0, std=0.1)\n",
        "        mm0_loc_param_w_l = pyro.param(\"mm0_loc_w_l\", lwmm0)\n",
        "        mm0_loc_param_w_s = softplus(pyro.param(\"mm0_loc_w_s\", swmm0))\n",
        "        mm0_prior_w = Laplace(loc=mm0_loc_param_w_l, scale=mm0_loc_param_w_s)\n",
        "        # bias\n",
        "        lbmm0 = torch.empty_like(self.model.mm0.bias)\n",
        "        sbmm0 = torch.empty_like(self.model.mm0.bias)\n",
        "        torch.nn.init.normal_(lbmm0, std=0.01)\n",
        "        torch.nn.init.normal_(sbmm0, std=0.1)\n",
        "        mm0_loc_param_b_l = pyro.param(\"mm0_loc_b_l\", lbmm0)\n",
        "        mm0_loc_param_b_s = softplus(pyro.param(\"mm0_loc_b_s\", sbmm0))\n",
        "        mm0_prior_b = Laplace(loc=mm0_loc_param_b_l, scale=mm0_loc_param_b_s)\n",
        "\n",
        "        # mm1\n",
        "        # weight\n",
        "        lwmm1 = torch.empty_like(self.model.mm1.weight)\n",
        "        swmm1 = torch.empty_like(self.model.mm1.weight)\n",
        "        torch.nn.init.normal_(lwmm1, std=0.01)\n",
        "        torch.nn.init.normal_(swmm1, std=0.1)\n",
        "        mm1_loc_param_w_l = pyro.param(\"mm1_loc_w_l\", lwmm1)\n",
        "        mm1_loc_param_w_s = softplus(pyro.param(\"mm1_loc_w_s\", swmm1))\n",
        "        mm1_prior_w = Laplace(loc=mm1_loc_param_w_l, scale=mm1_loc_param_w_s)\n",
        "        # bias\n",
        "        lbmm1 = torch.empty_like(self.model.mm1.bias)\n",
        "        sbmm1 = torch.empty_like(self.model.mm1.bias)\n",
        "        torch.nn.init.normal_(lbmm1, std=0.01)\n",
        "        torch.nn.init.normal_(sbmm1, std=0.1)\n",
        "        mm1_loc_param_b_l = pyro.param(\"mm1_loc_b_l\", lbmm1)\n",
        "        mm1_loc_param_b_s = softplus(pyro.param(\"mm1_loc_b_s\", sbmm1))\n",
        "        mm1_prior_b = Laplace(loc=mm1_loc_param_b_l, scale=mm1_loc_param_b_s)\n",
        "\n",
        "        priors = {\n",
        "        'linear_h_text.weight': linear_h_text_prior_w, 'linear_h_text.bias': linear_h_text_prior_b,\n",
        "        'linear_h_image.weight': linear_h_image_prior_w, 'linear_h_image.bias': linear_h_image_prior_b,\n",
        "        'linear_z.weight': linear_z_prior_w, 'linear_z.bias': linear_z_prior_b,\n",
        "        'mm0.weight': mm0_prior_w, 'mm0.bias': mm0_prior_b,\n",
        "        'mm1.weight': mm1_prior_w, 'mm1.bias': mm1_prior_b}\n",
        "\n",
        "        lifted_module = pyro.random_module(\"module\", self.model, priors)\n",
        "\n",
        "        return lifted_module()    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV4XuUhWPy-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Train-Validation-Test\n",
        "\n",
        "class Training_Testing_MM():\n",
        "\n",
        "    def __init__(self, Data_train_tensor_text, Data_train_tensor_image, Labels_train_tensor, \n",
        "                 Data_test_tensor_text, Data_test_tensor_image, Labels_test_tensor, \n",
        "                 Data_val_tensor_text, Data_val_tensor_image, Labels_val_tensor,\n",
        "                 Label_names = None, hidden_layer_size = 512, num_maxout_units = 2, weight_decay= 0.1, scheduler_step_size = 30, scheduler_lr_fraction = 0.8,\n",
        "                 hidden_activation = \"tanh\", batch_size = 32, epochs = 10, sigmoid_thresh = 0.2, learning_rate = 2e-5, num_labels = 23, dropout = 0.1, max_norm = 5):\n",
        "\n",
        "\n",
        "      self.gmu = GMU(num_maxout_units = num_maxout_units, hidden_layer_size = hidden_layer_size, hidden_activation = hidden_activation, dropout = dropout).cuda()\n",
        "      self.mlp = Maxout_MLP(hidden_layer_size, hidden_layer_size, dropout = dropout, num_labels = num_labels, num_maxout_units = num_maxout_units).cuda()\n",
        "      self.pyro = GMU_Pyro(model = self.gmu)\n",
        "      self.inference = SVI(self.pyro.gmu_model, self.pyro.gmu_guide, ClippedAdam({\"lr\": learning_rate}), loss = self.simple_mc_elbo)\n",
        "        \n",
        "      self.label_names = Label_names\n",
        "      self.num_labels = num_labels\n",
        "      self.batch_size = batch_size\n",
        "      self.learning_rate = learning_rate\n",
        "      self.max_norm = max_norm\n",
        "      self.epochs = epochs\n",
        "      self.sigmoid_thresh = sigmoid_thresh\n",
        "      self.scheduler_step_size = scheduler_step_size\n",
        "      self.scheduler_lr_fraction = scheduler_lr_fraction\n",
        "      self.weight_decay = weight_decay\n",
        "      self.optimizer = self.SetOptimizer()\n",
        "      self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "      self.results = pd.DataFrame(0, index=['Recall','Precision','F_Score'], columns=['micro', 'macro', 'weighted', 'samples']).astype(float)\n",
        "      self.epoch_loss_set = []\n",
        "      self.epoch_gmu_loss_set = []\n",
        "      self.train_dataloader = self.SetTrainDataloader_MM(Data_train_tensor_text, Data_train_tensor_image, Labels_train_tensor)\n",
        "      self.test_dataloader = self.SetTestDataloader_MM(Data_test_tensor_text, Data_test_tensor_image, Labels_test_tensor) \n",
        "      self.scheduler = self.SetScheduler()\n",
        "\n",
        "      self.val_accuracy_set = [] \n",
        "      self.val_dataloader = self.SetValDataloader_MM(Data_val_tensor_text, Data_val_tensor_image, Labels_val_tensor)\n",
        "      self.class_wise_metrics = None\n",
        "      self.predictions = None\n",
        "\n",
        "\n",
        "    # custom Elbo\n",
        "    def simple_mc_elbo(self, model, guide, *args):\n",
        "      guide_trace = poutine.trace(guide).get_trace(*args)\n",
        "      model_trace = poutine.trace(poutine.replay(model, trace=guide_trace)).get_trace(*args)\n",
        "      elbo = model_trace.log_prob_sum() - guide_trace.log_prob_sum()\n",
        "      return -elbo\n",
        "\n",
        "\n",
        "    def SetOptimizer(self) :\n",
        "\n",
        "      optimizer = AdamW(self.mlp.parameters(), lr=self.learning_rate,  eps = 1e-6, weight_decay=self.weight_decay)\n",
        "      #optimizer = Adam(self.mlp.parameters(), lr=self.learning_rate,  eps = 1e-6, weight_decay=self.weight_decay)\n",
        "      return(optimizer)\n",
        "\n",
        "    \n",
        "\n",
        "    def SetScheduler(self) :\n",
        "\n",
        "      '''\n",
        "      scheduler = get_linear_schedule_with_warmup(self.optimizer, num_warmup_steps = 10, \n",
        "                                                 num_training_steps = self.epochs*len(self.train_dataloader))\n",
        "      '''\n",
        "      scheduler = StepLR(self.optimizer, step_size = self.scheduler_step_size, gamma = self.scheduler_lr_fraction)\n",
        "      return(scheduler) \n",
        "\n",
        "\n",
        "\n",
        "    def Get_Metrics(self, actual, predicted) :\n",
        "\n",
        "      #acc = metrics.accuracy_score(actual, predicted)\n",
        "      #hamming = metrics.hamming_loss(actual, predicted)\n",
        "      #(metrics.roc_auc_score(actual, predicted, average=average)\n",
        "      averages = ('micro', 'macro', 'weighted', 'samples')\n",
        "      for average in averages:\n",
        "          precision, recall, fscore, _ = metrics.precision_recall_fscore_support(actual, predicted, average=average)\n",
        "          self.results[average]['Recall'] += recall\n",
        "          self.results[average]['Precision'] += precision\n",
        "          self.results[average]['F_Score'] += fscore\n",
        "\n",
        "\n",
        "\n",
        "    #source: https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "    def Plot_Training_Epoch_Loss(self) :\n",
        "\n",
        "      sns.set(style='darkgrid')\n",
        "      sns.set(font_scale=1.5)\n",
        "      plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "      plt.plot(self.epoch_loss_set, 'b-o')\n",
        "      plt.title(\"Training loss\")\n",
        "      plt.xlabel(\"Epoch\")\n",
        "      plt.ylabel(\"Loss\")\n",
        "      plt.savefig('Training_Epoch_Loss.png',bbox_inches='tight')\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "    def Plot_Training_Epoch_SVI_Loss(self) :\n",
        "\n",
        "      sns.set(style='darkgrid')\n",
        "      sns.set(font_scale=1.5)\n",
        "      plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "      plt.plot(self.epoch_gmu_loss_set, 'b-o')\n",
        "      plt.title(\"Training loss (SVI)\")\n",
        "      plt.xlabel(\"Epoch\")\n",
        "      plt.ylabel(\"Loss\")\n",
        "      plt.savefig('Training_Epoch_SVI_Loss.png',bbox_inches='tight')\n",
        "      plt.show()\n",
        "\n",
        "    \n",
        "    def Plot_Training_Epoch_Accuracy(self) :\n",
        "\n",
        "      sns.set(style='darkgrid')\n",
        "      sns.set(font_scale=1.5)\n",
        "      plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "      plt.plot(self.val_accuracy_set, 'b-o')\n",
        "      plt.title(\"Micro F1 Score\")\n",
        "      plt.xlabel(\"Epoch\")\n",
        "      plt.ylabel(\"Validation Accuracy\")\n",
        "      plt.savefig('Training_Validation_Accuracy.png',bbox_inches='tight')\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "    #source: https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "    def format_time(self, elapsed):\n",
        "      '''\n",
        "      Takes a time in seconds and returns a string hh:mm:ss\n",
        "      '''\n",
        "      # Round to the nearest second.\n",
        "      elapsed_rounded = int(round((elapsed)))\n",
        "      return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "\n",
        "    def SetTrainDataloader_MM(self, Data_train_tensor_text, Data_train_tensor_image, Labels_train_tensor) :\n",
        "\n",
        "      train_dataset = TensorDataset(Data_train_tensor_text, Data_train_tensor_image, Labels_train_tensor)\n",
        "      train_sampler = RandomSampler(train_dataset)\n",
        "      train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size = self.batch_size)\n",
        "      return(train_dataloader)\n",
        "\n",
        "\n",
        "    def SetTestDataloader_MM(self, Data_test_tensor_text, Data_test_tensor_image, Labels_test_tensor) :\n",
        "      \n",
        "      test_dataset = TensorDataset(Data_test_tensor_text, Data_test_tensor_image, Labels_test_tensor)\n",
        "      test_sampler = SequentialSampler(test_dataset)\n",
        "      #test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size = self.batch_size)\n",
        "      test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size = Data_test_tensor_text.shape[0])\n",
        "      return(test_dataloader)\n",
        "\n",
        "    \n",
        "    def SetValDataloader_MM(self, Data_val_tensor_text, Data_val_tensor_image, Labels_val_tensor) :\n",
        "      \n",
        "      val_dataset = TensorDataset(Data_val_tensor_text, Data_val_tensor_image, Labels_val_tensor)\n",
        "      val_sampler = SequentialSampler(val_dataset)\n",
        "      #test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size = self.batch_size)\n",
        "      val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size = Data_val_tensor_text.shape[0])\n",
        "      return(val_dataloader)\n",
        "\n",
        "   \n",
        "    def Train(self) :\n",
        "      \n",
        "      # clear param store\n",
        "      pyro.clear_param_store()\n",
        "      \n",
        "      for _ in trange(self.epochs, desc=\"Epoch\"):\n",
        "        \n",
        "        self.gmu.train()\n",
        "        self.mlp.train()\n",
        "        epoch_loss = 0\n",
        "        epoch_gmu_loss = 0\n",
        "\n",
        "        # Measure how long the training epoch takes.\n",
        "        t0 = time.time()\n",
        "    \n",
        "        for step_num, batch_data in enumerate(self.train_dataloader):\n",
        "\n",
        "          # Progress update every 30 batches.\n",
        "          if step_num % 30 == 0 and not step_num == 0:\n",
        "            elapsed = self.format_time(time.time() - t0)\n",
        "            print('  Batch : ',step_num, ' , Time elapsed : ',elapsed)\n",
        "\n",
        "          samples_text, samples_image, labels = tuple(t.to(self.device) for t in batch_data)\n",
        "          self.optimizer.zero_grad()\n",
        "\n",
        "          ##### Pyro - GMU #####\n",
        "          gmu_loss = self.inference.step(samples_text.float(), samples_image.float(), labels.t())\n",
        "          attendout = self.gmu(samples_text, samples_image)\n",
        "          epoch_gmu_loss += gmu_loss\n",
        "\n",
        "          ##### MLP ####\n",
        "          logits = self.mlp(attendout)\n",
        "          loss_fct = BCEWithLogitsLoss()\n",
        "          batch_loss = loss_fct(logits.view(-1, self.num_labels).float(), labels.view(-1, self.num_labels).float())\n",
        "          batch_loss.backward()\n",
        "          clip_grad_norm_(self.mlp.parameters(), norm_type = 2, max_norm = self.max_norm)\n",
        "          self.optimizer.step()\n",
        "          self.scheduler.step()\n",
        "          epoch_loss += batch_loss.item()\n",
        "\n",
        "        avg_epoch_loss = epoch_loss/len(self.train_dataloader)\n",
        "        avg_epoch_gmu_loss = epoch_gmu_loss/len(self.train_dataloader)\n",
        "        print(\"\\nTrain loss for epoch: \",avg_epoch_loss)\n",
        "        print(\"\\nTrain loss for epoch (gmu): \",avg_epoch_gmu_loss)\n",
        "        print(\"\\nTraining epoch took: {:}\".format(self.format_time(time.time() - t0)))\n",
        "        self.epoch_loss_set.append(avg_epoch_loss)\n",
        "        self.epoch_gmu_loss_set.append(avg_epoch_gmu_loss)\n",
        "\n",
        "        '''\n",
        "        #Validation on the epoch\n",
        "        self.mlp.eval()\n",
        "        epoch_f1_score = 0\n",
        "\n",
        "        for batch_data in self.val_dataloader:\n",
        "          samples_text, samples_image, labels = tuple(t.to(self.device) for t in batch_data)\n",
        "          with torch.no_grad():\n",
        "            output = self.mlp(samples_image.float(), samples_text.float())\n",
        "\n",
        "          threshold = torch.Tensor([self.sigmoid_thresh]).to(self.device)\n",
        "          predictions = (output > threshold).int()\n",
        "\n",
        "          predictions = predictions.detach().cpu().numpy()\n",
        "          labels = labels.to('cpu').numpy()\n",
        "      \n",
        "          micro_f_score = metrics.f1_score(labels,predictions,average=\"micro\")\n",
        "          epoch_f1_score += micro_f_score\n",
        "\n",
        "        avg_val_f1_score = epoch_f1_score/len(self.val_dataloader)\n",
        "        print(\"\\n Micro F1 score for epoch: \",avg_val_f1_score,\"\\n\")\n",
        "        self.val_accuracy_set.append(avg_val_f1_score)\n",
        "        '''\n",
        "\n",
        "      #torch.save(self.mlp.state_dict(), \"/content/drive/My Drive/dataset/model.pt\")\n",
        "      self.Plot_Training_Epoch_Loss()\n",
        "      self.Plot_Training_Epoch_SVI_Loss()\n",
        "      #self.Plot_Training_Epoch_Accuracy()\n",
        "   \n",
        "\n",
        "    def Test(self) :\n",
        "\n",
        "      # Put model in evaluation mode to evaluate loss on the test set\n",
        "      self.mlp.eval()\n",
        "      self.gmu.eval()\n",
        "\n",
        "      for batch_data in self.test_dataloader:\n",
        "  \n",
        "        samples_text, samples_image, labels = tuple(t.to(self.device) for t in batch_data)\n",
        "      \n",
        "        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "        # Forward pass, calculate logit predictions\n",
        "        with torch.no_grad():\n",
        "            attendout = self.gmu(samples_text.float(), samples_image.float())\n",
        "            output = self.mlp(attendout)\n",
        "\n",
        "        threshold = torch.Tensor([self.sigmoid_thresh]).to(self.device)\n",
        "        predictions = (output > threshold).int()\n",
        "\n",
        "        # Move preds and labels to CPU\n",
        "        predictions = predictions.detach().cpu().numpy()\n",
        "        labels = labels.to('cpu').numpy()\n",
        "\n",
        "        self.predictions = predictions\n",
        "        self.Get_Metrics(labels, predictions)\n",
        "        self.class_wise_metrics = metrics.classification_report(labels, predictions, target_names= list(self.label_names))\n",
        "        \n",
        "    \n",
        "      self.results = self.results/len(self.test_dataloader)\n",
        "      #print(\"Test data metrics : \\n\")\n",
        "\n",
        "      #print(\"\\nGenres with no predicted samples : \", self.label_names[np.where(np.sum(predictions, axis=0) == 0)[0]])\n",
        "      \n",
        "      return(self.results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e1J4EORP1hv",
        "colab_type": "code",
        "outputId": "b5e7ff28-a731-48fb-e5d7-2da1fbb91473",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "'''\n",
        "hidden_layer_size = random.choice([128,256,512,1024])\n",
        "batch_size = random.choice([128,256,512])\n",
        "learning_rate = np.random.uniform(0.01, 0.0001)\n",
        "dropout = np.random.uniform(0.5, 0.8)\n",
        "sigmoid_thresh = np.random.uniform(0.3, 0.6)\n",
        "weight_decay = np.random.uniform(0.1, 0.01)\n",
        "num_maxout_units = np.random.randint(2,10)\n",
        "max_norm = np.random.randint(5,20)\n",
        "'''\n",
        "epochs = 60\n",
        "hidden_layer_size = 512\n",
        "batch_size = 256\n",
        "learning_rate = 0.001\n",
        "dropout = 0.7\n",
        "sigmoid_thresh = 0.5\n",
        "weight_decay = 0.01\n",
        "num_maxout_units = 10\n",
        "max_norm = 10\n",
        "\n",
        "hyperparameters = {'hidden_layer_size' : hidden_layer_size, 'epochs' : epochs, 'batch_size' : batch_size, 'learning_rate' : learning_rate, 'dropout' : dropout, 'scheduler_step_size' : 99999, \n",
        "                    'scheduler_lr_fraction' : 0.85, 'sigmoid_thresh' : sigmoid_thresh, 'num_maxout_units' : num_maxout_units, 'weight_decay' : weight_decay, 'max_norm' : max_norm}\n",
        "for key, value in hyperparameters.items():\n",
        "  print(key,\" : \",value)\n",
        "\n",
        "\n",
        "train_test = Training_Testing_MM( Data_train_tensor_text, Data_train_tensor_image, Labels_train_tensor, \n",
        "                                  Data_test_tensor_text, Data_test_tensor_image, Labels_test_tensor, \n",
        "                                  Data_val_tensor_text, Data_val_tensor_image, Labels_val_tensor, Label_names=Label_names, \n",
        "                                  hidden_layer_size = hidden_layer_size, epochs = epochs, batch_size= batch_size, learning_rate = learning_rate, dropout = dropout, scheduler_step_size = 99999, \n",
        "                                  scheduler_lr_fraction = 0.85, sigmoid_thresh = sigmoid_thresh, num_maxout_units = num_maxout_units, weight_decay = weight_decay, max_norm = max_norm)\n",
        "train_test.Train()\n",
        "train_test.Test()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/60 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "hidden_layer_size  :  512\n",
            "epochs  :  60\n",
            "batch_size  :  256\n",
            "learning_rate  :  0.001\n",
            "dropout  :  0.7\n",
            "scheduler_step_size  :  99999\n",
            "scheduler_lr_fraction  :  0.85\n",
            "sigmoid_thresh  :  0.5\n",
            "num_maxout_units  :  10\n",
            "weight_decay  :  0.01\n",
            "max_norm  :  10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyro/primitives.py:406: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.\n",
            "  \"modules from `torch.nn.Module` instances.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:04\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   2%|▏         | 1/60 [00:04<04:33,  4.64s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.5878738234750926\n",
            "\n",
            "Train loss for epoch (gmu):  2616366.8203125\n",
            "\n",
            "Training epoch took: 0:00:05\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   3%|▎         | 2/60 [00:08<04:11,  4.33s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.27483089431189\n",
            "\n",
            "Train loss for epoch (gmu):  2438935.7734375\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   5%|▌         | 3/60 [00:11<03:56,  4.15s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.22745689866133034\n",
            "\n",
            "Train loss for epoch (gmu):  2280192.2578125\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   7%|▋         | 4/60 [00:15<03:43,  3.99s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.21263911039568484\n",
            "\n",
            "Train loss for epoch (gmu):  2131520.6953125\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   8%|▊         | 5/60 [00:19<03:34,  3.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.2008344226051122\n",
            "\n",
            "Train loss for epoch (gmu):  1991417.734375\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  10%|█         | 6/60 [00:22<03:25,  3.81s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.1912766289897263\n",
            "\n",
            "Train loss for epoch (gmu):  1859175.125\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  12%|█▏        | 7/60 [00:26<03:20,  3.79s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.18172120372764766\n",
            "\n",
            "Train loss for epoch (gmu):  1733274.7890625\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  13%|█▎        | 8/60 [00:30<03:14,  3.74s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.17367501044645905\n",
            "\n",
            "Train loss for epoch (gmu):  1614315.9921875\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  15%|█▌        | 9/60 [00:33<03:09,  3.71s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.16614615824073553\n",
            "\n",
            "Train loss for epoch (gmu):  1501433.0546875\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  17%|█▋        | 10/60 [00:37<03:04,  3.69s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.15774651081301272\n",
            "\n",
            "Train loss for epoch (gmu):  1395136.984375\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:03\n",
            "  Batch :  60  , Time elapsed :  0:00:04\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  18%|█▊        | 11/60 [00:42<03:13,  3.96s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.15155605785548687\n",
            "\n",
            "Train loss for epoch (gmu):  1295233.75\n",
            "\n",
            "Training epoch took: 0:00:05\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  20%|██        | 12/60 [00:45<03:04,  3.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.14535991451703012\n",
            "\n",
            "Train loss for epoch (gmu):  1200315.5\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:04\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  22%|██▏       | 13/60 [00:49<02:59,  3.81s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.14018339128233492\n",
            "\n",
            "Train loss for epoch (gmu):  1111542.8671875\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  23%|██▎       | 14/60 [00:53<02:52,  3.76s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.1345954806311056\n",
            "\n",
            "Train loss for epoch (gmu):  1027654.9921875\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  25%|██▌       | 15/60 [00:56<02:47,  3.71s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.13028275896795094\n",
            "\n",
            "Train loss for epoch (gmu):  948874.3515625\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  27%|██▋       | 16/60 [01:00<02:42,  3.69s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.12517295614816248\n",
            "\n",
            "Train loss for epoch (gmu):  875184.75\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  28%|██▊       | 17/60 [01:03<02:37,  3.67s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.12030431779567152\n",
            "\n",
            "Train loss for epoch (gmu):  806047.7109375\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  30%|███       | 18/60 [01:07<02:33,  3.65s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.11643225082661957\n",
            "\n",
            "Train loss for epoch (gmu):  741619.7890625\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:04\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  32%|███▏      | 19/60 [01:12<02:41,  3.95s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.11172631522640586\n",
            "\n",
            "Train loss for epoch (gmu):  681700.9609375\n",
            "\n",
            "Training epoch took: 0:00:05\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  33%|███▎      | 20/60 [01:15<02:33,  3.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.10813098715152591\n",
            "\n",
            "Train loss for epoch (gmu):  626018.2734375\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  35%|███▌      | 21/60 [01:19<02:28,  3.81s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.10457321151625365\n",
            "\n",
            "Train loss for epoch (gmu):  574451.75\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  37%|███▋      | 22/60 [01:23<02:22,  3.74s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.1019032426411286\n",
            "\n",
            "Train loss for epoch (gmu):  526352.0390625\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  38%|███▊      | 23/60 [01:26<02:17,  3.72s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.09921185579150915\n",
            "\n",
            "Train loss for epoch (gmu):  482442.40625\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  40%|████      | 24/60 [01:30<02:12,  3.69s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.09582353022415191\n",
            "\n",
            "Train loss for epoch (gmu):  441764.703125\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  42%|████▏     | 25/60 [01:34<02:08,  3.67s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.0932403578190133\n",
            "\n",
            "Train loss for epoch (gmu):  404394.359375\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  43%|████▎     | 26/60 [01:37<02:04,  3.65s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.08944303321186453\n",
            "\n",
            "Train loss for epoch (gmu):  370176.9921875\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  45%|████▌     | 27/60 [01:41<02:00,  3.65s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.08751403191126883\n",
            "\n",
            "Train loss for epoch (gmu):  339170.796875\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:04\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  47%|████▋     | 28/60 [01:45<02:05,  3.91s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.08518418855965137\n",
            "\n",
            "Train loss for epoch (gmu):  310138.5078125\n",
            "\n",
            "Training epoch took: 0:00:05\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  48%|████▊     | 29/60 [01:49<01:59,  3.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.08426692662760615\n",
            "\n",
            "Train loss for epoch (gmu):  284117.6796875\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  50%|█████     | 30/60 [01:53<01:53,  3.78s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.08221580612007529\n",
            "\n",
            "Train loss for epoch (gmu):  260249.84375\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  52%|█████▏    | 31/60 [01:56<01:47,  3.72s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.07897871325258166\n",
            "\n",
            "Train loss for epoch (gmu):  238314.1875\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  53%|█████▎    | 32/60 [02:00<01:43,  3.70s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.07775818917434663\n",
            "\n",
            "Train loss for epoch (gmu):  218699.1015625\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  55%|█████▌    | 33/60 [02:03<01:39,  3.68s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.07629464066121727\n",
            "\n",
            "Train loss for epoch (gmu):  200672.28125\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  57%|█████▋    | 34/60 [02:07<01:35,  3.66s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.07288233953295276\n",
            "\n",
            "Train loss for epoch (gmu):  184223.453125\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  58%|█████▊    | 35/60 [02:11<01:31,  3.68s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.07197560404893011\n",
            "\n",
            "Train loss for epoch (gmu):  169414.96875\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  60%|██████    | 36/60 [02:14<01:27,  3.66s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.07148370501818135\n",
            "\n",
            "Train loss for epoch (gmu):  155855.5390625\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:04\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  62%|██████▏   | 37/60 [02:19<01:31,  3.96s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.06939387763850391\n",
            "\n",
            "Train loss for epoch (gmu):  143783.7890625\n",
            "\n",
            "Training epoch took: 0:00:05\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  63%|██████▎   | 38/60 [02:23<01:24,  3.86s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.06772697065025568\n",
            "\n",
            "Train loss for epoch (gmu):  132719.0\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  65%|██████▌   | 39/60 [02:26<01:20,  3.81s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.06587189686251804\n",
            "\n",
            "Train loss for epoch (gmu):  122830.5390625\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  67%|██████▋   | 40/60 [02:30<01:15,  3.75s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.0654483997495845\n",
            "\n",
            "Train loss for epoch (gmu):  113890.03125\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  68%|██████▊   | 41/60 [02:34<01:10,  3.71s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.06515268952352926\n",
            "\n",
            "Train loss for epoch (gmu):  105942.4921875\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  70%|███████   | 42/60 [02:37<01:06,  3.68s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.062491692893672734\n",
            "\n",
            "Train loss for epoch (gmu):  98541.8984375\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  72%|███████▏  | 43/60 [02:41<01:02,  3.66s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.06221908499719575\n",
            "\n",
            "Train loss for epoch (gmu):  92111.875\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  73%|███████▎  | 44/60 [02:44<00:58,  3.65s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.061467754654586315\n",
            "\n",
            "Train loss for epoch (gmu):  86206.1328125\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  75%|███████▌  | 45/60 [02:48<00:54,  3.66s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.06010992452502251\n",
            "\n",
            "Train loss for epoch (gmu):  80973.1171875\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:04\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  77%|███████▋  | 46/60 [02:53<00:55,  3.93s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.05833128496306017\n",
            "\n",
            "Train loss for epoch (gmu):  76363.953125\n",
            "\n",
            "Training epoch took: 0:00:05\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  78%|███████▊  | 47/60 [02:56<00:49,  3.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.05817798781208694\n",
            "\n",
            "Train loss for epoch (gmu):  72135.109375\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  80%|████████  | 48/60 [03:00<00:45,  3.76s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.05730478069745004\n",
            "\n",
            "Train loss for epoch (gmu):  68242.34375\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  82%|████████▏ | 49/60 [03:04<00:40,  3.71s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.05649880552664399\n",
            "\n",
            "Train loss for epoch (gmu):  64926.5546875\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  83%|████████▎ | 50/60 [03:07<00:36,  3.68s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.0565614546649158\n",
            "\n",
            "Train loss for epoch (gmu):  61730.109375\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  85%|████████▌ | 51/60 [03:11<00:33,  3.69s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.055503893410786986\n",
            "\n",
            "Train loss for epoch (gmu):  59140.7265625\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  87%|████████▋ | 52/60 [03:14<00:29,  3.66s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.05427249672356993\n",
            "\n",
            "Train loss for epoch (gmu):  56693.734375\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  88%|████████▊ | 53/60 [03:18<00:25,  3.67s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.05270695604849607\n",
            "\n",
            "Train loss for epoch (gmu):  54591.828125\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  90%|█████████ | 54/60 [03:22<00:21,  3.65s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.051758830959443\n",
            "\n",
            "Train loss for epoch (gmu):  52754.25\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:03\n",
            "  Batch :  60  , Time elapsed :  0:00:04\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  92%|█████████▏| 55/60 [03:26<00:19,  3.95s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.0514422181295231\n",
            "\n",
            "Train loss for epoch (gmu):  50937.671875\n",
            "\n",
            "Training epoch took: 0:00:05\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  93%|█████████▎| 56/60 [03:30<00:15,  3.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.05239502456970513\n",
            "\n",
            "Train loss for epoch (gmu):  49472.75\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  95%|█████████▌| 57/60 [03:34<00:11,  3.78s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.05032277089776471\n",
            "\n",
            "Train loss for epoch (gmu):  48072.625\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  97%|█████████▋| 58/60 [03:37<00:07,  3.74s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.04971783247310668\n",
            "\n",
            "Train loss for epoch (gmu):  46926.671875\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  98%|█████████▊| 59/60 [03:41<00:03,  3.71s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.04886485374299809\n",
            "\n",
            "Train loss for epoch (gmu):  45644.15625\n",
            "\n",
            "Training epoch took: 0:00:04\n",
            "  Batch :  30  , Time elapsed :  0:00:02\n",
            "  Batch :  60  , Time elapsed :  0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch: 100%|██████████| 60/60 [03:45<00:00,  3.68s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss for epoch:  0.04846290324348956\n",
            "\n",
            "Train loss for epoch (gmu):  44623.4375\n",
            "\n",
            "Training epoch took: 0:00:04\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuUAAAGaCAYAAACopj13AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdeXzU1b3/8fdMZssKJISAQAAREkFA\nQNRcFxYRU4qKFFTUIrfK1VrtLV0Er9bfba8WRSrubcEugNQCEkRLZREUFVEQlCgGhIgCBsgQspJk\nZpKZ3x+QqTEzWSDJ95vM6/l42Ns53+1MTvG+c/ic87UEAoGAAAAAABjGanQHAAAAgEhHKAcAAAAM\nRigHAAAADEYoBwAAAAxGKAcAAAAMRigHAAAADEYoB4B2ZN68eUpLS5Pb7T6j6z0ej9LS0vTwww83\nc8+a5uWXX1ZaWpo++eQTQ/sBAK3FZnQHAKC9SUtLa/S5GzduVI8ePVqwNwCAtoBQDgDNbO7cubU+\n79ixQ8uWLdNNN92k4cOH1zqWmJjYrM/+2c9+pvvuu09Op/OMrnc6ncrOzlZUVFSz9gsAUD9COQA0\ns+uvv77W5+rqai1btkwXXnhhnWPhBAIBVVRUKCYmpknPttlsstnO7l/tZxroAQBnjppyADDYO++8\no7S0NP3zn//UokWLlJmZqUGDBumll16SJO3cuVP333+/xo0bpyFDhmjYsGG69dZb9dZbb9W5V6ia\n8pq2Q4cO6fHHH9cVV1yhQYMG6YYbbtCWLVtqXR+qpvzbbdu3b9fUqVM1ZMgQXXrppXr44YdVUVFR\npx/vv/++pkyZokGDBunyyy/XY489ps8//1xpaWlasGDBGf+sjh8/rocfflhXXnmlLrjgAo0ePVqP\nPPKIiouLa51XXl6u+fPn65prrtHgwYM1YsQIXXvttZo/f36t8958801NnTpVl1xyiQYPHqzRo0fr\npz/9qQ4dOnTGfQSAM8FMOQCYxMKFC1VaWqof/OAHSkpKUs+ePSVJa9eu1aFDhzR+/Hidc845OnHi\nhFatWqW7775bzz77rMaNG9eo+//iF7+Q0+nUnXfeKY/Ho7/97W/68Y9/rA0bNiglJaXB6z/99FOt\nW7dOkydP1nXXXaetW7dq2bJlcjgceuihh4Lnbd26VTNmzFBiYqLuuusuxcXFac2aNdq2bduZ/WBO\nKyoq0k033aS8vDxNmTJF6enp+vTTT/XSSy/pww8/1PLlyxUdHS1J+vWvf601a9bohhtu0IUXXiif\nz6evvvpKH3zwQfB+7733nu69914NGDBAd999t+Li4nTs2DFt2bJFhw8fDv78AaA1EMoBwCTy8/P1\nxhtvqGPHjrXaf/azn9UpY/nhD3+o6667Tn/4wx8aHcpTUlL0zDPPyGKxSFJwxn3FihW69957G7x+\n7969euWVVzRgwABJ0tSpU3X77bdr2bJluv/+++VwOCRJc+bMkd1u1/Lly9WtWzdJ0i233KKbb765\nUf0M549//KMOHz6sRx99VJMnTw629+vXT48//njwl4xAIKBNmzZp7NixmjNnTtj7vfnmm5KkRYsW\nKT4+PtjemJ8FADQ3ylcAwCR+8IMf1AnkkmoF8oqKChUWFsrj8ejiiy9WTk6OvF5vo+5/++23BwO5\nJA0fPlx2u11fffVVo64fMWJEMJDXuPTSS+X1enXkyBFJ0jfffKO9e/fqmmuuCQZySXI4HJo2bVqj\nnhNOzYz+pEmTarXfdtttio+P14YNGyRJFotFsbGx2rt3r3Jzc8PeLz4+XoFAQOvWrVN1dfVZ9Q0A\nzhYz5QBgEr179w7Znp+fr/nz5+utt95SYWFhneOlpaVKSkpq8P7fLcewWCzq0KGDioqKGtW/UOUc\nNb9EFBUVqVevXjp8+LAkqU+fPnXODdXWWIFAQHl5ebr00ktltdaeT3I4HEpNTQ0+W5IefPBB/c//\n/I/Gjx+vXr166ZJLLtGYMWM0atSo4C8mt99+u95++209+OCDeuyxx3TRRRfpiiuu0Pjx49WpU6cz\n7isAnAlCOQCYRE099LdVV1dr+vTpOnz4sKZNm6aBAwcqPj5eVqtV//jHP7Ru3Tr5/f5G3f+7YbZG\nIBA4q+ubco/W8r3vfU+XXHKJ3nnnHW3btk3vvfeeli9froyMDL344ouy2Wzq3LmzVq1ape3bt+v9\n99/X9u3b9cgjj+iZZ57Rn//8Z11wwQVGfw0AEYRQDgAm9tlnnyk3N1c///nPddddd9U6VrM7i5l0\n795dknTgwIE6x0K1NZbFYlH37t315Zdfyu/31/oFwev16uDBg0pNTa11TWJioiZOnKiJEycqEAjo\nd7/7nRYvXqx33nlHY8aMkXRqC8mMjAxlZGRIOvXznjx5sv70pz/p2WefPeP+AkBTUVMOACZWEz6/\nOxO9e/dubd682Ygu1atHjx7q37+/1q1bF6wzl04F58WLF5/VvceOHaujR4/q1VdfrdX+97//XaWl\npbr66qslST6fT2VlZbXOsVgsOv/88yUpuH3iiRMn6jzjvPPOk8PhaHRJDwA0F2bKAcDE0tLS1Lt3\nb/3hD39QSUmJevfurdzcXC1fvlxpaWnavXu30V2sY/bs2ZoxY4ZuvPFG3XzzzYqNjdWaNWtqLTI9\nE3fffbfWr1+vhx56SLt27VJaWpo+++wzZWVlqX///po+fbqkU/XtY8eO1dixY5WWlqbExEQdOnRI\nL7/8sjp16qSRI0dKku6//36VlJQoIyND3bt3V3l5uf75z3/K4/Fo4sSJZ/tjAIAmIZQDgIk5HA4t\nXLhQc+fO1cqVK+XxeNS/f389+eST2rFjhylD+WWXXaYFCxZo/vz5+uMf/6gOHTpowoQJGjt2rG69\n9Va5XK4zum/Hjh21bNkyPfvss9q4caNWrlyppKQk3XbbbbrvvvuCNfnx8fG67bbbtHXrVr377ruq\nqKhQcnKyxo0bp7vuukuJiYmSpEmTJmn16tXKyspSYWGh4uPj1a9fP73wwgu66qqrmu3nAQCNYQmY\nbXUOAKBdeu211/SrX/1Kzz//vMaOHWt0dwDAVKgpBwA0K7/fX2fvdK/Xq0WLFsnhcOiiiy4yqGcA\nYF6UrwAAmlVZWZnGjx+va6+9Vr1799aJEye0Zs0a7du3T/fee2/IFyQBQKQjlAMAmpXL5dJll12m\n9evX6/jx45Kkc889V//3f/+nG2+80eDeAYA5UVMOAAAAGIyacgAAAMBghHIAAADAYNSUn1ZYeFJ+\nf+tW8iQlxamgoKzhE2EIxse8GBvzYmzMi7ExL8bGvJp7bKxWizp1ig15zNBQ7vV69fTTT2v16tUq\nKSlRenq6Zs6cqYyMjEZd//rrr2vRokXav3+/HA6H+vfvr/vvv1+DBw9ucl/8/kCrh/Ka58K8GB/z\nYmzMi7ExL8bGvBgb82qtsTE0lM+ePVvr16/XtGnT1KtXL61atUozZszQkiVLNHTo0HqvnT9/vl58\n8UVdd911uummm1ReXq49e/bI7Xa3Uu8BAACA5mFYKM/OztaaNWv0wAMPaPr06ZKkiRMnasKECZo3\nb56WLl0a9tqdO3fqT3/6k5599lldffXVrdRjAAAAoGUYttBz7dq1stvtmjJlSrDN6XRq8uTJ2rFj\nh/Lz88Neu3jxYg0aNEhXX321/H6/Tp482RpdBgAAAFqEYaE8JydHffr0UWxs7WL3wYMHKxAIKCcn\nJ+y1W7du1aBBg/Tkk09q+PDhGjZsmMaMGaPXXnutpbsNAAAANDvDylfcbrdSUlLqtCcnJ0tS2Jny\n4uJiFRUVac2aNYqKitIvf/lLdezYUUuXLtWvfvUrRUdHU9ICAACANsWwUF5ZWSm73V6n3el0SpI8\nHk/I68rLyyVJRUVFWr58uYYMGSJJuvrqq3X11Vfr+eefP6NQnpQU1+RrmkNycrwhz0XjMD7mxdiY\nF2NjXoyNeTE25tVaY2NYKHe5XPL5fHXaa8J4TTj/rpr2Hj16BAO5JDkcDl1zzTVavHixTp48Wacs\npiEFBWWtvh1RcnK83O7SVn0mGo/xMS/GxrwYG/NibMyLsTGv5h4bq9USdiLYsJry5OTkkCUqNVsa\ndunSJeR1HTt2lMPhUOfOnesc69y5swKBgMrK2IAfAAAAbYdhoTw9PV0HDhyos3PKrl27gsdDsVqt\nOv/883Xs2LE6x44ePaqoqCh16NCh+TsMAAAAtBDDQnlmZqZ8Pp9WrFgRbPN6vcrKytKwYcOCi0Dz\n8vKUm5tb59ojR45oy5YtwbaysjK98cYbGjp0qFwuV+t8iTO0dfdR/eqFLbruF6v1qxe2aOvuo0Z3\nCQAAAAYyrKZ8yJAhyszM1Lx58+R2u5WamqpVq1YpLy9Pc+bMCZ43a9Ysbdu2TXv37g22TZ06VStW\nrNB9992n6dOnKyEhQStXrlRpaal+/vOfG/F1Gm3r7qNa9MYeeav8kqSCEo8WvbFHkpQxsKuRXQMA\nAIBBDAvlkjR37lw99dRTWr16tYqLi5WWlqYFCxZo+PDh9V4XHR2txYsXa+7cuXrppZdUWVmpgQMH\n6q9//WuD1xota3NuMJDX8Fb5lbU5l1AOAAAQoSyBQKB1txwxqdbafeVHj20Ke+wvs8e0+PPReKyG\nNy/GxrwYG/NibMyLsTGviNh9JVIlJYTe6jFcOwAAANo/QnkrmzSyrxy22j92h82qSSP7GtQjAAAA\nGM3QmvJIVFM3vuKt/Soq8you2qapY/tTTw4AABDBmCk3QMbArnp4+ghJ0g1XnEsgBwAAiHCEcoNE\nO0/9JUW5p8rgngAAAMBohHKDOGxW2aIshHIAAAAQyo1isVgU47KrwlNtdFcAAABgMEK5gWKj7apg\nphwAACDiEcoNFOuyqbySUA4AABDpCOUGOlW+QigHAACIdIRyA8VG21noCQAAAEK5kWKZKQcAAIAI\n5YaKibYxUw4AAABCuZHiXHZ5vNWq9vuN7goAAAAMRCg3UEy0XZLYqxwAACDCEcoNFOuySRJ15QAA\nABGOUG6g2NMz5exVDgAAENkI5QaKcdWUrxDKAQAAIhmh3ECxhHIAAACIUG6oYPkKoRwAACCiEcoN\nFHN6oSehHAAAILIRyg0UG035CgAAAAjlhrJFWeWwW9l9BQAAIMIRyg0W7bQxUw4AABDhCOUGiyGU\nAwAARDxCucFinDYWegIAAEQ4QrnBKF8BAAAAodxgMS6byj3VRncDAAAABiKUGyzaaVNFpc/obgAA\nAMBAhHKDRTuZKQcAAIh0hHKDxThtqqr2y1dFMAcAAIhUhHKDRTttksRsOQAAQAQjlBss5nQoZwcW\nAACAyEUoN1i0i1AOAAAQ6QjlBquZKS+vJJQDAABEKkK5waIpXwEAAIh4hHKDBWfKCeUAAAARi1Bu\nsGjKVwAAACIeodxgLmeULKJ8BQAAIJIRyg1mtVjkctooXwEAAIhghHITiHFGMVMOAAAQwWxGPtzr\n9erpp5/W6tWrVVJSovT0dM2cOVMZGRn1Xvfss8/queeeq9PeuXNnbdmypaW622KinXZCOQAAQAQz\nNJTPnj1b69ev17Rp09SrVy+tWrVKM2bM0JIlSzR06NAGr//tb38rl8sV/Pzt/96WxDijWOgJAAAQ\nwQwL5dnZ2VqzZo0eeOABTZ8+XZI0ceJETZgwQfPmzdPSpUsbvMf3vvc9JSQktHBPW16006bCUo/R\n3QAAAIBBDKspX7t2rex2u6ZMmRJsczqdmjx5snbs2KH8/PwG7xEIBFRWVqZAINCSXW1xMS4WegIA\nAEQyw0J5Tk6O+vTpo9jY2FrtgwcPViAQUE5OToP3GDVqlIYPH67hw4frgQceUFFRUUt1t0VFO23U\nlAMAAEQww8pX3G63UlJS6rQnJydLUr0z5QkJCfrhD3+oIUOGyG6364MPPtCyZcv0+eefa8WKFXI4\nHC3W75ZwKpRXKxAIyGKxGN0dAAAAtDLDQnllZaXsdnuddqfTKUnyeMLXWN9+++21PmdmZqpfv376\n7W9/q1dffVU33nhjk/uTlBTX5GuaQ3JyvLokxcofCCi+Q0zwDZ8wh+TkeKO7gDAYG/NibMyLsTEv\nxsa8WmtsDEuALpdLPp+vTntNGK8J5401depUPfHEE9q6desZhfKCgjL5/a1bm56cHC+3u1TVVdWS\npIOHC5WY0DZ3kGmPasYH5sPYmBdjY16MjXkxNubV3GNjtVrCTgQbVlOenJwcskTF7XZLkrp06dKk\n+1mtVqWkpKi4uLhZ+teaYk7PjlNXDgAAEJkMC+Xp6ek6cOCATp48Wat9165dweNN4fP5dOTIEXXq\n1KnZ+tha/h3Kqw3uCQAAAIxgWCjPzMyUz+fTihUrgm1er1dZWVkaNmxYcBFoXl6ecnNza1174sSJ\nOvf785//LI/HoyuuuKJlO94CaurIyz11y3kAAADQ/hlWUz5kyBBlZmZq3rx5crvdSk1N1apVq5SX\nl6c5c+YEz5s1a5a2bdumvXv3BttGjx6t8ePHq3///nI4HPrwww+1bt06DR8+XBMmTDDi65yVf4dy\nylcAAAAikaFbfcydO1dPPfWUVq9ereLiYqWlpWnBggUaPnx4vddde+212rlzp9auXSufz6fu3bvr\nnnvu0V133SWbre3tXhLjonwFAAAgkhmaYJ1Op2bNmqVZs2aFPWfJkiV12h555JGW7FarC86UV1K+\nAgAAEIkMqynHvzlsVkVZLcyUAwAARChCuQlYLJbTb/WkphwAACASEcpNIsZpY6EnAABAhCKUm0S0\ni5lyAACASEUoN4kYp03llYRyAACASEQoNwlqygEAACIXodwkqCkHAACIXIRyk4gmlAMAAEQsQrlJ\nRDuj5PFWy+8PGN0VAAAAtDJCuUnEuOySpAovs+UAAACRhlBuEtHOKEliBxYAAIAIRCg3iRinTZLY\ngQUAACACEcpNglAOAAAQuQjlJhHtOhXKKV8BAACIPIRyk4g+PVPOtogAAACRh1BuEpSvAAAARC5C\nuUkwUw4AABC5COUmYYuyymG3MlMOAAAQgQjlJhLttBHKAQAAIhCh3ERinDZ2XwEAAIhAhHITiWGm\nHAAAICIRyk0k2mlTuafa6G4AAACglRHKTeRUKGemHAAAINIQyk0kxkX5CgAAQCQilJsIu68AAABE\nJkK5iUQ7bfJV+eWr8hvdFQAAALQiQrmJxJx+qyez5QAAAJGFUG4ihHIAAIDIRCg3kejToZwdWAAA\nACILodxEYlyEcgAAgEhEKDeRmpnyikpCOQAAQCQhlJtItDNKEjPlAAAAkYZQbiIxTrskFnoCAABE\nGkK5ibicUbJIKqd8BQAAIKIQyk3EarHIxVs9AQAAIg6h3GRinFGEcgAAgAhDKDeZaKeNhZ4AAAAR\nhlBuMjGUrwAAAEQcQrnJMFMOAAAQeQjlJhPtsrH7CgAAQIQhlJsM5SsAAACRx9BQ7vV69cQTT+jy\nyy/X4MGDdeONN2rr1q1Nvs+MGTOUlpamRx99tAV62bqinTZVeKoVCASM7goAAABaiaGhfPbs2Vq0\naJGuu+46Pfjgg7JarZoxY4Y+/vjjRt/j7bff1kcffdSCvWxdMU6b/IGAPL5qo7sCAACAVmJYKM/O\nztaaNWv0y1/+Uvfff79uuukmLVq0SN26ddO8efMadQ+v16s5c+bojjvuaOHetp5ol02SVOEhlAMA\nAEQKw0L52rVrZbfbNWXKlGCb0+nU5MmTtWPHDuXn5zd4j8WLF6uysrJdhfIY56lQzg4sAAAAkcOw\nUJ6Tk6M+ffooNja2VvvgwYMVCASUk5NT7/Vut1svvPCCZs6cqejo6JbsaquKPh3KK9iBBQAAIGIY\nFsrdbre6dOlSpz05OVmSGpwpf/LJJ9WnTx9df/31LdI/ozBTDgAAEHlsRj24srJSdru9TrvT6ZQk\neTyesNdmZ2fr1Vdf1ZIlS2SxWJqlP0lJcc1yn6ZKTo6v9bnSf+r/2p22OsfQ+hgD82JszIuxMS/G\nxrwYG/NqrbExLJS7XC75fL467TVhvCacf1cgENCjjz6qcePG6aKLLmq2/hQUlMnvb91tCJOT4+V2\nl9Zqqzh56vsfdZfVOYbWFWp8YA6MjXkxNubF2JgXY2NezT02Vqsl7ESwYaE8OTk5ZImK2+2WpJCl\nLZK0YcMGZWdna+bMmTp8+HCtY2VlZTp8+LA6d+4sl8vV/J1uBTHB3VcoXwEAAIgUhoXy9PR0LVmy\nRCdPnqy12HPXrl3B46Hk5eXJ7/fr9ttvr3MsKytLWVlZWrhwoa688sqW6XgLc9isirJaCOUAAAAR\nxLBQnpmZqb/85S9asWKFpk+fLunUvuNZWVkaNmyYUlJSJJ0K4RUVFerbt68kacyYMerRo0ed+/3k\nJz/R6NGjNXnyZA0cOLDVvkdzs1gsinbaVM7uKwAAABHDsFA+ZMgQZWZmat68eXK73UpNTdWqVauU\nl5enOXPmBM+bNWuWtm3bpr1790qSUlNTlZqaGvKePXv21NixY1ul/y0pxmljphwAACCCGBbKJWnu\n3Ll66qmntHr1ahUXFystLU0LFizQ8OHDjeyW4aKdNrZEBAAAiCCGhnKn06lZs2Zp1qxZYc9ZsmRJ\no+5VM5PeHsS4COUAAACRxLCXByG8aMpXAAAAIgqh3ISinVEs9AQAAIgghHITinHamSkHAACIIIRy\nE4p2RqnSW93qbxgFAACAMQjlJhTjPP1WTy+z5QAAAJGAUG5C0a7ToZy6cgAAgIhAKDehmplytkUE\nAACIDIRyE4quKV8hlAMAAEQEQrkJxbiYKQcAAIgkhHITYqYcAAAgshDKTagmlPMCIQAAgMhAKDeh\nGGbKAQAAIgqh3IRsUVY5bFZVeKqN7goAAABaAaHcpKJdNpV7fEZ3AwAAAK2AUG5SMU6bypkpBwAA\niAiEcpOKdtqoKQcAAIgQhHKTinHa2H0FAAAgQhDKTYqZcgAAgMhBKDcpQjkAAEDkIJSbVIzLpnJC\nOQAAQEQglJtUtNMmX5VfVdV+o7sCAACAFtbkUP7111/rnXfeqdW2a9cu3X333br55pu1bNmyZutc\nJKt5qyez5QAAAO2frakXzJs3T0VFRbryyislSSdOnNCMGTNUXl4up9Op//3f/1VSUpLGjh3b7J2N\nJDWhvKKySgkxDoN7AwAAgJbU5Jnyzz77TP/xH/8R/LxmzRqVlZUpKytLW7du1ZAhQ7Ro0aJm7WQk\nimamHAAAIGI0OZSfOHFCXbp0CX5+9913NWzYMPXv318Oh0Pjx49Xbm5us3YyEkU7oyQRygEAACJB\nk0N5dHS0SktLJUnV1dXasWOHLrroouBxl8ulsrKy5uthhIpx2SWdKl8BAABA+9bkUN6vXz+9+uqr\nKiws1PLly1VeXq7LLrssePybb75RYmJis3YyEtXMlLNXOQAAQPvX5IWed9xxh+65555gXfn5559f\na6Z8y5YtGjBgQPP1MELFOE/NlFO+AgAA0P41OZSPGjVKixYt0saNGxUXF6fbbrtNFotFklRYWKiu\nXbtq4sSJzd7RSONyRskiZsoBAAAiQZNDuSSNGDFCI0aMqNPeqVMnPffcc2fdKUhWi0UuZxQz5QAA\nABHgjEL5d1VVVWnjxo0qLi7W6NGjlZyc3By3jXgxThsLPQEAACJAk0P53Llz9eGHH2rlypWSpEAg\noP/8z//URx99pEAgoI4dO2r58uVKTU1t9s5GmminjZlyAACACNDk3VfefffdWgs7N23apO3bt+uO\nO+7Q73//e0nSggULmq+HESzaaaOmHAAAIAI0eab86NGj6tWrV/DzW2+9pR49euiXv/ylJGnfvn16\n/fXXm6+HESzGaVNhmcfobgAAAKCFNXmm3OfzyWb7d5b/8MMPg9sjSlLPnj3ldrubp3cRLtrFTDkA\nAEAkaHIo79q1qz7++GNJp2bFDx06VGsnloKCAsXExDRfDyPYqfKVaqO7AQAAgBbW5PKV73//+3rh\nhRd04sQJ7du3T3FxcRo5cmTweE5ODos8m0mM06byyioFAoHgXvAAAABof5o8U37XXXfphhtu0Cef\nfCKLxaLHH39cCQkJkqTS0lJt2rRJGRkZzd7RSBTjtMkfCMjr8xvdFQAAALSgJs+UOxwO/e53vwt5\nLDY2Vu+9955cLtdZdwynylckqdxTJacjyuDeAAAAoKU0eaa83ptZrYqPj5fdbm/O20asGNe/QzkA\nAADarzN6o2d5eblefPFFbdiwQYcPH5Yk9ejRQ+PGjdMdd9zBQs9mUjNTzg4sAAAA7VuTQ3lRUZFu\nvfVW5ebmKjExUeeff74k6auvvtLzzz+vtWvXaunSperYsWOD9/J6vXr66ae1evVqlZSUKD09XTNn\nzmywJv21117TK6+8otzcXBUXF6tLly665JJLdO+996p79+5N/UqmRSgHAACIDE0O5c8884y+/PJL\n/frXv9bNN9+sqKhTtc7V1dVatmyZHnnkET333HN66KGHGrzX7NmztX79ek2bNk29evXSqlWrNGPG\nDC1ZskRDhw4Ne92ePXuUkpKikSNHqkOHDsrLy9Py5cv19ttv67XXXlNycnJTv5YpxdTUlFcSygEA\nANqzJofyTZs2acqUKbr11ltrtUdFRemWW25RTk6O3nzzzQZDeXZ2ttasWaMHHnhA06dPlyRNnDhR\nEyZM0Lx587R06dKw195///112q666ipNmjRJr732mu64446mfi1TYqYcAAAgMjR5oefx48eDJSuh\nDBgwQMePH2/wPmvXrpXdbteUKVOCbU6nU5MnT9aOHTuUn5/fpH6dc845kqSSkpImXWdmNQs9CeUA\nAADtW5Nnyjt37qycnJywx3NyctS5c+cG75OTk6M+ffooNja2VvvgwYMVCASUk5OjLl261HuPoqIi\nVVdXKy8vT88//7wktas90h02q6KsFnZfAQAAaOeaHMpHjx6tZcuWacCAAbrxxhtltZ6abPf7/Vqx\nYoVWrlypm266qcH7uN1upaSk1GmvqQdvzEz5Nddco6KiIklSx44d9fDDD+vSSy9tytcxNYvFomin\njVAOAADQzjU5lP/0pz/V+++/r9/85jd69tln1adPH0nSgQMHdOLECaWmpuq+++5r8D6VlZUh9zN3\nOp2SJI/H0+A9nnvuOZWXlwGaECwAACAASURBVOvAgQN67bXXdPLkySZ+m39LSoo742vPRnJyfL3H\n42Mc8gcsDZ6HlsHP3bwYG/NibMyLsTEvxsa8WmtsmhzKO3XqpJUrV2rhwoV688039emnn0qSevbs\nqcmTJ2vGjBmKi2s44LpcLvl8vjrtNWG8JpzXZ8SIEZKkkSNH6qqrrtK1116rmJgY3XbbbU35SpKk\ngoIy+f2BJl93NpKT4+V2l9Z7jsNmVWFJRYPnofk1ZnxgDMbGvBgb82JszIuxMa/mHhur1RJ2IviM\nXh4UFxenmTNnaubMmXWO/eMf/9DixYv1r3/9q957JCcnhyxRcbvdktRgPfl39ezZUwMHDtTrr79+\nRqHcrKKdUSz0BAAAaOeavPtKQwoLC3XgwIEGz0tPT9eBAwfqlJzs2rUreLypKisrVVravn7TjHHZ\nqSkHAABo55o9lDdWZmamfD6fVqxYEWzzer3KysrSsGHDgotA8/LylJubW+vaEydO1LnfZ599pj17\n9mjgwIEt2/FWxkw5AABA+3dG5SvNYciQIcrMzNS8efPkdruVmpqqVatWKS8vT3PmzAmeN2vWLG3b\ntk179+4Nto0ePVrf+9731L9/f8XExGj//v1auXKlYmNjdc899xjxdVpMtNNGKAcAAGjnDAvlkjR3\n7lw99dRTWr16tYqLi5WWlqYFCxZo+PDh9V53yy23aOvWrXrzzTdVWVmp5ORkZWZm6p577lHPnj1b\nqfetI8ZpU4WnWn5/QFarxejuAAAAoAUYGsqdTqdmzZqlWbNmhT1nyZIlddrqO7+9iXGeGqJKb5Vi\nXHW3kAQAAEDb16hQ/te//rXRN9y5c+cZdwZ1RZ8O5eUeQjkAAEB71ahQ/vjjjzfpphYLZRbNJcZ1\nOpRXVkkdDO4MAAAAWkSjQvnixYtbuh8Io2amnMWeAAAA7VejQvnFF1/c0v1AGPu/KZYkPf73j5WU\n4NSkkX2VMbCrwb0CAABAczJsn3I0bOvuo1qz9evg54ISjxa9sUdbdx81sFcAAABoboRyE8vanCtf\nlb9Wm7fKr6zNuWGuAAAAQFtEKDexghJPk9oBAADQNhHKTSwpwdmkdgAAALRNhHITmzSyrxy22kPk\nsFk1aWRfg3oEAACAlmDoGz1Rv5pdVrI256qgxCOrRZqWmcbuKwAAAO0ModzkMgZ2VcbArtqWc0x/\nXL1bCbEOo7sEAACAZkb5ShsxtF+y4qLt2vxJntFdAQAAQDMjlLcRdptVlw3qqk/2HVfxSa/R3QEA\nAEAzIpS3IVcOOUfV/oC2fHrE6K4AAACgGRHK25BuSbHq37Oj3tmVJ38gYHR3AAAA0EwI5W3MyCHn\nKL+wQnu/LjS6KwAAAGgmhPI2ZnhasmJdNm3exYJPAACA9oJQ3sY47FHKGNhVO79wq7ScBZ8AAADt\nAaG8DbrywnNUVR3Q+58dNborAAAAaAaE8jaoR3Kc+nZP0Du78hRgwScAAECbRyhvo64cco6OFJRr\n3+Fio7sCAACAs0Qob6MuTk9RtDOKN3wCAAC0A4TyNsrpiNKlA7rqo735OlnpM7o7AAAAOAuE8jZs\n5IXnyFfl11YWfAIAALRphPI2LDUlXr27xmszCz4BAADaNEJ5GzfywnP0jfukvswrMborAAAAOEOE\n8jbu4vNT5LRH8YZPAACANoxQ3sZFO226ZEAXbcs5pgpPldHdAQAAwBkglLcDIy/sLq/Prw8+P2Z0\nVwAAAHAGbEZ3AGevd9d4JcY79fcNX2jJur1KSnBq0si+yhjY1eiuAQAAoBGYKW8HPvj8mIpPelXt\nP7UDS0GJR4ve2KOtu9kqEQAAoC0glLcDWZtzg4G8hrfKr6zNuQb1CAAAAE1BKG8HCko8TWoHAACA\nuRDK24GkBGfI9g6xjlbuCQAAAM4EobwdmDSyrxy2ukNZVuHVthx2ZAEAADA7Qnk7kDGwq27/Xnpw\nxjwpwalbr+6nPud00B9X79ayTftU7fcb3EsAAACEw5aI7UTGwK51tkAceWF3Ldu4X+u2HdLXR0t1\n9/UXKIGSFgAAANMhlLdjtiirbh3XX727xWvxur36zd+2a+SQc/Rudp4KSjzsZw4AAGAShPIIcNmg\nbuqRHKffL/tYr753INhes5+5JII5AACAgagpjxC9usbLbouq085+5gAAAMYjlEeQwlL2MwcAADAj\nQ8tXvF6vnn76aa1evVolJSVKT0/XzJkzlZGRUe9169ev17/+9S9lZ2eroKBA3bp10+jRo3XPPfco\nPj6+lXrf9iQlOEMGcIfNqsJSjzrFh97vHAAAAC3L0Jny2bNna9GiRbruuuv04IMPymq1asaMGfr4\n44/rve7Xv/61cnNzdf311+uhhx7S5ZdfriVLlmjq1KnyeJj1DSfUfuZRVouqqv16cOEH2rjjsPz+\ngEG9AwAAiFyGzZRnZ2drzZo1euCBBzR9+nRJ0sSJEzVhwgTNmzdPS5cuDXvtM888o0suuaRW2wUX\nXKBZs2ZpzZo1mjRpUkt2vc2qWcyZtTm31u4r556ToCXr9mrphi+0dfdR3Z6Zrp5d4gzuLQAAQOQw\nLJSvXbtWdrtdU6ZMCbY5nU5NnjxZ8+fPV35+vrp06RLy2u8GckkaO3asJCk3l0WL9Qm1n7kk/eKm\nC/XB7mN6eeM+/fZv2zXu4p7qmhij1947wPaJAAAALcywUJ6Tk6M+ffooNja2VvvgwYMVCASUk5MT\nNpSHcvz4cUlSp06dmrWfkcJisSjjgq4a1DdJyzft1xsfHKx1nO0TAQAAWo5hNeVutztk6E5OTpYk\n5efnN+l+CxcuVFRUlMaNG9cs/YtUcdF2/ej75yshxl7nGNsnAgAAtAzDZsorKytlt9cNfk7nqR1A\nmrJg8/XXX9crr7yiu+66S6mpqWfUn6QkY2qok5PNuVtMabkvZHtBice0fW4JkfRd2xrGxrwYG/Ni\nbMyLsTGv1hobw0K5y+WSz1c3+NWE8Zpw3pCPPvpIDz74oEaNGqX//u//PuP+FBSUtfrOI8nJ8XK7\nS1v1mY2VGGb7REn67cKtuu7yPureOTbk8fbCzOMT6Rgb82JszIuxMS/Gxryae2ysVkvYiWDDQnly\ncnLIEhW32y1Jjaon37Nnj3784x8rLS1N8+fPV1RU3TdW4sxMGtlXi97YI2+VP9hmt1k1sE8nZX9Z\noI/25OuSgSm6/rI++vJISZ0dXag7BwAAaDzDQnl6erqWLFmikydP1lrsuWvXruDx+hw8eFB33nmn\nEhMT9ac//UkxMTEt2t9IE277xIyBXVVa7tXaDw9q487D+mD3MVktUs1fMrAgFAAAoOkMW+iZmZkp\nn8+nFStWBNu8Xq+ysrI0bNgwpaSkSJLy8vLqbHPodrv1ox/9SBaLRX/+85+VmJjYqn2PFBkDu+qJ\ney7TX2aP0RP3XBYM2fExDk0ZfZ4ev/s/5HJE6btVPywIBQAAaBrDZsqHDBmizMxMzZs3T263W6mp\nqVq1apXy8vI0Z86c4HmzZs3Stm3btHfv3mDbnXfeqUOHDunOO+/Ujh07tGPHjuCx1NRUDR06tFW/\nS6TqEOtQpbc65LFw9egAAACoy7BQLklz587VU089pdWrV6u4uFhpaWlasGCBhg8fXu91e/acKo94\n8cUX6xy74YYbCOWtKKmeBaF/3/CFxmf0Use4xi3aBQAAiFSWQCDQuluOmBS7r5yZrbuPhlwQem63\neO07XKKoKItGD+2u8Zf20u6vTrSpBaHtYXzaK8bGvBgb82JszIuxMa+I2H0F7UN9C0LzC8v1+pav\ntOGjQ9q445ACsgR/8WFBKAAAwL8RynHWMgZ2DRmsu3SK0R0TBmh8Ri/95q/ba82mS/9eEEooBwAA\nkc6w3VcQObolxdYJ5DUKSjyiggoAAEQ6QjlaRVJC+MWejyz+SFs/OypfmOAOAADQ3lG+glYR6g2h\nDptVI9K76MsjJVr4z8+1bNM+jRraXaOGdlfO14VtalEoAADA2SCUo1XUtyDUHwjo869OaONHh/X6\nlq/0+pavZOEtoQAAIIIQytFqwi0ItVosuqBPki7ok6RjheX6zV+313kpEYtCAQBAe0ZNOUwlpVNM\nvW8JPXCkpJV7BAAA0PKYKYfp1PeW0P9b9JF6pcRr5NBzdMn5Kfpk/3FqzwEAQJtHKIfphFsUOnVs\nP1VVB7T5k2+0eO1eLd3whQJ+yR/ghUQAAKBtI5TDdOpbFCpJY4Z115d5JZr3j4/lqeaFRAAAoO0j\nlMOUwi0KlSSLxaK+3TvI4wv/QqIKT5WinfzPGwAAtA2kFrRZ9dWe//KFLbpyyDkaO7ynkjq4tHX3\nUWrPAQCAaRHK0WaFqz0ff2kv5RWc1Ibth7Vh+2H16Ravg8fK5Dtd6kLtOQAAMBtCOdqshmrPj4+q\n0MYdh7Vu26E611J7DgAAzIRQjjatvtrzzh2iddOYfiFDuXRqxtzrq5bDHtWSXQQAAGgQoRztXn21\n5//9zHsacl6SLkrrokF9k+S0RwXrz0+UeJRI/TkAAGgFhHK0e+Fqz8de1EPllVXa8YVb23Ly5bBb\n1b1zrA7ll6mqmr3PAQBA6yGUo91rqPb81nH99cWhYn20J19vf/KNTr+LKIj6cwAA0NII5YgI9dWe\nR1mtOr9XJ53fq5Pe+vibkOcUlHi0dfdRXXhe5+D+52yzCAAAmguhHPiWcPXnVou08PXPZYuy6II+\nSeoQ59D7nx2Vr4ptFgEAwNmzGt0BwEwmjewrh632HwuHzaofff98/c9twzVmWA8dzC/V5k/ygoG8\nRk2ZCwAAQFMxUw58y7frz0PtvnJejw66ccx5uvPxt0JeX1Di0Sf7jys9taNcDspcAABA4xDKge+o\nqT9PTo6X211a57jVYql3m8VnXslWlNWifj06KD7Grk/2FfA2UQAAUC/KV4AzELbMZXy6fnHzhbp6\nRE+VVVRp+x53MJDXoMwFAAB8FzPlwBloaJvFgb0TdeNo6UePbQp5fUGJR+u2HdSQ8zqra2JMsJ1S\nFwAAIhOhHDhD9W2zWCNcmUuU1aJlm/Zr2ab9SukUrSHndZbdZtWG7YeCLzmi1AUAgMhBKAdaULi3\nid7+vXT1695Bu3ILtGv/cW3aeTj4FtFvC/fiImbUAQBoXwjlQAtqqMzlquE9dNXwHqr0VumeJ98J\neY+CEo9effdL9evRUeeek6BP9h+vFfSZUQcAoO0jlAMtrDFlLi6Hrd5Sl9e3fKWAJIvl1O4v1f7a\ns+rhZtQBAEDbQCgHTKK+UpcLz+us3Lxi7T9crNe2fBXy+oISjzy+ajntUcE2ylwAAGgbCOWASTRU\n6nJBnyRd0CdJWz49EnaP9Pueekf9enTUwD6J8lf79c+tX1PmAgBAG0AoB0ykMaUu4WbUr7qou/x+\nafeBQr3yduh90ClzAQDAnAjlQBvT0Iy6JBWVefTz57aEvL6gxKNtOceUntpJCbEOSZS5AABgNEI5\n0AY1NKPeMc4ZduGoRdIfV++WJPVIjlWHOIf2HiwKbslImQsAAK2PUA60U+HKXH6YmaZuibHK+fqE\ncr4u1O4DhXWu9Vb5tfyt/bpkQIqsFkutY8yqAwDQ/AjlQDvVUJnLueck6PsZvfWjxzaFvL64zKt7\n57+j3l3j1btbgvp0S1BhaaWyNn/J4lEAAJoZoRxoxxqzcDRcmUusy6aLB6ToqyMlevOjQyHfOCqx\neBQAgOZAKAciXLgyl1uu7h8M2lXVfh12l+m3f/so5D0KSjx6/f2vlJ7aUX26JcgWZaXMBQCAJiCU\nAxGuMbu52KKs6t01od63jq5650tJksNuVXIHl46eqAi+eZQyFwAA6kcoB9CoMhep/reOXtAnUV8c\nKtKeg0V6++NvgoG8hrfKr6UbvlByh2j17BInp+PUm0eZUQcAwOBQ7vV69fTTT2v16tUqKSlRenq6\nZs6cqYyMjHqvy87OVlZWlrKzs/XFF1/I5/Np7969rdRrIHI1NKs+PK2Lhqd10cYdh0NeX15Zpd+9\ntEMWSSmJMYpxRenro2XMqAMAIp6hoXz27Nlav369pk2bpl69emnVqlWaMWOGlixZoqFDh4a9bvPm\nzVqxYoXS0tLUs2dPffnll63YayCync3i0Y5xTv1wXH99faxUB4+VKTv3uL4zoS5vlV8vrd8rhy1K\nqSlx6tzBJcvpbRlrZtVPlHiUyKw6AKAdsQQCgdBbKrSw7OxsTZkyRQ888ICmT58uSfJ4PJowYYK6\ndOmipUuXhr32+PHjiouLk8vl0qOPPqrFixef9Ux5QUGZ/N9NBy0sOTlebndpqz4Tjcf4nLmtu4+G\nLXP5dogOtx3jt0U7bUrtEie7zaqcg4Wq/tYuMKHuWfN8SmKMwZ8b82JszIuxMa/mHhur1aKkpLiQ\nxwybKV+7dq3sdrumTJkSbHM6nZo8ebLmz5+v/Px8denSJeS1nTt3bq1uAjgDjVk8KoWfUU+Md+qe\nGwbp4LFSHcwv08Fjpdp7qKjOed4qvxav3aviMq9SOkWrS6doHThSopfWf8Fe6gCANsWwUJ6Tk6M+\nffooNja2VvvgwYMVCASUk5MTNpQDML/GlLmEWzj6g1F9de45CTr3nIRge7hZdY+vWsvf2l/vc7xV\nfi3ftF8XpSXLbosKtjOjDgAwC8NCudvtVkpKSp325ORkSVJ+fn5rdwlAK2vsjLoUflY9KcGp//3R\nxcovrNCxwnIteO3zkM8qPunV3fM2q1OCUymdYqRAQF8cLm70IlMCPACgJRkWyisrK2W32+u0O51O\nSafqy1tTuPqelpacHG/Ic9E4jE/Lu25UvK4b1a/B86ZPGKjnVuySx1cdbHPaozR9wkD17pmo3j1P\nta1694DchRV1ro+PcejaK85V3vEyHXGf1BeHivTdFTXeKr+WrNsrv8WqPt0S1PucBHWIc+rtHYe0\neO3e4LMLSjxavHavEuJdGjW855l/+XaKPzfmxdiYF2NjXq01NoaFcpfLJZ/PV6e9JozXhPPWwkJP\nfBfjYy4DUztqWmZand1XBqZ2rDVOEy/vE7Ik5uarzmvUItNKb7X+/Npnwc8d4hwqr6iSr9pf6zyP\nr1p/++duDUztWKs90mfU+XNjXoyNeTE25hURCz2Tk5NDlqi43W5Jop4cQB01der1/UvybBeZJiU4\n9dDtI3TYXaZDx8p02F2m9z87GvJZBSUe/XH1Z0ruGK0uHaPlLqrQuu2H5GORKQCgiQwL5enp6Vqy\nZIlOnjxZa7Hnrl27gscB4EyczSLTSSP7qkOsQx1iEzWwd6Ikae/BwpAB3h5l1YEjJfpoj1v+MLvL\n1iwyHZHeRbYoa7A90mfUAQC1GRbKMzMz9Ze//EUrVqwI7lPu9XqVlZWlYcOGBReB5uXlqaKiQn37\n9jWqqwDaoaYsMg0X4Gv2SK+q9utESaVm/+mDkM8qPunVPU9uVo/kOPXulqDqar8+2H0sWBLDIlMA\ngGGhfMiQIcrMzNS8efPkdruVmpqqVatWKS8vT3PmzAmeN2vWLG3btq3Wy4G++eYbrV69WpL06aef\nSpJeeOEFSadm2MeMGdOK3wRAW9WYGfWa86TwAd4WZVWXTjFhS2Liou26YnA3fXW0VB9+fkwVnqo6\n55x6k+kXqqr2q1OcUx3jneoY51R27nEtXru3UfuuE94BoO0yLJRL0ty5c/XUU09p9erVKi4uVlpa\nmhYsWKDhw4fXe93hw4f19NNP12qr+XzDDTcQygE0u7MpiZk6tl/wWn8goDsffyvk9RWeKv31X3sa\n7Iu3yq9/bNync7slKDHBJbvNWuctqoR3AGhbLIFAmELICMPuK/guxse8zDw2jQm8v3phS+g3mSY4\ndf8tw1RU6lFRmUdFZV79Y+O+Bp/ZIc6hkxU+VVXX/XdYfIxd90y8QC6HTS5HlD47UKAVb+WGLcU5\nk+/zbWYem0jH2JgXY2NeEbH7CgC0R2f1JtORfdXl9E4uNTZsPxgywCfE2DVl9HkqKKnU8eJKvZd9\nJOSzSst9evzvH9fbH2+VX4vX7tGxE+VKTHCpU7xTneKdyv2mWC+/ua9Rs+8AgLNDKAeAVtYci0xv\nuqpfrfNzvjoRMrx3iHVoxrUD5PFWq9JbrYX/DP3GU4/Pr9e3fKWG/r6wod1kvr2HPItWAaDxCOUA\nYIDmWmRaI1x4v3HMeRpwemtHScp6Jzfs/uxz7spQcZlXhaUenSit1B9X7w7Zp+KTXv3495vVNSlG\n3TvHyu8P6JP9x4PlM+Fm1JtS915zPgEeQKQglAOAyTUmwJ9teJ80sq9sUVYldXApqYNLUgeteGt/\nmN1kbLpySHd94y7Tl3klOl5cWeccb5Vff1mTo80ff6MYl12xLpt2fOGu9dya81a8tV9D+3WW0x4l\ni8UiqekBHgDaOkI5ALQTzRnepfp2k+lf6/wfPbYp5LOq/QFZLBYdL67UwXyfKr3VIc8rKvPqniff\nkS3KqvgYu+Ki7TpSUK6q6roBPmtzLrvJAGiXCOUAEGGau3Qm3P7sSQlOzbp1WPBzuF1nYl02jc/o\npbJyn0orfCor9+lQflnIPhWUePTsymz1SI5Tzy5xOl5coVffPdCsW0ES8gEYgVAOAAjrbHaTmTSy\nb6POu+Xq/nWeES7AO2xWHT1Rrk/2H1e4DX29VX4tWbdXBcWVcjqi5LJH6WB+mTZ/8k2tuve/vbFH\nZeU+XTwgRbYoi2xRVu3Ym9/olzVJBHgAzYdQDgA4K9+eUa9v95XmKJ2p2U/d66vWN8dP6v8WfRSy\nT5XeamW982W9/fZV+fXyxn16uYG94E9tGblX7sIKdUpwKjHepcQEp/YdLtLfNzRuy0jCO4CGEMoB\nAGetZka9oRdtNFfpjMMepT7dEuotnfndf2XI46tWpbdK9/9ha9hn3Tauv6qq/KryB/TK27khz/H4\nqvXqewca7Le3yq+l67+Q3x9QQqxDCTEO7fumSK9864VNzRXem1qO09B2lQCMRSgHAJjS2ZbO2G1W\n2W1WxUXb6w3vY4b1CH5+a+fhekN+YZlHhSWVOlHq0cLXQ+/5Xu6p0p/X5NTb75oSm9KTXnWMd6pD\nrEMHjpQ0qT6+MbvTsIsN0HYQygEAbVZzbAXZ2PPsNmutN67WPPO7OsU7NeuWoSo56VPxSa+eX/Vp\nyL5Xeqv1j0376/1+3iq//vqvU1tLRkVZZYuyyhZl0e4DJ0JuL7lk3V4dyi9TlNUiq8WiN3ccCnne\nyrPcxYZFs0DzI5QDANq05twKsjnq3ieP6qsunWLUpdOptvpm6f/ff16s4jKPik569ft/fBKy71XV\nAVmtFvmq/ar0VqmqOlAnaNeo9FZr047DqvYHVO0P/37WEyUePfTih+rSMVopidEqq/Dpw8+PNfgC\nKKllZukJ74BkCQTCrV+PLAUFZfLX8y+wltBQ7SWMxfiYF2NjXpE2No0Jk98Np1LtRas1wu04k5Tg\n1BP3XFarrbHn/vKFLToR4jyXI0rn9+qk/KIK5RdWyBcm5Fskxbhsslotp/6xWFRc5lGo/3dpt1k1\nrH+yHDarnPYovffpkZB703eIdejh6SMU67LJYY9q9M+nRnucpY+0PzdtSXOPjdVqUVJSXMhjzJQD\nAHCGWuttq9/V2HN/EOa8H16TFny+PxDQnY+/FbLvAUmXDuwq/+mZd78/oPc+PRLyXF+VXweOlMjr\nq5bX5w/7sqjik1794vktwb5UVfvrhHxvlV9/3/CFYl02dYh1nlo0G2vXtpx8Q2fp2+MvBDAPQjkA\nAC2sud+2eiblOOF2X7FaLPWW2Nx6df9abTlfnwh77mN3ZQQ/h5vNj4u2a9LIc3WywqeTFVVau+1g\nyJ/HycoqPbUiO/jZcvo/vvv3+94qvxa9sUcf7zsuq0WyWCz6eJ87ZC390vVfqNJTJacjSk67TQeO\nFGv99kN1ynYCgYD+44Juta5vybKdxuyMQ9Bv/yhfOY3yFXwX42NejI15MTbmVd/YNKWEpLHnnm3Z\nTsc4h35ywyAVn/Se+qfMo9e2fBX2+3VLilEgIAUCAR0rrAj/g2gkh80qhz1KTnuUnI4oHTtRHrJO\n326zKj21kwIKSAFp78Ei+arrlgO5HFH6fkYvuRw2xTht+vpYiTbt/PcLrWqeeTY/85pzjQrv7fEX\nB8pXAABAq2npWfozKduZMvo89e3eoda5Wz49EnaW/tEZlwY/hwv6ifFO/fr2i1Tpq5bHW63//ev2\nsD+T0cO6y+Pzy+OtltdXrbzjJ0Oe56vyq7TcK8vpqfxQgVw6tQh35eb6X2jlrfLrxdc/V9bmLxXr\nsinGZVOsy67PDhSEnPlf8dZ+DembpGinTRaLxdCyHRb2nj1COQAAaPSLnZpybnOX7Zzt1pY/GNVX\nHeKcqon69ZXt3DSmX622+hbXPjx9RKPO+91/XaoKT7UqvFV64E8fhPhpnKrj79+zo8orfSr3VOno\niXJ5fKGDflGZV/c+9a6irBbFxdhVVu6rM5vvrfJr6YYvVFXtV7TDJpcjSvu/KdYbHxwM/gJRUOLR\n397Yo6pqv/7jgq6yWiyynPotI2TY/tsbe1Ra7tWA3omq9J56QdfLb+4L+YvDyrdzz7q8J1Lq/Slf\nOY3yFXwX42NejI15MTbm1V7GpjnDlxnLdpqy205ctE0TMnqrpNyn0nKv3s0OvQj3TFgtp3bcqQoz\n898UsS6bOsU71SnepS8OFcnjC70rz6+mDg2+9GvX/uN1gn5rjc23tWb5CqH8NEI5vovxMS/GxrwY\nG/NibEIzaja2JX4hCBfeO8U79cCtw07Palfrdy/tCPvzuOGKPqd22gkEVF0d0Bsfhl6IK0l3Xz9Q\nrtOz73949TMVn/TWOSfaadOlA1NUWOJRYalHXx87u/8NRlktSk2Jl9Nulcthk9MRpU/2uUP+bUKM\ny6bJo/rKZrUqKsqil9/cp7IKX53zQv0iVIOacgAAgFZghrKdhnZfOdv6/Mmj+qrz6TfRSvWX7Vx7\nWZ9abdtyjoU99+LzHbDvxAAAEF5JREFUU4KfbxxzXshn3zauf6N+cYiPtuvWcf3lq/IH304bSrU/\noFiXTR5ftU6UVJ5aHxCmvKe8skqL14a+z7eF6o8RCOUAAAAGqAnvjZmNNfue+Gf77JvH9qsV8v+1\n9auwvwz8/KYLa7XV9zcED027SNXVflX7A3ps6c6Qs/lJCc46bUYglAMAALQTRu2J39zPbo5fHCaP\n6qtO8f8O3OFm80Pd0wiEcgAAgAjTEmU7zflsI7fpNAqhHAAAAKZjVL2/UaxGdwAAAACIdIRyAAAA\nwGCEcgAAAMBghHIAAADAYIRyAAAAwGCEcgAAAMBghHIAAADAYIRyAAAAwGCEcgAAAMBgvNHzNKvV\nElHPReMwPubF2JgXY2NejI15MTbm1ZxjU9+9LIFAINBsTwIAAADQZJSvAAAAAAYjlAMAAAAGI5QD\nAAAABiOUAwAAAAYjlAMAAAAGI5QDAAAABiOUAwAAAAYjlAMAAAAGI5QDAAAABiOUAwAAAAYjlLcy\nr9erJ554QpdffrkGDx6sG2+8UVu3bjW6WxEnPz9f8+bN0w9/+EMNHTpUaWlp+vDDD0Oeu3HjRt1w\nww0aNGiQRo0apeeee05VVVWt3OPIkJ2drd/85jcaP368LrzwQo0aNUozZ87U119/XefcnTt3aurU\nqRoyZIguu+wyPfLII6qoqDCg15Hj008/1U9+8hONHj1agwcP1mWXXaY77rhDO3furHMu42OshQsX\nKi0tTddff32dY4xN6/rwww+VlpYW8p/c3Nxa5zI2xsjOztZ//dd/acSIERo6dKiuu+46ZWVl1Tqn\nNbKArVnvhgbNnj1b69ev17Rp09Tr/7d397E13v8fx59K1f2mlJliSm9UTW8ytG6+pljXMF2QWrXu\naxRxswmbSLaZ7I+VzKrM2hEWwXQ42mQoOrZ1LIwOLdYq2kyp27aqN7i+f0jPb2enu0l+357L2tcj\naeJ6f97V9znvnPbdc32uq926sXv3bmJjY/nyyy8JCAgwu7wGIz8/n6SkJLp164a3tzenTp2qNe/I\nkSPMmTOHAQMGsHz5ci5evEhiYiJ37txh+fLlDq66/ktOTubnn38mLCwMb29viouL2bp1KxEREaSk\npNCjRw8AcnJymDJlCj179mTp0qUUFRWxceNGCgsL+eyzz0x+FPVXQUEBjx49Yvz48bi5uVFaWkpq\nairR0dEkJSUxcOBAQP0xW3FxMevXr6dFixZ2a+qNeSZPnkzv3r1tYh07drT+W70xR83P+X79+jF/\n/nyaNGnC5cuXuXbtml1Onc8ChjhMVlaW4eXlZWzatMkaq6ioMIYPH25ERUWZV1gDVFpaaty+fdsw\nDMNIT083vLy8jGPHjtnlhYeHG6+//rrx8OFDa2z16tWGj4+PkZ+f76hyG4yTJ08alZWVNrH8/HzD\nz8/PWLJkiTU2Y8YMY/DgwUZZWZk19tVXXxleXl5GZmamw+oVwygvLzdCQkKMmTNnWmPqj7mWLFli\nxMTEGNHR0cZrr71ms6beON6xY8cMLy8vIz09/S/z1BvHKykpMYKDg40VK1b8ZZ6jZgFtX3Ggffv2\n4ezszPjx460xFxcXxo0bx8mTJ7lx44aJ1TUsrVq1om3btn+Zk5ubS25uLpGRkTRu3Ngaj4qK4vHj\nxxw4cKCuy2xwAgMDadq0qU3shRdewNPT03qat6ysjMzMTCIiImjZsqU1b8yYMbRo0YJvvvnGoTU3\ndM2bN8fV1ZWSkhJA/THbL7/8wt69e3nnnXfs1tQb85WVldW65UG9MUdqaiolJSXMnz8feNIHwzBs\nchw5C2god6CcnBy6d+9u84IDePHFFzEMg5ycHJMqk9pkZ2cD4OfnZxPv2LEjzz33nHVd6pZhGNy8\nedP6S9SFCxd4+PChXV+aNm1Kr1699DpygLKyMm7fvs2lS5dYvXo1Fy9eJDg4GFB/zGQYBitWrCAi\nIoJevXrZras35lq8eDFBQUH07duXadOmceHCBeuaemOOH3/8EQ8PD44cOcJ//vMfgoKC6NevH/Hx\n8Tx69Ahw7CygPeUOVFxcbLN/rIabmxuA3il/yhQXFwP/15/fc3NzU78cZO/evVy/fp2FCxcCf9+X\n06dPO7S+hujdd99l//79ADg7OzNhwgRmzZoFqD9m2rNnD7m5uSQmJta6rt6Yw9nZmVdeeYUhQ4bQ\ntm1bLly4wMaNG4mKiiIlJYXu3burNya5cuUKRUVFLF26lBkzZuDr60tGRgZJSUlUVlaybNkyh84C\nGsodqKKiAmdnZ7u4i4sLAJWVlY4uSf5CRUUFgN12CnjSM10RX/fy8vL44IMPCAoKst5F4u/6UrMu\ndWfOnDlERkZSVFSExWKhqqqK6upqmjZtqv6YpKysjFWrVjFz5kw6dOhQa456Y47AwEACAwOtx6Gh\noQwbNoyxY8eydu1aVq1apd6YpLy8nHv37vHWW28xc+ZMAEaOHEl5eTnbtm1j9uzZDp0FtH3FgZo1\na0Z1dbVdvGYYrxnO5enQrFkz4MltLP+osrLSui51o7i4mDfffJNnnnmGNWvW4OT05NuV+mI+b29v\nBg4cyNixY/niiy84d+6cdQ+z+mOO9evX4+zszNSpU/80R715evj4+BAcHMyxY8cA9cYsNc/rqFGj\nbOKjR4+murqaM2fOOLQ3Gsod6M9Oc9ScGvmzdzfEHDWnqmr683vFxcXqVx0qLS0lNjaW0tJSkpOT\nbU4bqi9PF2dnZ0JDQzlw4AAVFRXqjwlu3LjB5s2biYqK4ubNmxQWFlJYWEhlZSXV1dUUFhZy7949\n9eYp06lTJ+7duwfo+5pZap739u3b28Rrjh39utFQ7kA+Pj7k5+dz//59m3hWVpZ1XZ4eNRdKnT17\n1iZ+/fp1ioqKar2QSv7/KisrmTVrFpcvX2bDhg14eHjYrHt5edGkSRO7vlRVVZGTk6O+mKCiogLD\nMLh//776Y4Jbt25RXV1NfHw8oaGh1o+srCzy8vIIDQ0lKSlJvXnKFBQUWC9gV2/MUXPf+OvXr9vE\ni4qKAHB1dXXoLKCh3IHCwsKorq5m586d1lhVVRW7du0iMDCw1otAxTyenp54eHiwY8cO61XYANu2\nbcPJyYmRI0eaWF399OjRIxYsWMDp06dZs2YN/v7+djmtW7cmODgYi8Vi8wuuxWKhvLycsLAwR5bc\noNy+fdsuVlZWxv79++nUqRPt2rVTf0zg7u5OYmKi3YenpyedO3cmMTGRiIgI9cYktb1uTpw4wfHj\nxxk0aBCg72tmqXleU1JSrDHDMNi5cyctWrTA39/fobNAI+OPN2SUOjV//nwOHTrE5MmT6dq1K7t3\n7+bs2bNs3ryZoKAgs8trUNatWwc8uZgwLS2NsWPH4u7uTps2bYiOjgYgIyOD2bNnM2DAAMLDw7l4\n8SJbt24lMjKS9957z8Tq66eVK1eyZcsWXn75ZV599VWbtZYtWzJ8+HAAzp07x4QJE/D09GT8+PEU\nFRWxadMm+vfvT1JSkhmlNwiTJk3CxcWFgIAA3NzcuHbtGrt27aKoqIjVq1cTHh4OqD9Pi5iYGEpK\nSrBYLNaYeuN4kyZNonnz5gQEBNC2bVt+/fVXduzYQevWrUlJSeH5558H1BuzLFmyBIvFwrhx4/D1\n9eXIkSN8++23LF68mBkzZgCOmwU0lDtYZWUln3zyCampqdy7dw9vb28WLVpESEiI2aU1ON7e3rXG\nO3fuzOHDh63HBw8eZO3ateTl5eHq6srYsWOJi4ujSRPdvOh/LSYmhp9++qnWtT/25cSJE8THx5Od\nnU2rVq0IDw9n0aJFtf5pcfnfSElJwWKxkJubS0lJCa1bt8bf359p06bRr18/m1z1x3y1DeWg3jja\nli1bSE1N5erVq5SVleHq6sqgQYOYN2+edSCvod44XlVVFevWrWPPnj3cvHkTd3d3pkyZwoQJE2zy\nHDELaCgXERERETGZ9pSLiIiIiJhMQ7mIiIiIiMk0lIuIiIiImExDuYiIiIiIyTSUi4iIiIiYTEO5\niIiIiIjJNJSLiIiIiJhMQ7mIiJgmJiaGYcOGmV2GiIjp9CcJRUTqmePHjzNp0qQ/XW/cuDHZ2dkO\nrEhERP6OhnIRkXpq1KhRDBkyxC7u5KSTpCIiTxsN5SIi9ZSvry9jxowxuwwREfkH9HaJiEgDVVhY\niLe3NwkJCaSlpTF69Gj69OnD0KFDSUhI4OHDh3afc/78eebMmUP//v3p06cP4eHhJCUl8ejRI7vc\n4uJiPvzwQ0JDQ/Hz8yM4OJipU6fyww8/2OVev36dRYsW8dJLL9G3b1+mT59Ofn5+nTxuEZGnkd4p\nFxGppx48eMDt27ft4k2bNqVVq1bW48OHD1NQUMDEiRNp3749hw8fZu3atfz222989NFH1rwzZ84Q\nExNDkyZNrLkZGRnEx8dz/vx5Vq1aZc0tLCzkjTfe4NatW4wZMwY/Pz8ePHhAVlYWmZmZDBw40Jpb\nXl5OdHQ0ffv2ZeHChRQWFrJlyxbi4uJIS0ujcePGdfQMiYg8PTSUi4jUUwkJCSQkJNjFhw4dyoYN\nG6zH58+fJyUlhd69ewMQHR3N3Llz2bVrF5GRkfj7+wOwcuVKqqqq2L59Oz4+PtbcBQsWkJaWxrhx\n4wgODgbg/fff58aNGyQnJzN48GCbr//48WOb4zt37jB9+nRiY2OtMVdXVz7++GMyMzPtPl9EpD7S\nUC4iUk9FRkYSFhZmF3d1dbU5DgkJsQ7kAI0aNWLGjBkcPHiQ9PR0/P39uXXrFqdOnWLEiBHWgbwm\nd/bs2ezbt4/09HSCg4O5e/cu3333HYMHD651oP7jhaZOTk52d4sZMGAAAFeuXNFQLiINgoZyEZF6\nqlu3boSEhPxtXo8ePexiPXv2BKCgoAB4sh3l9/Hf8/DwwMnJyZp79epVDMPA19f3H9XZoUMHXFxc\nbGLPPvssAHfv3v1H/4eIyL+dLvQUERFT/dWeccMwHFiJiIh5NJSLiDRweXl5drHc3FwAunTpAoC7\nu7tN/PcuXbrE48ePrbldu3alUaNG5OTk1FXJIiL1joZyEZEGLjMzk3PnzlmPDcMgOTkZgOHDhwPQ\nrl07AgICyMjI4OLFiza5n3/+OQAjRowAnmw9GTJkCEePHiUzM9Pu6+ndbxERe9pTLiJST2VnZ2Ox\nWGpdqxm2AXx8fJg8eTITJ07Ezc2NQ4cOkZmZyZgxYwgICLDmLVu2jJiYGCZOnEhUVBRubm5kZGTw\n/fffM2rUKOudVwCWL19OdnY2sbGxRERE0Lt3byorK8nKyqJz584sXry47h64iMi/kIZyEZF6Ki0t\njbS0tFrXDhw4YN3LPWzYMLp3786GDRvIz8+nXbt2xMXFERcXZ/M5ffr0Yfv27Xz66ads27aN8vJy\nunTpwttvv820adNscrt06cLXX39NYmIiR48exWKx0KZNG3x8fIiMjKybBywi8i/WyNB5RBGRBqmw\nsJDQ0FDmzp3LvHnzzC5HRKRB055yERERERGTaSgXERERETGZhnIREREREZNpT7mIiIiIiMn0TrmI\niIiIiMk0lIuIiIiImExDuYiIiIiIyTSUi4iIiIiYTEO5iIiIiIjJNJSLiIiIiJjsv9mzV04Y6Wzj\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxQAAAGaCAYAAACWpgUQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEVGLMVAQEREREVGL/T/HBaO4UEkQ/AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>micro</th>\n",
              "      <th>macro</th>\n",
              "      <th>weighted</th>\n",
              "      <th>samples</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Recall</th>\n",
              "      <td>0.586289</td>\n",
              "      <td>0.447095</td>\n",
              "      <td>0.586289</td>\n",
              "      <td>0.637535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Precision</th>\n",
              "      <td>0.576954</td>\n",
              "      <td>0.525727</td>\n",
              "      <td>0.565313</td>\n",
              "      <td>0.614437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>F_Score</th>\n",
              "      <td>0.581584</td>\n",
              "      <td>0.471192</td>\n",
              "      <td>0.566385</td>\n",
              "      <td>0.582441</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              micro     macro  weighted   samples\n",
              "Recall     0.586289  0.447095  0.586289  0.637535\n",
              "Precision  0.576954  0.525727  0.565313  0.614437\n",
              "F_Score    0.581584  0.471192  0.566385  0.582441"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKx6TVkeElch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}