{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PP-MM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rishi15-t/PP-MM/blob/master/PP_MM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZyaSvq7Poeg",
        "colab_type": "code",
        "outputId": "80c6d293-d60d-49ce-cc22-b01605a1d5ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daviCRfrPsDK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "dataset = pd.read_pickle('/content/drive/My Drive/dataset/w2v_vgg_embeddings.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lq5KYZXPtiv",
        "colab_type": "code",
        "outputId": "5bce30f5-b204-404c-bee7-bfdf101fb8d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def Train_Test_Val_Split(data , test_data_fraction = 0.3, val_data_fraction = 0.1) :\n",
        "    \n",
        "  \n",
        "    data_genres_one_hot_encoding = pd.DataFrame.from_items(zip(data['genres'].index, data['genres'].values)).T\n",
        "    Label_names = np.array(['Action', 'Adventure', 'Animation', 'Biography', 'Comedy', 'Crime',\n",
        "       'Documentary', 'Drama', 'Family', 'Fantasy', 'Film-Noir',\n",
        "       'History', 'Horror', 'Music', 'Musical', 'Mystery', 'Romance',\n",
        "       'Sci-Fi', 'Short', 'Sport', 'Thriller', 'War', 'Western'])\n",
        "    data_genres_one_hot_encoding.columns = Label_names\n",
        "    Data_train, Data_test, Labels_train, Labels_test = train_test_split(data, data_genres_one_hot_encoding, test_size = test_data_fraction)\n",
        "\n",
        "    Data_train, Data_val, Labels_train, Labels_val = train_test_split(Data_train, Labels_train, test_size = val_data_fraction)\n",
        "\n",
        "    Data_train = Data_train.reset_index(drop=True)\n",
        "    Data_test = Data_test.reset_index(drop=True)\n",
        "    Data_val = Data_val.reset_index(drop=True)\n",
        "    \n",
        "    Labels_train = torch.tensor(Labels_train.values)\n",
        "    Labels_test = torch.tensor(Labels_test.values)\n",
        "    Labels_val = torch.tensor(Labels_val.values)\n",
        "    \n",
        "    return (Data_train, Data_test, Data_val, Labels_train, Labels_test, Labels_val, Label_names)\n",
        "    \n",
        "Data_train, Data_test, Data_val, Labels_train_tensor, Labels_test_tensor, Labels_val_tensor, Label_names = Train_Test_Val_Split(dataset)\n",
        "\n",
        "Data_train_tensor_text = torch.tensor(Data_train['w2v_embeddings'])\n",
        "Data_test_tensor_text = torch.tensor(Data_test['w2v_embeddings'])\n",
        "Data_val_tensor_text = torch.tensor(Data_val['w2v_embeddings'])\n",
        "\n",
        "Data_train_tensor_image = torch.tensor(Data_train['vgg16_embeddings'])\n",
        "Data_test_tensor_image = torch.tensor(Data_test['vgg16_embeddings'])\n",
        "Data_val_tensor_image = torch.tensor(Data_val['vgg16_embeddings'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: from_items is deprecated. Please use DataFrame.from_dict(dict(items), ...) instead. DataFrame.from_dict(OrderedDict(items)) may be used to preserve the key order.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9oDVZe52y9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install git+https://github.com/uber/pyro.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDL0pDxw0Zsw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import IterableDataset, Dataset, DataLoader\n",
        "from torch import autograd, nn, tanh, optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "import h5py\n",
        "import numpy as np \n",
        "from pathlib import Path\n",
        "import pyro\n",
        "from pyro.distributions import Normal, Categorical, Laplace\n",
        "from pyro.infer import SVI, Trace_ELBO\n",
        "from pyro.optim import Adam\n",
        "\n",
        "\n",
        "# model\n",
        "\n",
        "class GMU_PP(nn.Module):\n",
        "\n",
        "    def __init__(self, num_maxout_units = 2, hidden_layer_size = 512, text_embeddings_size = 300, img_embeddings_size = 4096, num_labels = 23, hidden_activation = None, dropout = 0.1):\n",
        "\n",
        "        super(GMU_PP, self).__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.hidden_layer_size = hidden_layer_size\n",
        "        self.text_embeddings_size = text_embeddings_size\n",
        "        self.img_embeddings_size = img_embeddings_size\n",
        "\n",
        "        self.linear_h_text = torch.nn.Linear(text_embeddings_size, self.hidden_layer_size, bias = False)\n",
        "        self.linear_h_image = torch.nn.Linear(img_embeddings_size, self.hidden_layer_size, bias = False)\n",
        "        self.linear_z = torch.nn.Linear(text_embeddings_size + img_embeddings_size, self.hidden_layer_size, bias = False)\n",
        "\n",
        "        self.tanh = torch.nn.Tanh()\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.linear = torch.nn.Linear(self.hidden_layer_size, self.num_labels)\n",
        "        \n",
        "        #self.maxout = Maxout_MLP(self.hidden_layer_size, self.hidden_layer_size, dropout, num_maxout_units=num_maxout_units)\n",
        "\n",
        "        #Added one hidden layer in between GMU units and output layer for testing\n",
        "        self.linear_test = torch.nn.Linear(self.hidden_layer_size, self.hidden_layer_size)\n",
        "        self.relu_test = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, image_embeddings, text_embeddings):\n",
        "        \n",
        "        image_h = self.linear_h_image(image_embeddings)\n",
        "        image_h = self.tanh(image_h)\n",
        "        text_h = self.linear_h_text(text_embeddings)\n",
        "        text_h = self.tanh(text_h)\n",
        "        concat = torch.cat((image_embeddings, text_embeddings), 1)\n",
        "        z = self.linear_z(concat)\n",
        "        z = self.sigmoid(z)\n",
        "        gmu_output = z*image_h + (1-z)*text_h\n",
        "        \n",
        "        #maxout_mlp_output = self.maxout(gmu_output)\n",
        "\n",
        "        test = linear_test(gmu_output)\n",
        "        test = relu_test(test)\n",
        "        logits = self.linear(test)\n",
        "        \n",
        "        #logits = self.linear(maxout_mlp_output)\n",
        "        \n",
        "        if(self.training) :\n",
        "            return logits\n",
        "        else :\n",
        "            output = self.sigmoid(logits)\n",
        "            return output\n",
        "\n",
        "\n",
        "    # pyro model\n",
        "    def model(self, image_embeddings, text_embeddings, labels):\n",
        "\n",
        "        def model_dist(*shape):\n",
        "            loc = torch.zeros(*shape)\n",
        "            scale = torch.ones(*shape)\n",
        "            return Laplace(loc, scale)\n",
        "        \n",
        "        priors = {\n",
        "            'linear_h_text.weight': model_dist(self.text_embeddings_size, self.hidden_layer_size), #no bias\n",
        "            'linear_h_image.weight': model_dist(self.image_embeddings_size, self.hidden_layer_size), #no bias\n",
        "            'linear_z.weight': model_dist(self.text_embeddings_size + self.image_embeddings_size, self.hidden_layer_size), #no bias\n",
        "            'linear_test.weight': model_dist(self.hidden_layer_size, self.hidden_layer_size), 'linear_test.bias': model_dist(self.hidden_layer_size),\n",
        "            'linear.weight': model_dist(self.hidden_layer_size, self.num_labels), 'linear.bias': model_dist(self.hidden_layer_size)\n",
        "            }\n",
        "        \n",
        "        lifted_module = pyro.random_module(\"net\", net, priors)\n",
        "        lifted_reg_model = lifted_module()\n",
        "        lhat = lifted_reg_model(image_embeddings, text_embeddings)\n",
        "        pyro.sample(\"obs\", Categorical(logits=lhat), obs=labels)\n",
        "\n",
        "    # pyro guide\n",
        "    def guide(self, image_embeddings, text_embeddings, labels):\n",
        "\n",
        "        def infer_dist(name, *shape):\n",
        "            l = torch.empty(*shape, requires_grad=True)\n",
        "            s = torch.empty(*shape, requires_grad=True)\n",
        "            torch.nn.init.normal_(l, std=0.01)\n",
        "            torch.nn.init.normal_(s, std=0.01)\n",
        "            loc = pyro.param(name+\"_loc\", l)\n",
        "            scale = nn.functional.softplus(pyro.param(name+\"_scale\", s))\n",
        "            return Laplace(loc, scale)\n",
        "\n",
        "        dists = {\n",
        "            'linear_h_text.weight': infer_dist(\"W1\", self.text_embeddings_size, self.hidden_layer_size), #no bias\n",
        "            'linear_h_image.weight': infer_dist(\"W2\", self.image_embeddings_size, self.hidden_layer_size), #no bias\n",
        "            'linear_z.weight': infer_dist(\"W3\", self.text_embeddings_size + self.image_embeddings_size, self.hidden_layer_size), #no bias\n",
        "            'linear_test.weight': infer_dist(\"W4\", self.hidden_layer_size, self.hidden_layer_size), 'linear_test.bias': infer_dist(\"b4\", self.hidden_layer_size),\n",
        "            'linear.weight': infer_dist(\"W5\", self.hidden_layer_size, self.num_labels), 'linear.bias': infer_dist(\"b5\", self.hidden_layer_size)\n",
        "            }\n",
        "\n",
        "        lifted_module = pyro.random_module(\"net\", net, dists)\n",
        "        return lifted_module()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXrSeLOoxkZt",
        "colab_type": "code",
        "outputId": "417cebc7-0327-464f-ec16-f29ece07966a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "    import torch\n",
        "    from torch.nn import BCEWithLogitsLoss\n",
        "    from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "    from torch.optim.lr_scheduler import StepLR\n",
        "    from tqdm import tqdm, trange\n",
        "    from sklearn import metrics\n",
        "    import matplotlib.pyplot as plt\n",
        "    % matplotlib inline\n",
        "    import seaborn as sns\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import time\n",
        "    import datetime\n",
        "\n",
        "\n",
        "    def SetTrainDataloader_MM(Data_train_tensor_text, Data_train_tensor_image, Labels_train_tensor) :\n",
        "\n",
        "      train_dataset = TensorDataset(Data_train_tensor_text, Data_train_tensor_image, Labels_train_tensor)\n",
        "      train_sampler = RandomSampler(train_dataset)\n",
        "      train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size =  batch_size)\n",
        "      return(train_dataloader)\n",
        "\n",
        "\n",
        "    #source: https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "    def Plot_Training_Epoch_Loss(epoch_loss_set) :\n",
        "\n",
        "      sns.set(style='darkgrid')\n",
        "      sns.set(font_scale=1.5)\n",
        "      plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "      plt.plot( epoch_loss_set, 'b-o')\n",
        "      plt.title(\"Training loss\")\n",
        "      plt.xlabel(\"Epoch\")\n",
        "      plt.ylabel(\"Loss\")\n",
        "      plt.savefig('Training_Epoch_Loss.png',bbox_inches='tight')\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "    #source: https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "    def format_time(elapsed):\n",
        "      '''\n",
        "      Takes a time in seconds and returns a string hh:mm:ss\n",
        "      '''\n",
        "      # Round to the nearest second.\n",
        "      elapsed_rounded = int(round((elapsed)))\n",
        "      return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "    \n",
        "\n",
        "    def Train(train_dataloader, inference, epoch_loss_set) :\n",
        "\n",
        "      for _ in trange( epochs, desc=\"Epoch\"):\n",
        "        \n",
        "        net.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        # Measure how long the training epoch takes.\n",
        "        t0 = time.time()\n",
        "    \n",
        "        for step_num, batch_data in enumerate(train_dataloader):\n",
        "\n",
        "          # Progress update every 30 batches.\n",
        "          if step_num % 30 == 0 and not step_num == 0:\n",
        "            elapsed =  format_time(time.time() - t0)\n",
        "            print('  Batch : ',step_num, ' , Time elapsed : ',elapsed)\n",
        "\n",
        "          samples_image, samples_text, labels = tuple(t.to( device) for t in batch_data)\n",
        "          batch_loss = inference.step(samples_image.float(), samples_text.float(), labels)\n",
        "          epoch_loss += batch_loss\n",
        "\n",
        "        avg_epoch_loss = epoch_loss/len( train_dataloader)\n",
        "        print(\"\\nTrain loss for epoch: \",avg_epoch_loss)\n",
        "        print(\"\\nTraining epoch took: {:}\".format( format_time(time.time() - t0)))\n",
        "        epoch_loss_set.append(avg_epoch_loss)\n",
        "    \n",
        "      #torch.save( model.state_dict(), \"/content/drive/My Drive/dataset/model.pt\")\n",
        "      Plot_Training_Epoch_Loss(epoch_loss_set)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-952fa45ee472>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0minference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGMU_PP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGMU_PP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrace_ELBO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0mTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_loss_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-952fa45ee472>\u001b[0m in \u001b[0;36mTrain\u001b[0;34m(train_dataloader, inference, epoch_loss_set)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m       \u001b[0msamples_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m       \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyro/infer/svi.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# get loss and compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mparam_capture\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         params = set(site[\"value\"].unconstrained()\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyro/infer/trace_elbo.py\u001b[0m in \u001b[0;36mloss_and_grads\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# grab a trace from the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmodel_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide_trace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_traces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m             \u001b[0mloss_particle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msurrogate_loss_particle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_differentiable_loss_particle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_particle\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_particles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyro/infer/elbo.py\u001b[0m in \u001b[0;36m_get_traces\u001b[0;34m(self, model, guide, args, kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_particles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyro/infer/trace_elbo.py\u001b[0m in \u001b[0;36m_get_trace\u001b[0;34m(self, model, guide, args, kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \"\"\"\n\u001b[1;32m     52\u001b[0m         model_trace, guide_trace = get_importance_trace(\n\u001b[0;32m---> 53\u001b[0;31m             \"flat\", self.max_plate_nesting, model, guide, args, kwargs)\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_validation_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mcheck_if_enumerated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguide_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyro/infer/enum.py\u001b[0m in \u001b[0;36mget_importance_trace\u001b[0;34m(graph_type, max_plate_nesting, model, guide, args, kwargs, detach)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mrun\u001b[0m \u001b[0magainst\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \"\"\"\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mguide_trace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mguide_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyro/poutine/trace_messenger.py\u001b[0m in \u001b[0;36mget_trace\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mCalls\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mpoutine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mits\u001b[0m \u001b[0mtrace\u001b[0m \u001b[0minstead\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsngr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyro/poutine/trace_messenger.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                                       args=args, kwargs=kwargs)\n\u001b[1;32m    164\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: guide() missing 1 required positional argument: 'labels'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj8rNWGO_-hW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    #Parameters\n",
        "\n",
        "    net = GMU_PP(num_maxout_units = 2, hidden_layer_size = 512, hidden_activation = None, dropout = 0.5).cuda()\n",
        "    label_names = Label_names\n",
        "    num_labels = 23\n",
        "    batch_size = 256\n",
        "    learning_rate = 0.01\n",
        "    epochs = 50\n",
        "    sigmoid_thresh = 0.5\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    results = pd.DataFrame(0, index=['Recall','Precision','F_Score'], columns=['micro', 'macro', 'weighted', 'samples']).astype(float)\n",
        "    epoch_loss_set = []\n",
        "    train_dataloader =  SetTrainDataloader_MM(Data_train_tensor_image[0:10], Data_train_tensor_text[0:10], Labels_train_tensor[0:10])\n",
        "\n",
        "    # clear param store\n",
        "    pyro.clear_param_store()\n",
        "\n",
        "    # pyro svi, initialise losses\n",
        "    inference = SVI(GMU_PP.model, GMU_PP.guide, Adam({\"lr\": learning_rate}), loss=Trace_ELBO())\n",
        "\n",
        "    Train(train_dataloader, inference, epoch_loss_set)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}