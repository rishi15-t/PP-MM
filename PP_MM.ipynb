{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PP_MM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rishi15-t/PP-MM/blob/master/PP_MM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZyaSvq7Poeg",
        "colab_type": "code",
        "outputId": "35b78a77-7711-4ada-90a9-17e95ec0297a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daviCRfrPsDK",
        "colab_type": "code",
        "outputId": "ecf4ddef-1f40-4e1a-f2bc-4591464523fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        }
      },
      "source": [
        "import pandas as pd\n",
        "!pip install git+https://github.com/uber/pyro.git\n",
        "#dataset = pd.read_pickle('/content/drive/My Drive/dataset/dataset_gmu_paper.pkl')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/uber/pyro.git\n",
            "  Cloning https://github.com/uber/pyro.git to /tmp/pip-req-build-6uuvtpsp\n",
            "  Running command git clone -q https://github.com/uber/pyro.git /tmp/pip-req-build-6uuvtpsp\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl==1.2.1+a11f1110) (1.17.5)\n",
            "Requirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl==1.2.1+a11f1110) (3.1.0)\n",
            "Collecting pyro-api>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/c2/bc/6cdbd1929e32fff62a33592633c2cc0393c7f7739131ccc9c9c4e28ac8dd/pyro_api-0.1.1-py3-none-any.whl\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl==1.2.1+a11f1110) (1.4.0)\n",
            "Collecting tqdm>=4.36\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/47/55/fd9170ba08a1a64a18a7f8a18f088037316f2a41be04d2fe6ece5a653e8f/tqdm-4.43.0-py2.py3-none-any.whl (59kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyro-ppl\n",
            "  Building wheel for pyro-ppl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyro-ppl: filename=pyro_ppl-1.2.1+a11f1110-cp36-none-any.whl size=494336 sha256=39dcc90eb40005aaacce9d74e8171eaddf2383f4ded72f2cc733bf8d959f2a72\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dlzjito2/wheels/3c/10/ed/73d2332a097e2e9dc7d601ca2a99b0b4f9634e393474b78137\n",
            "Successfully built pyro-ppl\n",
            "Installing collected packages: pyro-api, tqdm, pyro-ppl\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed pyro-api-0.1.1 pyro-ppl-1.2.1+a11f1110 tqdm-4.43.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lq5KYZXPtiv",
        "colab_type": "code",
        "outputId": "1cfc4f17-ccea-4996-a32e-e265b29d268e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "from torch.nn.functional import softplus\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.optim import AdamW\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from tqdm import tqdm, trange\n",
        "from sklearn import metrics\n",
        "#!pip install transformers\n",
        "#from transformers import get_linear_schedule_with_warmup\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "#!pip install git+https://github.com/uber/pyro.git\n",
        "import pyro\n",
        "from pyro import poutine\n",
        "from pyro.distributions import Normal, Categorical, Laplace\n",
        "from pyro.infer import SVI, Trace_ELBO, TraceEnum_ELBO\n",
        "from pyro.optim import ClippedAdam\n",
        "\n",
        "\n",
        "'''def Train_Test_Val_Split(data , test_data_fraction = 0.3, val_data_fraction = 0.1) :\n",
        "    \n",
        "    mlb = MultiLabelBinarizer()\n",
        "    data_genres_one_hot_encoding = mlb.fit_transform(data['genres'])\n",
        "    Label_names = mlb.classes_\n",
        "    data_genres_one_hot_encoding = pd.DataFrame(data_genres_one_hot_encoding, columns = mlb.classes_)\n",
        "    Data_train, Data_test, Labels_train, Labels_test = train_test_split(data, data_genres_one_hot_encoding, test_size = test_data_fraction)\n",
        "    Labels_train = torch.tensor(Labels_train.values)\n",
        "    Labels_test = torch.tensor(Labels_test.values)\n",
        "    \n",
        "    Data_train, Data_val, Labels_train, Labels_val = train_test_split(Data_train, Labels_train, test_size = val_data_fraction)\n",
        "\n",
        "    Data_train = Data_train.reset_index(drop=True)\n",
        "    Data_test = Data_test.reset_index(drop=True)\n",
        "    Data_val = Data_val.reset_index(drop=True)\n",
        "    \n",
        "\n",
        "    return (Data_train, Data_test, Data_val, Labels_train, Labels_test, Labels_val, Label_names)\n",
        "    \n",
        "Data_train, Data_test, Data_val, Labels_train_tensor, Labels_test_tensor, Labels_val_tensor, Label_names = Train_Test_Val_Split(dataset)\n",
        "\n",
        "\n",
        "Data_train_tensor_text = torch.tensor(Data_train['w2v_embeddings'])\n",
        "Data_test_tensor_text = torch.tensor(Data_test['w2v_embeddings'])\n",
        "Data_val_tensor_text = torch.tensor(Data_val['w2v_embeddings'])\n",
        "\n",
        "Data_train_tensor_image = torch.tensor(Data_train['vgg16_embeddings'])\n",
        "Data_test_tensor_image = torch.tensor(Data_test['vgg16_embeddings'])\n",
        "Data_val_tensor_image = torch.tensor(Data_val['vgg16_embeddings'])'''"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"def Train_Test_Val_Split(data , test_data_fraction = 0.3, val_data_fraction = 0.1) :\\n    \\n    mlb = MultiLabelBinarizer()\\n    data_genres_one_hot_encoding = mlb.fit_transform(data['genres'])\\n    Label_names = mlb.classes_\\n    data_genres_one_hot_encoding = pd.DataFrame(data_genres_one_hot_encoding, columns = mlb.classes_)\\n    Data_train, Data_test, Labels_train, Labels_test = train_test_split(data, data_genres_one_hot_encoding, test_size = test_data_fraction)\\n    Labels_train = torch.tensor(Labels_train.values)\\n    Labels_test = torch.tensor(Labels_test.values)\\n    \\n    Data_train, Data_val, Labels_train, Labels_val = train_test_split(Data_train, Labels_train, test_size = val_data_fraction)\\n\\n    Data_train = Data_train.reset_index(drop=True)\\n    Data_test = Data_test.reset_index(drop=True)\\n    Data_val = Data_val.reset_index(drop=True)\\n    \\n\\n    return (Data_train, Data_test, Data_val, Labels_train, Labels_test, Labels_val, Label_names)\\n    \\nData_train, Data_test, Data_val, Labels_train_tensor, Labels_test_tensor, Labels_val_tensor, Label_names = Train_Test_Val_Split(dataset)\\n\\n\\nData_train_tensor_text = torch.tensor(Data_train['w2v_embeddings'])\\nData_test_tensor_text = torch.tensor(Data_test['w2v_embeddings'])\\nData_val_tensor_text = torch.tensor(Data_val['w2v_embeddings'])\\n\\nData_train_tensor_image = torch.tensor(Data_train['vgg16_embeddings'])\\nData_test_tensor_image = torch.tensor(Data_test['vgg16_embeddings'])\\nData_val_tensor_image = torch.tensor(Data_val['vgg16_embeddings'])\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pgLuDzGK1nR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Data_train_tensor_text = torch.load(\"/content/drive/My Drive/dataset/Data_train_tensor_text.pt\")\n",
        "Data_test_tensor_text = torch.load( \"/content/drive/My Drive/dataset/Data_test_tensor_text.pt\")\n",
        "Data_val_tensor_text = torch.load( \"/content/drive/My Drive/dataset/Data_val_tensor_text.pt\")\n",
        "Data_train_tensor_image = torch.load( \"/content/drive/My Drive/dataset/Data_train_tensor_image.pt\")\n",
        "Data_test_tensor_image = torch.load( \"/content/drive/My Drive/dataset/Data_test_tensor_image.pt\")\n",
        "Data_val_tensor_image = torch.load( \"/content/drive/My Drive/dataset/Data_val_tensor_image.pt\")\n",
        "Labels_train_tensor = torch.load(\"/content/drive/My Drive/dataset/Labels_train_tensor.pt\")\n",
        "Labels_test_tensor = torch.load(\"/content/drive/My Drive/dataset/Labels_test_tensor.pt\")\n",
        "Labels_val_tensor = torch.load(\"/content/drive/My Drive/dataset/Labels_val_tensor.pt\")\n",
        "\n",
        "Label_names = np.array(['Action', 'Adventure', 'Animation', 'Biography', 'Comedy', 'Crime',\n",
        "       'Documentary', 'Drama', 'Family', 'Fantasy', 'Film-Noir',\n",
        "       'History', 'Horror', 'Music', 'Musical', 'Mystery', 'Romance',\n",
        "       'Sci-Fi', 'Short', 'Sport', 'Thriller', 'War', 'Western'],\n",
        "      dtype=object)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6TKgAUIPwu0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Neural networks\n",
        "\n",
        "\n",
        "#source: https://github.com/Duncanswilson/maxout-pytorch/blob/master/maxout_pytorch.ipynb\n",
        "class ListModule(object):\n",
        "    def __init__(self, module, prefix, *args):\n",
        "        self.module = module\n",
        "        self.prefix = prefix\n",
        "        self.num_module = 0\n",
        "        for new_module in args:\n",
        "            self.append(new_module)\n",
        "\n",
        "    def append(self, new_module):\n",
        "        if not isinstance(new_module, nn.Module):\n",
        "            raise ValueError('Not a Module')\n",
        "        else:\n",
        "            self.module.add_module(self.prefix + str(self.num_module), new_module)\n",
        "            self.num_module += 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_module\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        if i < 0 or i >= self.num_module:\n",
        "            raise IndexError('Out of bound')\n",
        "        return getattr(self.module, self.prefix + str(i))\n",
        "\n",
        "\n",
        "class Maxout_MLP(nn.Module):\n",
        "    \n",
        "    def __init__(self, hidden_layer_size1, hidden_layer_size2, dropout, num_labels = 23, num_maxout_units=2):\n",
        "        \n",
        "        super(Maxout_MLP, self).__init__()\n",
        "        self.fc1_list = ListModule(self, \"fc1_\")\n",
        "        self.fc2_list = ListModule(self, \"fc2_\")\n",
        "        self.hidden_layer_size1 = hidden_layer_size1\n",
        "        self.hidden_layer_size2 = hidden_layer_size2\n",
        "        for _ in range(num_maxout_units):\n",
        "            self.fc1_list.append(nn.Linear(self.hidden_layer_size1, self.hidden_layer_size2))\n",
        "            self.fc2_list.append(nn.Linear(self.hidden_layer_size2, self.hidden_layer_size2))\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "        self.linear = torch.nn.Linear(self.hidden_layer_size2, num_labels)  #PP_MM_pattern 4\n",
        "\n",
        "        self.bn0 = nn.BatchNorm1d(self.hidden_layer_size1)\n",
        "        self.bn1 = nn.BatchNorm1d(self.hidden_layer_size2)\n",
        "        self.bn2 = nn.BatchNorm1d(self.hidden_layer_size2)\n",
        "\n",
        "    def forward(self, x): \n",
        "        \n",
        "        x = x.view(-1, self.hidden_layer_size1)\n",
        "        x = self.bn0(x)\n",
        "        x = self.maxout(x, self.fc1_list)\n",
        "        x = self.bn1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.maxout(x, self.fc2_list)\n",
        "        x = self.bn2(x)\n",
        "        '''\n",
        "        PP_MM_pattern 4\n",
        "        '''\n",
        "        logits = self.linear(x)  \n",
        "        if(self.training) :     \n",
        "            return logits\n",
        "        else :\n",
        "            output = self.sigmoid(logits)\n",
        "            return output\n",
        "\n",
        "    def maxout(self, x, layer_list):\n",
        "        \n",
        "        max_output = layer_list[0](x)\n",
        "        for _, layer in enumerate(layer_list, start=1):\n",
        "            max_output = torch.max(max_output, layer(x))\n",
        "        return max_output\n",
        "\n",
        "\n",
        "class GMU(nn.Module):\n",
        "\n",
        "    def __init__(self, num_maxout_units = 2, hidden_layer_size = 512, text_embeddings_size = 300, img_embeddings_size = 4096, num_labels = 23, hidden_activation = None, dropout = 0.1):\n",
        "\n",
        "        super(GMU, self).__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.hidden_layer_size = hidden_layer_size\n",
        "\n",
        "        self.linear_h_text = torch.nn.Linear(text_embeddings_size, self.hidden_layer_size)\n",
        "        self.linear_h_image = torch.nn.Linear(img_embeddings_size, self.hidden_layer_size)\n",
        "        self.linear_z = torch.nn.Linear(text_embeddings_size + img_embeddings_size, self.hidden_layer_size)\n",
        "       \n",
        "        self.tanh = torch.nn.Tanh()\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        self.bn0 = nn.BatchNorm1d(img_embeddings_size)\n",
        "        self.bn1 = nn.BatchNorm1d(text_embeddings_size)\n",
        "        self.bn2 = nn.BatchNorm1d(text_embeddings_size + img_embeddings_size)\n",
        "\n",
        "    def forward(self, text_embeddings, image_embeddings):\n",
        "        \n",
        "        image_embeddings = self.bn0(image_embeddings)\n",
        "        image_h = self.linear_h_image(image_embeddings)\n",
        "        image_h = self.tanh(image_h)\n",
        "\n",
        "        text_embeddings = self.bn1(text_embeddings)\n",
        "        text_h = self.linear_h_text(text_embeddings)\n",
        "        text_h = self.tanh(text_h)\n",
        "\n",
        "        concat = torch.cat((image_embeddings, text_embeddings), 1)\n",
        "        concat = self.bn2(concat)\n",
        "        z = self.linear_z(concat)\n",
        "        z = self.sigmoid(z)\n",
        "        gmu_output = z*image_h + (1-z)*text_h\n",
        "        \n",
        "        attendout = self.tanh(gmu_output)  \n",
        "\n",
        "        return attendout   \n",
        "\n",
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eipBUThdV-ym",
        "colab": {}
      },
      "source": [
        "# Pyro module defined only over the GMU NN\n",
        "\n",
        "class GMU_Pyro():\n",
        "\n",
        "    def __init__(self, model):\n",
        "\n",
        "        self.model = model \n",
        "    \n",
        "    def gmu_model(self, text_embeddings, image_embeddings, labels):\n",
        "\n",
        "        pyro.module(\"GMU\", self.model)\n",
        "        #global x_label\n",
        "\n",
        "        linear_h_text_prior_w = Laplace(loc=torch.ones_like(self.model.linear_h_text.weight), scale=torch.ones_like(self.model.linear_h_text.weight))\n",
        "        linear_h_text_prior_b = Laplace(loc=torch.ones_like(self.model.linear_h_text.bias), scale=torch.ones_like(self.model.linear_h_text.bias))\n",
        "        linear_h_image_prior_w = Laplace(loc=torch.ones_like(self.model.linear_h_image.weight), scale=torch.ones_like(self.model.linear_h_image.weight))\n",
        "        linear_h_image_prior_b = Laplace(loc=torch.ones_like(self.model.linear_h_image.bias), scale=torch.ones_like(self.model.linear_h_image.bias))\n",
        "        linear_z_prior_w = Laplace(loc=torch.ones_like(self.model.linear_z.weight), scale=torch.ones_like(self.model.linear_z.weight))\n",
        "        linear_z_prior_b = Laplace(loc=torch.ones_like(self.model.linear_z.bias), scale=torch.ones_like(self.model.linear_z.bias))\n",
        "\n",
        "        priors = {\n",
        "            'linear_h_text.weight': linear_h_text_prior_w, 'linear_h_text.bias': linear_h_text_prior_b,\n",
        "            'linear_h_image.weight': linear_h_image_prior_w, 'linear_h_image.bias': linear_h_image_prior_b,\n",
        "            'linear_z.weight': linear_z_prior_w, 'linear_z.bias': linear_z_prior_b\n",
        "            }\n",
        "\n",
        "        lifted_module = pyro.random_module(\"module\", self.model, priors)\n",
        "        lifted_reg_model = lifted_module()\n",
        "\n",
        "        lhat = torch.sigmoid(lifted_reg_model(text_embeddings, image_embeddings))\n",
        "        pyro.sample(\"obs\", Categorical(logits=lhat), obs=labels)\n",
        "\n",
        "    \n",
        "    def gmu_guide(self, text_embeddings, image_embeddings, labels):\n",
        "\n",
        "        pyro.module(\"GMU\", self.model)\n",
        "\n",
        "        # linear_h_text\n",
        "        # weight\n",
        "        lwlinear_h_text = torch.empty_like(self.model.linear_h_text.weight)\n",
        "        swlinear_h_text = torch.empty_like(self.model.linear_h_text.weight)\n",
        "        torch.nn.init.normal_(lwlinear_h_text, std=0.001)\n",
        "        torch.nn.init.normal_(swlinear_h_text, std=0.01)\n",
        "        linear_h_text_loc_param_w_l = pyro.param(\"linear_h_text_loc_w_l\", lwlinear_h_text)\n",
        "        linear_h_text_loc_param_w_s = softplus(pyro.param(\"linear_h_text_loc_w_s\", swlinear_h_text))\n",
        "        linear_h_text_prior_w = Laplace(loc=linear_h_text_loc_param_w_l, scale=linear_h_text_loc_param_w_s)\n",
        "        # bias\n",
        "        lblinear_h_text = torch.empty_like(self.model.linear_h_text.bias)\n",
        "        sblinear_h_text = torch.empty_like(self.model.linear_h_text.bias)\n",
        "        torch.nn.init.normal_(lblinear_h_text, std=0.001)\n",
        "        torch.nn.init.normal_(sblinear_h_text, std=0.01)\n",
        "        linear_h_text_loc_param_b_l = pyro.param(\"linear_h_text_loc_b_l\", lblinear_h_text)\n",
        "        linear_h_text_loc_param_b_s = softplus(pyro.param(\"linear_h_text_loc_b_s\", sblinear_h_text))\n",
        "        linear_h_text_prior_b = Laplace(loc=linear_h_text_loc_param_b_l, scale=linear_h_text_loc_param_b_s)\n",
        "\n",
        "        # linear_h_image\n",
        "        # weight\n",
        "        lwlinear_h_image = torch.empty_like(self.model.linear_h_image.weight)\n",
        "        swlinear_h_image = torch.empty_like(self.model.linear_h_image.weight)\n",
        "        torch.nn.init.normal_(lwlinear_h_image, std=0.001)\n",
        "        torch.nn.init.normal_(swlinear_h_image, std=0.01)\n",
        "        linear_h_image_loc_param_w_l = pyro.param(\"linear_h_image_loc_w_l\", lwlinear_h_image)\n",
        "        linear_h_image_loc_param_w_s = softplus(pyro.param(\"linear_h_image_loc_w_s\", swlinear_h_image))\n",
        "        linear_h_image_prior_w = Laplace(loc=linear_h_image_loc_param_w_l, scale=linear_h_image_loc_param_w_s)\n",
        "        # bias\n",
        "        lblinear_h_image = torch.empty_like(self.model.linear_h_image.bias)\n",
        "        sblinear_h_image = torch.empty_like(self.model.linear_h_image.bias)\n",
        "        torch.nn.init.normal_(lblinear_h_image, std=0.001)\n",
        "        torch.nn.init.normal_(sblinear_h_image, std=0.01)\n",
        "        linear_h_image_loc_param_b_l = pyro.param(\"linear_h_image_loc_b_l\", lblinear_h_image)\n",
        "        linear_h_image_loc_param_b_s = softplus(pyro.param(\"linear_h_image_loc_b_s\", sblinear_h_image))\n",
        "        linear_h_image_prior_b = Laplace(loc=linear_h_image_loc_param_b_l, scale=linear_h_image_loc_param_b_s)\n",
        "\n",
        "        # linear_z\n",
        "        # weight\n",
        "        lwlinear_z = torch.empty_like(self.model.linear_z.weight)\n",
        "        swlinear_z = torch.empty_like(self.model.linear_z.weight)\n",
        "        torch.nn.init.normal_(lwlinear_z, std=0.001)\n",
        "        torch.nn.init.normal_(swlinear_z, std=0.01)\n",
        "        linear_z_loc_param_w_l = pyro.param(\"linear_z_loc_w_l\", lwlinear_z)\n",
        "        linear_z_loc_param_w_s = softplus(pyro.param(\"linear_z_loc_w_s\", swlinear_z))\n",
        "        linear_z_prior_w = Laplace(loc=linear_z_loc_param_w_l, scale=linear_z_loc_param_w_s)\n",
        "        # bias\n",
        "        lblinear_z = torch.empty_like(self.model.linear_z.bias)\n",
        "        sblinear_z = torch.empty_like(self.model.linear_z.bias)\n",
        "        torch.nn.init.normal_(lblinear_z, std=0.001)\n",
        "        torch.nn.init.normal_(sblinear_z, std=0.01)\n",
        "        linear_z_loc_param_b_l = pyro.param(\"linear_z_loc_b_l\", lblinear_z)\n",
        "        linear_z_loc_param_b_s = softplus(pyro.param(\"linear_z_loc_b_s\", sblinear_z))\n",
        "        linear_z_prior_b = Laplace(loc=linear_z_loc_param_b_l, scale=linear_z_loc_param_b_s)\n",
        "\n",
        "        priors = {\n",
        "        'linear_h_text.weight': linear_h_text_prior_w, 'linear_h_text.bias': linear_h_text_prior_b,\n",
        "        'linear_h_image.weight': linear_h_image_prior_w, 'linear_h_image.bias': linear_h_image_prior_b,\n",
        "        'linear_z.weight': linear_z_prior_w, 'linear_z.bias': linear_z_prior_b\n",
        "        }\n",
        "\n",
        "        lifted_module = pyro.random_module(\"module\", self.model, priors)\n",
        "\n",
        "        return lifted_module() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV4XuUhWPy-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Train-Validation-Test\n",
        "\n",
        "class Training_Testing_MM():\n",
        "\n",
        "    def __init__(self, Data_train_tensor_text, Data_train_tensor_image, Labels_train_tensor, \n",
        "                 Data_test_tensor_text, Data_test_tensor_image, Labels_test_tensor, \n",
        "                 Data_val_tensor_text, Data_val_tensor_image, Labels_val_tensor,\n",
        "                 Label_names = None, hidden_layer_size = 512, num_maxout_units = 2, weight_decay= 0.1, reg_param = 0.1, scheduler_step_size = 30, scheduler_lr_fraction = 0.8,\n",
        "                 hidden_activation = \"tanh\", batch_size = 32, epochs = 10, sigmoid_thresh = 0.2, learning_rate = 2e-5, num_labels = 23, dropout = 0.1, max_norm = 5):\n",
        "\n",
        "\n",
        "      self.gmu = GMU(num_maxout_units = num_maxout_units, hidden_layer_size = hidden_layer_size, hidden_activation = hidden_activation, dropout = dropout).cuda()\n",
        "      self.mlp = Maxout_MLP(hidden_layer_size, hidden_layer_size, dropout = dropout, num_labels = num_labels, num_maxout_units = num_maxout_units).cuda()\n",
        "      self.pyro = GMU_Pyro(model = self.gmu)\n",
        "      #self.inference = SVI(self.pyro.gmu_model, self.pyro.gmu_guide, ClippedAdam({\"lr\": learning_rate}), loss = self.custom_mc_elbo)\n",
        "      #self.inference = SVI(self.pyro.gmu_model, self.pyro.gmu_guide, ClippedAdam({\"lr\": learning_rate}), loss = Trace_ELBO())\n",
        "      #self.inference = SVI(self.pyro.gmu_model, self.pyro.gmu_guide, ClippedAdam({\"lr\": learning_rate}), loss = self.simple_elbo_kl_annealing)\n",
        "      self.inference = SVI(self.pyro.gmu_model, self.pyro.gmu_guide, ClippedAdam({\"lr\": learning_rate}), loss = self.custom_elbo_with_wd)\n",
        "      self.label_names = Label_names\n",
        "      self.num_labels = num_labels\n",
        "      self.batch_size = batch_size\n",
        "      self.learning_rate = learning_rate\n",
        "      self.max_norm = max_norm\n",
        "      self.epochs = epochs\n",
        "      self.sigmoid_thresh = sigmoid_thresh\n",
        "      self.scheduler_step_size = scheduler_step_size\n",
        "      self.scheduler_lr_fraction = scheduler_lr_fraction\n",
        "      self.weight_decay = weight_decay\n",
        "      self.reg_param = reg_param\n",
        "      self.optimizer = self.SetOptimizer()\n",
        "      self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "      self.results = pd.DataFrame(0, index=['Recall','Precision','F_Score'], columns=['micro', 'macro', 'weighted', 'samples']).astype(float)\n",
        "      self.epoch_loss_set = []\n",
        "      self.epoch_gmu_loss_set = []\n",
        "      self.train_dataloader = self.SetTrainDataloader_MM(Data_train_tensor_text, Data_train_tensor_image, Labels_train_tensor)\n",
        "      self.test_dataloader = self.SetTestDataloader_MM(Data_test_tensor_text, Data_test_tensor_image, Labels_test_tensor) \n",
        "      self.scheduler = self.SetScheduler()\n",
        "\n",
        "      self.val_accuracy_set = [] \n",
        "      self.val_dataloader = self.SetValDataloader_MM(Data_val_tensor_text, Data_val_tensor_image, Labels_val_tensor)\n",
        "      self.class_wise_metrics = None\n",
        "      self.predictions = None\n",
        "\n",
        "\n",
        "    # custom Elbo\n",
        "    def custom_mc_elbo(self, model, guide, *args):\n",
        "      guide_trace = poutine.trace(guide).get_trace(*args)\n",
        "      model_trace = poutine.trace(poutine.replay(model, trace=guide_trace)).get_trace(*args)\n",
        "      elbo = model_trace.log_prob_sum() - guide_trace.log_prob_sum()\n",
        "      return -elbo\n",
        "\n",
        "\n",
        "    # custom Elbo with WD\n",
        "    def custom_elbo_with_wd(self, model, guide, *args):\n",
        "        guide_trace = poutine.trace(guide).get_trace(*args)\n",
        "        model_trace = poutine.trace(poutine.replay(model, trace=guide_trace)).get_trace(*args)\n",
        "        logm = model_trace.log_prob_sum()\n",
        "        logg = guide_trace.log_prob_sum()\n",
        "        wds = 0.01\n",
        "        wd = 0.\n",
        "        for node in model_trace.nodes.values():\n",
        "            if node[\"type\"] == \"param\":\n",
        "                wd = wd + wds * torch.sum(node[\"value\"])\n",
        "        for node in guide_trace.nodes.values():\n",
        "            if node[\"type\"] == \"param\":\n",
        "                wd = wd + wds * torch.sum(node[\"value\"])\n",
        "        #mixmod_trace = mmd(model_trace, guide_trace)\n",
        "        elbo = logm - logg + wd\n",
        "        return -elbo\n",
        "\n",
        "\n",
        "    def simple_elbo_kl_annealing(self, model, guide, *args):\n",
        "        # get the annealing factor and latents to anneal from the keyword\n",
        "        # arguments passed to the model and guide\n",
        "        annealing_factor = 2.4\n",
        "        latents_to_anneal = [\"my_latent\"]\n",
        "        # run the guide and replay the model against the guide\n",
        "        guide_trace = poutine.trace(guide).get_trace(*args)\n",
        "        model_trace = poutine.trace(poutine.replay(model, trace=guide_trace)).get_trace(*args)\n",
        "\n",
        "        elbo = 0.0\n",
        "        # loop through all the sample sites in the model and guide trace and\n",
        "        # construct the loss; note that we scale all the log probabilities of\n",
        "        # samples sites in `latents_to_anneal` by the factor `annealing_factor`\n",
        "        for site in model_trace.nodes.values():\n",
        "            if site[\"type\"] == \"sample\":\n",
        "                factor = annealing_factor if site[\"name\"] in latents_to_anneal else 1.0\n",
        "                elbo = elbo + factor * site[\"fn\"].log_prob(site[\"value\"]).sum()\n",
        "        for site in guide_trace.nodes.values():\n",
        "            if site[\"type\"] == \"sample\":\n",
        "                factor = annealing_factor if site[\"name\"] in latents_to_anneal else 1.0\n",
        "                elbo = elbo - factor * site[\"fn\"].log_prob(site[\"value\"]).sum()\n",
        "        return -elbo\n",
        "\n",
        "\n",
        "    def L2_Regularizer(self):\n",
        "      reg_loss = 0.0\n",
        "      for param in self.gmu.parameters():\n",
        "          reg_loss = reg_loss + param.pow(2.0).sum()\n",
        "      return self.reg_param*reg_loss\n",
        "\n",
        "\n",
        "    def SetOptimizer(self) :\n",
        "\n",
        "      optimizer = AdamW(self.mlp.parameters(), lr=self.learning_rate,  eps = 1e-6, weight_decay=self.weight_decay)\n",
        "      #optimizer = Adam(self.mlp.parameters(), lr=self.learning_rate,  eps = 1e-6, weight_decay=self.weight_decay)\n",
        "      return(optimizer)\n",
        "\n",
        "    \n",
        "\n",
        "    def SetScheduler(self) :\n",
        "\n",
        "      '''\n",
        "      scheduler = get_linear_schedule_with_warmup(self.optimizer, num_warmup_steps = 10, \n",
        "                                                 num_training_steps = self.epochs*len(self.train_dataloader))\n",
        "      '''\n",
        "      scheduler = StepLR(self.optimizer, step_size = self.scheduler_step_size, gamma = self.scheduler_lr_fraction)\n",
        "      return(scheduler) \n",
        "\n",
        "\n",
        "\n",
        "    def Get_Metrics(self, actual, predicted) :\n",
        "\n",
        "      #acc = metrics.accuracy_score(actual, predicted)\n",
        "      #hamming = metrics.hamming_loss(actual, predicted)\n",
        "      #(metrics.roc_auc_score(actual, predicted, average=average)\n",
        "      averages = ('micro', 'macro', 'weighted', 'samples')\n",
        "      for average in averages:\n",
        "          precision, recall, fscore, _ = metrics.precision_recall_fscore_support(actual, predicted, average=average)\n",
        "          self.results[average]['Recall'] += recall\n",
        "          self.results[average]['Precision'] += precision\n",
        "          self.results[average]['F_Score'] += fscore\n",
        "\n",
        "\n",
        "\n",
        "    #source: https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "    def Plot_Training_Epoch_Loss(self) :\n",
        "\n",
        "      sns.set(style='darkgrid')\n",
        "      sns.set(font_scale=1.5)\n",
        "      plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "      plt.plot(self.epoch_loss_set, 'b-o')\n",
        "      plt.title(\"Training loss\")\n",
        "      plt.xlabel(\"Epoch\")\n",
        "      plt.ylabel(\"Loss\")\n",
        "      plt.savefig('Training_Epoch_Loss.png',bbox_inches='tight')\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "    def Plot_Training_Epoch_SVI_Loss(self) :\n",
        "\n",
        "      sns.set(style='darkgrid')\n",
        "      sns.set(font_scale=1.5)\n",
        "      plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "      plt.plot(self.epoch_gmu_loss_set, 'b-o')\n",
        "      plt.title(\"Training loss (SVI)\")\n",
        "      plt.xlabel(\"Epoch\")\n",
        "      plt.ylabel(\"Loss\")\n",
        "      plt.savefig('Training_Epoch_SVI_Loss.png',bbox_inches='tight')\n",
        "      plt.show()\n",
        "\n",
        "    \n",
        "    def Plot_Training_Epoch_Accuracy(self) :\n",
        "\n",
        "      sns.set(style='darkgrid')\n",
        "      sns.set(font_scale=1.5)\n",
        "      plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "      plt.plot(self.val_accuracy_set, 'b-o')\n",
        "      plt.title(\"Weighted F1 Score\")\n",
        "      plt.xlabel(\"Epoch\")\n",
        "      plt.ylabel(\"Validation Accuracy\")\n",
        "      plt.savefig('Training_Validation_Accuracy.png',bbox_inches='tight')\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "    #source: https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "    def format_time(self, elapsed):\n",
        "      '''\n",
        "      Takes a time in seconds and returns a string hh:mm:ss\n",
        "      '''\n",
        "      # Round to the nearest second.\n",
        "      elapsed_rounded = int(round((elapsed)))\n",
        "      return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "\n",
        "    def SetTrainDataloader_MM(self, Data_train_tensor_text, Data_train_tensor_image, Labels_train_tensor) :\n",
        "\n",
        "      train_dataset = TensorDataset(Data_train_tensor_text, Data_train_tensor_image, Labels_train_tensor)\n",
        "      train_sampler = RandomSampler(train_dataset)\n",
        "      train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size = self.batch_size)\n",
        "      return(train_dataloader)\n",
        "\n",
        "\n",
        "    def SetTestDataloader_MM(self, Data_test_tensor_text, Data_test_tensor_image, Labels_test_tensor) :\n",
        "      \n",
        "      test_dataset = TensorDataset(Data_test_tensor_text, Data_test_tensor_image, Labels_test_tensor)\n",
        "      test_sampler = SequentialSampler(test_dataset)\n",
        "      #test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size = self.batch_size)\n",
        "      test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size = Data_test_tensor_text.shape[0])\n",
        "      return(test_dataloader)\n",
        "\n",
        "    \n",
        "    def SetValDataloader_MM(self, Data_val_tensor_text, Data_val_tensor_image, Labels_val_tensor) :\n",
        "      \n",
        "      val_dataset = TensorDataset(Data_val_tensor_text, Data_val_tensor_image, Labels_val_tensor)\n",
        "      val_sampler = SequentialSampler(val_dataset)\n",
        "      #test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size = self.batch_size)\n",
        "      val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size = Data_val_tensor_text.shape[0])\n",
        "      return(val_dataloader)\n",
        "\n",
        "   \n",
        "    def Train(self) :\n",
        "      \n",
        "      # clear param store\n",
        "      pyro.clear_param_store()\n",
        "      \n",
        "      for _ in trange(self.epochs, desc=\"Epoch\"):\n",
        "        \n",
        "        self.gmu.train()\n",
        "        self.mlp.train()\n",
        "        epoch_loss = 0\n",
        "        epoch_gmu_loss = 0\n",
        "\n",
        "        # Measure how long the training epoch takes.\n",
        "        t0 = time.time()\n",
        "    \n",
        "        for step_num, batch_data in enumerate(self.train_dataloader):\n",
        "\n",
        "          # Progress update every 30 batches.\n",
        "          if step_num % 30 == 0 and not step_num == 0:\n",
        "            elapsed = self.format_time(time.time() - t0)\n",
        "            print('  Batch : ',step_num, ' , Time elapsed : ',elapsed)\n",
        "\n",
        "          samples_text, samples_image, labels = tuple(t.to(self.device) for t in batch_data)\n",
        "          self.optimizer.zero_grad()\n",
        "\n",
        "          ##### Pyro - GMU #####\n",
        "          gmu_loss = self.inference.step(samples_text.float(), samples_image.float(), labels.t()) #+ self.L2_Regularizer()\n",
        "          attendout = self.gmu(samples_text, samples_image)\n",
        "          #epoch_gmu_loss += gmu_loss.detach().cpu().numpy()\n",
        "          epoch_gmu_loss += gmu_loss\n",
        "\n",
        "          ##### MLP ####\n",
        "          logits = self.mlp(attendout)\n",
        "          loss_fct = BCEWithLogitsLoss()\n",
        "          batch_loss = loss_fct(logits.view(-1, self.num_labels).float(), labels.view(-1, self.num_labels).float())\n",
        "          batch_loss.backward()\n",
        "          clip_grad_norm_(self.mlp.parameters(), norm_type = 2, max_norm = self.max_norm)\n",
        "          self.optimizer.step()\n",
        "          self.scheduler.step()\n",
        "          epoch_loss += batch_loss.item()\n",
        "\n",
        "        avg_epoch_loss = epoch_loss/len(self.train_dataloader)\n",
        "        avg_epoch_gmu_loss = epoch_gmu_loss/len(self.train_dataloader)\n",
        "        print(\"\\nTrain loss for epoch: \",avg_epoch_loss)\n",
        "        print(\"\\nTrain loss for epoch (gmu): \",avg_epoch_gmu_loss)\n",
        "        print(\"\\nTraining epoch took: {:}\".format(self.format_time(time.time() - t0)))\n",
        "        self.epoch_loss_set.append(avg_epoch_loss)\n",
        "        self.epoch_gmu_loss_set.append(avg_epoch_gmu_loss)\n",
        "\n",
        "        \n",
        "        #Validation on the epoch\n",
        "        self.mlp.eval()\n",
        "        self.gmu.eval()\n",
        "        epoch_f1_score = 0\n",
        "\n",
        "        for batch_data in self.val_dataloader:\n",
        "          samples_text, samples_image, labels = tuple(t.to(self.device) for t in batch_data)\n",
        "          with torch.no_grad():\n",
        "            attendout = self.gmu(samples_text.float(), samples_image.float())\n",
        "            output = self.mlp(attendout)\n",
        "\n",
        "          threshold = torch.Tensor([self.sigmoid_thresh]).to(self.device)\n",
        "          predictions = (output > threshold).int()\n",
        "\n",
        "          predictions = predictions.detach().cpu().numpy()\n",
        "          labels = labels.to('cpu').numpy()\n",
        "      \n",
        "          weighted_f_score = metrics.f1_score(labels,predictions,average=\"weighted\")\n",
        "          epoch_f1_score += weighted_f_score\n",
        "\n",
        "        avg_val_f1_score = epoch_f1_score/len(self.val_dataloader)\n",
        "        print(\"\\nWeighted F1 score for epoch: \",avg_val_f1_score,\"\\n\")\n",
        "        self.val_accuracy_set.append(avg_val_f1_score)\n",
        "        \n",
        "\n",
        "      #torch.save(self.mlp.state_dict(), \"/content/drive/My Drive/dataset/model.pt\")\n",
        "      self.Plot_Training_Epoch_Loss()\n",
        "      self.Plot_Training_Epoch_SVI_Loss()\n",
        "      self.Plot_Training_Epoch_Accuracy()\n",
        "   \n",
        "\n",
        "    def Test(self) :\n",
        "\n",
        "      # Put model in evaluation mode to evaluate loss on the test set\n",
        "      self.mlp.eval()\n",
        "      self.gmu.eval()\n",
        "\n",
        "      for batch_data in self.test_dataloader:\n",
        "  \n",
        "        samples_text, samples_image, labels = tuple(t.to(self.device) for t in batch_data)\n",
        "      \n",
        "        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "        # Forward pass, calculate logit predictions\n",
        "        with torch.no_grad():\n",
        "            attendout = self.gmu(samples_text.float(), samples_image.float())\n",
        "            output = self.mlp(attendout)\n",
        "\n",
        "        threshold = torch.Tensor([self.sigmoid_thresh]).to(self.device)\n",
        "        predictions = (output > threshold).int()\n",
        "\n",
        "        # Move preds and labels to CPU\n",
        "        predictions = predictions.detach().cpu().numpy()\n",
        "        labels = labels.to('cpu').numpy()\n",
        "\n",
        "        self.predictions = predictions\n",
        "        self.Get_Metrics(labels, predictions)\n",
        "        self.class_wise_metrics = metrics.classification_report(labels, predictions, target_names= list(self.label_names))\n",
        "        \n",
        "    \n",
        "      self.results = self.results/len(self.test_dataloader)\n",
        "      #print(\"Test data metrics : \\n\")\n",
        "\n",
        "      #print(\"\\nGenres with no predicted samples : \", self.label_names[np.where(np.sum(predictions, axis=0) == 0)[0]])\n",
        "      \n",
        "      return(self.results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4lBxrvmlHuI",
        "colab_type": "code",
        "outputId": "2c8ff38d-bdbf-4954-de6d-9b0bbee46327",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "epochs = 25\n",
        "hidden_layer_size = 3000\n",
        "batch_size = 512\n",
        "learning_rate = 0.005\n",
        "dropout = 0.9\n",
        "sigmoid_thresh = 0.3\n",
        "weight_decay = 0.01\n",
        "num_maxout_units = 5\n",
        "max_norm = 20\n",
        "reg_param = 0.1\n",
        "\n",
        "hyperparameters = {'hidden_layer_size' : hidden_layer_size, 'epochs' : epochs, 'batch_size' : batch_size, 'learning_rate' : learning_rate, 'dropout' : dropout, 'scheduler_step_size' : 99999, \n",
        "                    'scheduler_lr_fraction' : 0.85, 'sigmoid_thresh' : sigmoid_thresh, 'num_maxout_units' : num_maxout_units, 'weight_decay' : weight_decay, 'max_norm' : max_norm, 'reg_param': reg_param}\n",
        "for key, value in hyperparameters.items():\n",
        "  print(key,\" : \",value)\n",
        "\n",
        "\n",
        "train_test = Training_Testing_MM( Data_train_tensor_text, Data_train_tensor_image, Labels_train_tensor, \n",
        "                                  Data_test_tensor_text, Data_test_tensor_image, Labels_test_tensor, \n",
        "                                  Data_val_tensor_text, Data_val_tensor_image, Labels_val_tensor, Label_names=Label_names, \n",
        "                                  hidden_layer_size = hidden_layer_size, epochs = epochs, batch_size= batch_size, learning_rate = learning_rate, dropout = dropout, scheduler_step_size = 99999, \n",
        "                                  scheduler_lr_fraction = 0.85, sigmoid_thresh = sigmoid_thresh, num_maxout_units = num_maxout_units, weight_decay = weight_decay, max_norm = max_norm, reg_param=reg_param)\n",
        "train_test.Train()\n",
        "train_test.Test()\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hidden_layer_size  :  3000\n",
            "epochs  :  25\n",
            "batch_size  :  512\n",
            "learning_rate  :  0.005\n",
            "dropout  :  0.9\n",
            "scheduler_step_size  :  99999\n",
            "scheduler_lr_fraction  :  0.85\n",
            "sigmoid_thresh  :  0.3\n",
            "num_maxout_units  :  5\n",
            "weight_decay  :  0.01\n",
            "max_norm  :  20\n",
            "reg_param  :  0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/25 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/pyro/primitives.py:406: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.\n",
            "  \"modules from `torch.nn.Module` instances.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  Batch :  30  , Time elapsed :  0:00:15\n",
            "\n",
            "Train loss for epoch:  0.6276457961648703\n",
            "\n",
            "Train loss for epoch (gmu):  12409496.34375\n",
            "\n",
            "Training epoch took: 0:00:16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   4%|▍         | 1/25 [00:16<06:34, 16.45s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Weighted F1 score for epoch:  0.22138217853168515 \n",
            "\n",
            "  Batch :  30  , Time elapsed :  0:00:15\n",
            "\n",
            "Train loss for epoch:  0.41445751208812\n",
            "\n",
            "Train loss for epoch (gmu):  9369641.46875\n",
            "\n",
            "Training epoch took: 0:00:16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   8%|▊         | 2/25 [00:32<06:14, 16.28s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Weighted F1 score for epoch:  0.15389149860734108 \n",
            "\n",
            "  Batch :  30  , Time elapsed :  0:00:15\n",
            "\n",
            "Train loss for epoch:  0.38584691006690264\n",
            "\n",
            "Train loss for epoch (gmu):  6952448.484375\n",
            "\n",
            "Training epoch took: 0:00:16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  12%|█▏        | 3/25 [00:48<05:55, 16.18s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Weighted F1 score for epoch:  0.29921819159326674 \n",
            "\n",
            "  Batch :  30  , Time elapsed :  0:00:15\n",
            "\n",
            "Train loss for epoch:  0.3044147836044431\n",
            "\n",
            "Train loss for epoch (gmu):  5054988.09375\n",
            "\n",
            "Training epoch took: 0:00:16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  16%|█▌        | 4/25 [01:04<05:39, 16.15s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Weighted F1 score for epoch:  0.2732893934223071 \n",
            "\n",
            "  Batch :  30  , Time elapsed :  0:00:15\n",
            "\n",
            "Train loss for epoch:  0.3011829750612378\n",
            "\n",
            "Train loss for epoch (gmu):  3578194.8984375\n",
            "\n",
            "Training epoch took: 0:00:16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  20%|██        | 5/25 [01:20<05:22, 16.12s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Weighted F1 score for epoch:  0.08669867681705372 \n",
            "\n",
            "  Batch :  30  , Time elapsed :  0:00:15\n",
            "\n",
            "Train loss for epoch:  0.30054611526429653\n",
            "\n",
            "Train loss for epoch (gmu):  2432126.49609375\n",
            "\n",
            "Training epoch took: 0:00:16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  24%|██▍       | 6/25 [01:36<05:06, 16.12s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Weighted F1 score for epoch:  0.1989168507975055 \n",
            "\n",
            "  Batch :  30  , Time elapsed :  0:00:15\n",
            "\n",
            "Train loss for epoch:  0.30091093946248293\n",
            "\n",
            "Train loss for epoch (gmu):  1542107.140625\n",
            "\n",
            "Training epoch took: 0:00:16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  28%|██▊       | 7/25 [01:52<04:50, 16.11s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Weighted F1 score for epoch:  0.3179725642997625 \n",
            "\n",
            "  Batch :  30  , Time elapsed :  0:00:15\n",
            "\n",
            "Train loss for epoch:  0.3003098964691162\n",
            "\n",
            "Train loss for epoch (gmu):  852050.400390625\n",
            "\n",
            "Training epoch took: 0:00:16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  32%|███▏      | 8/25 [02:08<04:33, 16.11s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Weighted F1 score for epoch:  0.07074502724422863 \n",
            "\n",
            "  Batch :  30  , Time elapsed :  0:00:15\n",
            "\n",
            "Train loss for epoch:  0.2993702758103609\n",
            "\n",
            "Train loss for epoch (gmu):  313154.291015625\n",
            "\n",
            "Training epoch took: 0:00:16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  36%|███▌      | 9/25 [02:24<04:17, 16.12s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Weighted F1 score for epoch:  0.21917759543086215 \n",
            "\n",
            "  Batch :  30  , Time elapsed :  0:00:15\n",
            "\n",
            "Train loss for epoch:  0.2987661622464657\n",
            "\n",
            "Train loss for epoch (gmu):  -110793.2265625\n",
            "\n",
            "Training epoch took: 0:00:16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  40%|████      | 10/25 [02:41<04:01, 16.13s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Weighted F1 score for epoch:  0.26462290774249964 \n",
            "\n",
            "  Batch :  30  , Time elapsed :  0:00:15\n",
            "\n",
            "Train loss for epoch:  0.2982207229360938\n",
            "\n",
            "Train loss for epoch (gmu):  -447741.875\n",
            "\n",
            "Training epoch took: 0:00:16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  44%|████▍     | 11/25 [02:57<03:46, 16.15s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Weighted F1 score for epoch:  0.08046921670585513 \n",
            "\n",
            "  Batch :  30  , Time elapsed :  0:00:15\n",
            "\n",
            "Train loss for epoch:  0.2990926615893841\n",
            "\n",
            "Train loss for epoch (gmu):  -718964.61328125\n",
            "\n",
            "Training epoch took: 0:00:16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  48%|████▊     | 12/25 [03:13<03:29, 16.13s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Weighted F1 score for epoch:  0.21072138118256586 \n",
            "\n",
            "  Batch :  30  , Time elapsed :  0:00:15\n",
            "\n",
            "Train loss for epoch:  0.29886571876704693\n",
            "\n",
            "Train loss for epoch (gmu):  -941570.3125\n",
            "\n",
            "Training epoch took: 0:00:16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  52%|█████▏    | 13/25 [03:29<03:13, 16.12s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Weighted F1 score for epoch:  0.03129317577678509 \n",
            "\n",
            "  Batch :  30  , Time elapsed :  0:00:15\n",
            "\n",
            "Train loss for epoch:  0.29981054179370403\n",
            "\n",
            "Train loss for epoch (gmu):  -1127729.78125\n",
            "\n",
            "Training epoch took: 0:00:16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  56%|█████▌    | 14/25 [03:45<02:57, 16.13s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Weighted F1 score for epoch:  0.2804581986495617 \n",
            "\n",
            "  Batch :  30  , Time elapsed :  0:00:15\n",
            "\n",
            "Train loss for epoch:  0.298987353220582\n",
            "\n",
            "Train loss for epoch (gmu):  -1286579.203125\n",
            "\n",
            "Training epoch took: 0:00:16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  60%|██████    | 15/25 [04:01<02:41, 16.12s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Weighted F1 score for epoch:  0.059774525348730095 \n",
            "\n",
            "  Batch :  30  , Time elapsed :  0:00:15\n",
            "\n",
            "Train loss for epoch:  0.29881633166223764\n",
            "\n",
            "Train loss for epoch (gmu):  -1425655.56640625\n",
            "\n",
            "Training epoch took: 0:00:16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  64%|██████▍   | 16/25 [04:17<02:24, 16.10s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Weighted F1 score for epoch:  0.24753462568483914 \n",
            "\n",
            "  Batch :  30  , Time elapsed :  0:00:15\n",
            "\n",
            "Train loss for epoch:  0.29760390520095825\n",
            "\n",
            "Train loss for epoch (gmu):  -1549968.58203125\n",
            "\n",
            "Training epoch took: 0:00:16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  68%|██████▊   | 17/25 [04:33<02:08, 16.10s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Weighted F1 score for epoch:  0.07064356016660218 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-360115a971c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m                                   \u001b[0mhidden_layer_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_layer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler_step_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m99999\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                                   scheduler_lr_fraction = 0.85, sigmoid_thresh = sigmoid_thresh, num_maxout_units = num_maxout_units, weight_decay = weight_decay, max_norm = max_norm, reg_param=reg_param)\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mtrain_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mtrain_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-860e287c38ec>\u001b[0m in \u001b[0;36mTrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    245\u001b[0m           \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m           \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m           \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}