{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PP-MM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rishi15-t/PP-MM/blob/master/PP_MM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZyaSvq7Poeg",
        "colab_type": "code",
        "outputId": "61f222e5-8975-4577-885f-fd2c8cde6eb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daviCRfrPsDK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "dataset = pd.read_pickle('/content/drive/My Drive/dataset/w2v_vgg_embeddings.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lq5KYZXPtiv",
        "colab_type": "code",
        "outputId": "2912c906-da70-4e39-cd68-35818442723b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def Train_Test_Val_Split(data , test_data_fraction = 0.3, val_data_fraction = 0.1) :\n",
        "    \n",
        "  \n",
        "    data_genres_one_hot_encoding = pd.DataFrame.from_items(zip(data['genres'].index, data['genres'].values)).T\n",
        "    Label_names = np.array(['Action', 'Adventure', 'Animation', 'Biography', 'Comedy', 'Crime',\n",
        "       'Documentary', 'Drama', 'Family', 'Fantasy', 'Film-Noir',\n",
        "       'History', 'Horror', 'Music', 'Musical', 'Mystery', 'Romance',\n",
        "       'Sci-Fi', 'Short', 'Sport', 'Thriller', 'War', 'Western'])\n",
        "    data_genres_one_hot_encoding.columns = Label_names\n",
        "    Data_train, Data_test, Labels_train, Labels_test = train_test_split(data, data_genres_one_hot_encoding, test_size = test_data_fraction)\n",
        "\n",
        "    Data_train, Data_val, Labels_train, Labels_val = train_test_split(Data_train, Labels_train, test_size = val_data_fraction)\n",
        "\n",
        "    Data_train = Data_train.reset_index(drop=True)\n",
        "    Data_test = Data_test.reset_index(drop=True)\n",
        "    Data_val = Data_val.reset_index(drop=True)\n",
        "    \n",
        "    Labels_train = torch.tensor(Labels_train.values)\n",
        "    Labels_test = torch.tensor(Labels_test.values)\n",
        "    Labels_val = torch.tensor(Labels_val.values)\n",
        "    \n",
        "    return (Data_train, Data_test, Data_val, Labels_train, Labels_test, Labels_val, Label_names)\n",
        "    \n",
        "Data_train, Data_test, Data_val, Labels_train_tensor, Labels_test_tensor, Labels_val_tensor, Label_names = Train_Test_Val_Split(dataset)\n",
        "\n",
        "Data_train_tensor_text = torch.tensor(Data_train['w2v_embeddings'])\n",
        "Data_test_tensor_text = torch.tensor(Data_test['w2v_embeddings'])\n",
        "Data_val_tensor_text = torch.tensor(Data_val['w2v_embeddings'])\n",
        "\n",
        "Data_train_tensor_image = torch.tensor(Data_train['vgg16_embeddings'])\n",
        "Data_test_tensor_image = torch.tensor(Data_test['vgg16_embeddings'])\n",
        "Data_val_tensor_image = torch.tensor(Data_val['vgg16_embeddings'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: from_items is deprecated. Please use DataFrame.from_dict(dict(items), ...) instead. DataFrame.from_dict(OrderedDict(items)) may be used to preserve the key order.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9oDVZe52y9s",
        "colab_type": "code",
        "outputId": "96f10865-6ef6-4a1b-8a51-65b362101e97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        }
      },
      "source": [
        "!pip install git+https://github.com/uber/pyro.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/uber/pyro.git\n",
            "  Cloning https://github.com/uber/pyro.git to /tmp/pip-req-build-vmlvmstc\n",
            "  Running command git clone -q https://github.com/uber/pyro.git /tmp/pip-req-build-vmlvmstc\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl==1.2.1+74110869) (1.17.5)\n",
            "Requirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl==1.2.1+74110869) (3.1.0)\n",
            "Collecting pyro-api>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/c2/bc/6cdbd1929e32fff62a33592633c2cc0393c7f7739131ccc9c9c4e28ac8dd/pyro_api-0.1.1-py3-none-any.whl\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl==1.2.1+74110869) (1.4.0)\n",
            "Collecting tqdm>=4.36\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/80/5bb262050dd2f30f8819626b7c92339708fe2ed7bd5554c8193b4487b367/tqdm-4.42.1-py2.py3-none-any.whl (59kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyro-ppl\n",
            "  Building wheel for pyro-ppl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyro-ppl: filename=pyro_ppl-1.2.1+74110869-cp36-none-any.whl size=476778 sha256=5d4c203ed201b4520dd5b7e388ae3e7d6798af1a2f7f2825aecaecf5a49859ff\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1dogowyd/wheels/3c/10/ed/73d2332a097e2e9dc7d601ca2a99b0b4f9634e393474b78137\n",
            "Successfully built pyro-ppl\n",
            "Installing collected packages: pyro-api, tqdm, pyro-ppl\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed pyro-api-0.1.1 pyro-ppl-1.2.1+74110869 tqdm-4.42.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDL0pDxw0Zsw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import IterableDataset, Dataset, DataLoader\n",
        "from torch import autograd, nn, tanh, optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "import h5py\n",
        "import numpy as np \n",
        "from pathlib import Path\n",
        "import pyro\n",
        "from pyro.distributions import Normal, Categorical, Laplace\n",
        "from pyro.infer import SVI, Trace_ELBO\n",
        "from pyro.optim import Adam\n",
        "\n",
        "\n",
        "# model\n",
        "\n",
        "class GMU_PP(nn.Module):\n",
        "\n",
        "    def __init__(self, num_maxout_units = 2, hidden_layer_size = 512, text_embeddings_size = 300, img_embeddings_size = 4096, num_labels = 23, hidden_activation = None, dropout = 0.1):\n",
        "\n",
        "        super(GMU_PP, self).__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.hidden_layer_size = hidden_layer_size\n",
        "        self.text_embeddings_size = text_embeddings_size\n",
        "        self.img_embeddings_size = img_embeddings_size\n",
        "\n",
        "        self.linear_h_text = torch.nn.Linear(text_embeddings_size, self.hidden_layer_size, bias = False)\n",
        "        self.linear_h_image = torch.nn.Linear(img_embeddings_size, self.hidden_layer_size, bias = False)\n",
        "        self.linear_z = torch.nn.Linear(text_embeddings_size + img_embeddings_size, self.hidden_layer_size, bias = False)\n",
        "\n",
        "        self.tanh = torch.nn.Tanh()\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.linear = torch.nn.Linear(self.hidden_layer_size, self.num_labels)\n",
        "        \n",
        "        #self.maxout = Maxout_MLP(self.hidden_layer_size, self.hidden_layer_size, dropout, num_maxout_units=num_maxout_units)\n",
        "\n",
        "        #Added one hidden layer in between GMU units and output layer for testing\n",
        "        self.linear_test = torch.nn.Linear(self.hidden_layer_size, self.hidden_layer_size)\n",
        "        self.relu_test = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, image_embeddings, text_embeddings):\n",
        "        \n",
        "        image_h = self.linear_h_image(image_embeddings)\n",
        "        image_h = self.tanh(image_h)\n",
        "        text_h = self.linear_h_text(text_embeddings)\n",
        "        text_h = self.tanh(text_h)\n",
        "        concat = torch.cat((image_embeddings, text_embeddings), 1)\n",
        "        z = self.linear_z(concat)\n",
        "        z = self.sigmoid(z)\n",
        "        gmu_output = z*image_h + (1-z)*text_h\n",
        "        \n",
        "        #maxout_mlp_output = self.maxout(gmu_output)\n",
        "\n",
        "        test = self.linear_test(gmu_output)\n",
        "        test = self.relu_test(test)\n",
        "        logits = self.linear(test)\n",
        "        \n",
        "        #logits = self.linear(maxout_mlp_output)\n",
        "        \n",
        "        if(self.training) :\n",
        "            return logits\n",
        "        else :\n",
        "            output = self.sigmoid(logits)\n",
        "            return output\n",
        "\n",
        "    '''\n",
        "    # pyro model\n",
        "    def model(self, image_embeddings, text_embeddings, labels):\n",
        "\n",
        "        \n",
        "        def model_dist(*shape):\n",
        "            loc = torch.zeros(*shape).cuda()\n",
        "            scale = torch.ones(*shape).cuda()\n",
        "            return Laplace(loc, scale)\n",
        "        \n",
        "        priors = {\n",
        "            'linear_h_text.weight': model_dist(self.hidden_layer_size, self.text_embeddings_size), #no bias\n",
        "            'linear_h_image.weight': model_dist(self.hidden_layer_size, self.img_embeddings_size), #no bias\n",
        "            'linear_z.weight': model_dist(self.hidden_layer_size, self.text_embeddings_size + self.img_embeddings_size), #no bias\n",
        "            'linear_test.weight': model_dist(self.hidden_layer_size, self.hidden_layer_size), 'linear_test.bias': model_dist(self.hidden_layer_size),\n",
        "            'linear.weight': model_dist(self.num_labels, self.hidden_layer_size), 'linear.bias': model_dist(self.num_labels)\n",
        "            }\n",
        "        \n",
        "        lifted_module = pyro.random_module(\"net\", net, priors)\n",
        "        lifted_reg_model = lifted_module().cuda()\n",
        "        lhat = lifted_reg_model(image_embeddings, text_embeddings)\n",
        "        pyro.sample(\"obs\", Categorical(logits=lhat), obs=labels)\n",
        "\n",
        "    # pyro guide\n",
        "    def guide(self, image_embeddings, text_embeddings, labels):\n",
        "\n",
        "        def infer_dist(name, *shape):\n",
        "            l = torch.empty(*shape, requires_grad=True).cuda()\n",
        "            s = torch.empty(*shape, requires_grad=True).cuda()\n",
        "            torch.nn.init.normal_(l, std=0.01)\n",
        "            torch.nn.init.normal_(s, std=0.01)\n",
        "            loc = pyro.param(name+\"_loc\", l)\n",
        "            scale = nn.functional.softplus(pyro.param(name+\"_scale\", s))\n",
        "            return Laplace(loc, scale)\n",
        "\n",
        "        dists = {\n",
        "            'linear_h_text.weight': infer_dist(\"W1\", self.hidden_layer_size, self.text_embeddings_size), #no bias\n",
        "            'linear_h_image.weight': infer_dist(\"W2\", self.hidden_layer_size, self.img_embeddings_size), #no bias\n",
        "            'linear_z.weight': infer_dist(\"W3\", self.hidden_layer_size, self.text_embeddings_size + self.img_embeddings_size), #no bias\n",
        "            'linear_test.weight': infer_dist(\"W4\", self.hidden_layer_size, self.hidden_layer_size), 'linear_test.bias': infer_dist(\"b4\", self.hidden_layer_size),\n",
        "            'linear.weight': infer_dist(\"W5\", self.num_labels, self.hidden_layer_size), 'linear.bias': infer_dist(\"b5\", self.num_labels)\n",
        "            }\n",
        "\n",
        "        lifted_module = pyro.random_module(\"net\", net, dists)\n",
        "        return lifted_module()\n",
        "\n",
        "    '''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiD7OYvGJOwm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(image_embeddings, text_embeddings, labels):\n",
        "\n",
        "    linear_h_text_w_prior = Laplace(loc=torch.zeros_like(net.linear_h_text.weight), scale=torch.ones_like(net.linear_h_text.weight))\n",
        "    linear_h_image_w_prior = Laplace(loc=torch.zeros_like(net.linear_h_image.weight), scale=torch.ones_like(net.linear_h_image.weight))\n",
        "    linear_z_w_prior = Laplace(loc=torch.zeros_like(net.linear_z.weight), scale=torch.ones_like(net.linear_z.weight))\n",
        "\n",
        "    linear_test_w_prior = Laplace(loc=torch.zeros_like(net.linear_test.weight), scale=torch.ones_like(net.linear_test.weight))\n",
        "    linear_test_b_prior = Laplace(loc=torch.zeros_like(net.linear_test.bias), scale=torch.ones_like(net.linear_test.bias))\n",
        "\n",
        "    linear_w_prior = Laplace(loc=torch.zeros_like(net.linear.weight), scale=torch.ones_like(net.linear.weight))\n",
        "    linear_b_prior = Laplace(loc=torch.zeros_like(net.linear.bias), scale=torch.ones_like(net.linear.bias))\n",
        "\n",
        "    priors = {\n",
        "        'linear_h_text.weight': linear_h_text_w_prior, #no bias\n",
        "        'linear_h_image.weight': linear_h_image_w_prior, #no bias\n",
        "        'linear_z.weight': linear_z_w_prior, #no bias\n",
        "        'linear_test.weight': linear_test_w_prior, \n",
        "        'linear_test.bias': linear_test_b_prior,\n",
        "        'linear.weight': linear_w_prior, \n",
        "        'linear.bias': linear_b_prior\n",
        "    }  \n",
        "\n",
        "    lifted_module = pyro.random_module(\"net\", net, priors)\n",
        "    lifted_reg_model = lifted_module().cuda()\n",
        "    lhat = lifted_reg_model(image_embeddings, text_embeddings)\n",
        "    pyro.sample(\"obs\", Categorical(logits=lhat), obs=labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkcZwJ7fJjV7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " softplus = torch.nn.Softplus()\n",
        " \n",
        " def guide(image_embeddings, text_embeddings, labels):\n",
        "\n",
        "    # linear_h_text weight\n",
        "    linear_h_text_w_mu = torch.randn_like(net.linear_h_text.weight)\n",
        "    linear_h_text_w_sigma = torch.randn_like(net.linear_h_text.weight)\n",
        "    linear_h_text_w_mu_param = pyro.param(\"linear_h_text_w_mu\", linear_h_text_w_mu)\n",
        "    linear_h_text_w_sigma_param = softplus(pyro.param(\"linear_h_text_w_sigma\", linear_h_text_w_sigma))\n",
        "    linear_h_text_w_dist = Laplace(loc = linear_h_text_w_mu_param, scale = linear_h_text_w_sigma_param)\n",
        "\n",
        "\n",
        "    # linear_h_image weight\n",
        "    linear_h_image_w_mu = torch.randn_like(net.linear_h_image.weight)\n",
        "    linear_h_image_w_sigma = torch.randn_like(net.linear_h_image.weight)\n",
        "    linear_h_image_w_mu_param = pyro.param(\"linear_h_image_w_mu\", linear_h_image_w_mu)\n",
        "    linear_h_image_w_sigma_param = softplus(pyro.param(\"linear_h_image_w_sigma\", linear_h_image_w_sigma))\n",
        "    linear_h_image_w_dist = Laplace(loc = linear_h_image_w_mu_param, scale = linear_h_image_w_sigma_param)\n",
        "\n",
        "\n",
        "    # linear_z weight\n",
        "    linear_z_w_mu = torch.randn_like(net.linear_z.weight)\n",
        "    linear_z_w_sigma = torch.randn_like(net.linear_z.weight)\n",
        "    linear_z_w_mu_param = pyro.param(\"linear_z_w_mu\", linear_z_w_mu)\n",
        "    linear_z_w_sigma_param = softplus(pyro.param(\"linear_z_w_sigma\", linear_z_w_sigma))\n",
        "    linear_z_w_dist = Laplace(loc = linear_z_w_mu_param, scale = linear_z_w_sigma_param)\n",
        "\n",
        "\n",
        "    # linear_test weight\n",
        "    linear_test_w_mu = torch.randn_like(net.linear_test.weight)\n",
        "    linear_test_w_sigma = torch.randn_like(net.linear_test.weight)\n",
        "    linear_test_w_mu_param = pyro.param(\"linear_test_w_mu\", linear_test_w_mu)\n",
        "    linear_test_w_sigma_param = softplus(pyro.param(\"linear_test_w_sigma\", linear_test_w_sigma))\n",
        "    linear_test_w_dist = Laplace(loc = linear_test_w_mu_param, scale = linear_test_w_sigma_param)\n",
        "\n",
        "    \n",
        "    # linear_test bias\n",
        "    linear_test_b_mu = torch.randn_like(net.linear_test.bias)\n",
        "    linear_test_b_sigma = torch.randn_like(net.linear_test.bias)\n",
        "    linear_test_b_mu_param = pyro.param(\"linear_test_b_mu\", linear_test_b_mu)\n",
        "    linear_test_b_sigma_param = softplus(pyro.param(\"linear_test_b_sigma\", linear_test_b_sigma))\n",
        "    linear_test_b_dist = Laplace(loc = linear_test_b_mu_param, scale = linear_test_b_sigma_param)\n",
        "\n",
        "\n",
        "    # linear weight\n",
        "    linear_w_mu = torch.randn_like(net.linear.weight)\n",
        "    linear_w_sigma = torch.randn_like(net.linear.weight)\n",
        "    linear_w_mu_param = pyro.param(\"linear_w_mu\", linear_w_mu)\n",
        "    linear_w_sigma_param = softplus(pyro.param(\"linear_w_sigma\", linear_w_sigma))\n",
        "    linear_w_dist = Laplace(loc = linear_w_mu_param, scale = linear_w_sigma_param)\n",
        "\n",
        "    \n",
        "    # linear bias\n",
        "    linear_b_mu = torch.randn_like(net.linear.bias)\n",
        "    linear_b_sigma = torch.randn_like(net.linear.bias)\n",
        "    linear_b_mu_param = pyro.param(\"linear_b_mu\", linear_b_mu)\n",
        "    linear_b_sigma_param = softplus(pyro.param(\"linear_b_sigma\", linear_b_sigma))\n",
        "    linear_b_dist = Laplace(loc = linear_b_mu_param, scale = linear_b_sigma_param)\n",
        "\n",
        "\n",
        "    dists = {\n",
        "        'linear_h_text.weight': linear_h_text_w_dist, #no bias\n",
        "        'linear_h_image.weight': linear_h_image_w_dist, #no bias\n",
        "        'linear_z.weight': linear_z_w_dist, #no bias\n",
        "        'linear_test.weight': linear_test_w_dist, \n",
        "        'linear_test.bias': linear_test_b_dist,\n",
        "        'linear.weight': linear_w_dist, \n",
        "        'linear.bias': linear_b_dist\n",
        "    } \n",
        "    \n",
        "    lifted_module = pyro.random_module(\"net\", net, dists)\n",
        "    return lifted_module()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXrSeLOoxkZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from tqdm import tqdm, trange\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "\n",
        "def SetTrainDataloader_MM(Data_train_tensor_text, Data_train_tensor_image, Labels_train_tensor) :\n",
        "\n",
        "  train_dataset = TensorDataset(Data_train_tensor_text, Data_train_tensor_image, Labels_train_tensor)\n",
        "  train_sampler = RandomSampler(train_dataset)\n",
        "  train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size =  batch_size)\n",
        "  return(train_dataloader)\n",
        "\n",
        "\n",
        "#source: https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "def Plot_Training_Epoch_Loss(epoch_loss_set) :\n",
        "\n",
        "  sns.set(style='darkgrid')\n",
        "  sns.set(font_scale=1.5)\n",
        "  plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "  plt.plot( epoch_loss_set, 'b-o')\n",
        "  plt.title(\"Training loss\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.savefig('Training_Epoch_Loss.png',bbox_inches='tight')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "#source: https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "def format_time(elapsed):\n",
        "  '''\n",
        "  Takes a time in seconds and returns a string hh:mm:ss\n",
        "  '''\n",
        "  # Round to the nearest second.\n",
        "  elapsed_rounded = int(round((elapsed)))\n",
        "  return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "\n",
        "def Train(net, train_dataloader, device, inference, learning_rate, epoch_loss_set) :\n",
        "  \n",
        "  net.cuda()\n",
        "  for _ in trange( epochs, desc=\"Epoch\"):\n",
        "    \n",
        "    net.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    for step_num, batch_data in enumerate(train_dataloader):\n",
        "\n",
        "      # Progress update every 30 batches.\n",
        "      if step_num % 30 == 0 and not step_num == 0:\n",
        "        elapsed =  format_time(time.time() - t0)\n",
        "        print('  Batch : ',step_num, ' , Time elapsed : ',elapsed)\n",
        "\n",
        "      samples_image, samples_text, labels = tuple(t.to(device) for t in batch_data)\n",
        "      '''\n",
        "      # define optimizer and loss function\n",
        "      optimizer = Adam({\"lr\": learning_rate})\n",
        "      loss_fn = pyro.infer.Trace_ELBO()\n",
        "\n",
        "      # compute loss\n",
        "      batch_loss = loss_fn(net.model, net.guide, samples_image.float(), samples_text.float(), labels)\n",
        "      batch_loss.backward()\n",
        "      \n",
        "      # take a step and zero the parameter gradients\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      '''\n",
        "      batch_loss = inference.step(samples_image.float(), samples_text.float(), labels)\n",
        "      epoch_loss += batch_loss\n",
        "\n",
        "    avg_epoch_loss = epoch_loss/len( train_dataloader)\n",
        "    print(\"\\nTrain loss for epoch: \",avg_epoch_loss)\n",
        "    print(\"\\nTraining epoch took: {:}\".format( format_time(time.time() - t0)))\n",
        "    epoch_loss_set.append(avg_epoch_loss)\n",
        "\n",
        "  #torch.save( model.state_dict(), \"/content/drive/My Drive/dataset/model.pt\")\n",
        "  Plot_Training_Epoch_Loss(epoch_loss_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj8rNWGO_-hW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Parameters\n",
        "\n",
        "net = GMU_PP(num_maxout_units = 2, hidden_layer_size = 512, hidden_activation = None, dropout = 0.5).cuda()\n",
        "label_names = Label_names\n",
        "num_labels = 23\n",
        "batch_size = 512\n",
        "learning_rate = 0.01\n",
        "epochs = 125\n",
        "sigmoid_thresh = 0.5\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "results = pd.DataFrame(0, index=['Recall','Precision','F_Score'], columns=['micro', 'macro', 'weighted', 'samples']).astype(float)\n",
        "epoch_loss_set = []\n",
        "train_dataloader =  SetTrainDataloader_MM(Data_train_tensor_image, Data_train_tensor_text, Labels_train_tensor[:,0])\n",
        "\n",
        "# clear param store\n",
        "pyro.clear_param_store()\n",
        "\n",
        "# pyro svi, initialise losses\n",
        "inference = SVI(model, guide, Adam({\"lr\": learning_rate}), loss=Trace_ELBO())\n",
        "#inference = SVI(net.model, net.guide, Adam({\"lr\": learning_rate}), loss=Trace_ELBO())\n",
        "\n",
        "Train(net, train_dataloader, device, inference, learning_rate, epoch_loss_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmTVSATyHvjN",
        "colab_type": "code",
        "outputId": "6b1569b6-2c0f-417e-d87e-c897346dacbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "net = GMU_PP(num_maxout_units = 2, hidden_layer_size = 512, hidden_activation = None, dropout = 0.5).cuda()\n",
        "label_names = Label_names\n",
        "num_labels = 23\n",
        "batch_size = 512\n",
        "learning_rate = 0.01\n",
        "epochs = 125\n",
        "sigmoid_thresh = 0.5\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "results = pd.DataFrame(0, index=['Recall','Precision','F_Score'], columns=['micro', 'macro', 'weighted', 'samples']).astype(float)\n",
        "epoch_loss_set = []\n",
        "train_dataloader =  SetTrainDataloader_MM(Data_train_tensor_image, Data_train_tensor_text, Labels_train_tensor)\n",
        "\n",
        "# clear param store\n",
        "pyro.clear_param_store()\n",
        "\n",
        "# pyro svi, initialise losses\n",
        "inference = SVI(model, guide, Adam({\"lr\": learning_rate}), loss=Trace_ELBO())\n",
        "#inference = SVI(net.model, net.guide, Adam({\"lr\": learning_rate}), loss=Trace_ELBO())\n",
        "\n",
        "Train(net, train_dataloader, device, inference, learning_rate, epoch_loss_set)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/125 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/pyro/primitives.py:371: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.\n",
            "  \"modules from `torch.nn.Module` instances.\", FutureWarning)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-be9f4ca302ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#inference = SVI(net.model, net.guide, Adam({\"lr\": learning_rate}), loss=Trace_ELBO())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_loss_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-28b3f82529bf>\u001b[0m in \u001b[0;36mTrain\u001b[0;34m(net, train_dataloader, device, inference, learning_rate, epoch_loss_set)\u001b[0m\n\u001b[1;32m     78\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m       '''\n\u001b[0;32m---> 80\u001b[0;31m       \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m       \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyro/infer/svi.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# get loss and compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mparam_capture\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         params = set(site[\"value\"].unconstrained()\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyro/infer/trace_elbo.py\u001b[0m in \u001b[0;36mloss_and_grads\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# grab a trace from the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmodel_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide_trace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_traces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m             \u001b[0mloss_particle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msurrogate_loss_particle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_differentiable_loss_particle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_particle\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_particles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyro/infer/elbo.py\u001b[0m in \u001b[0;36m_get_traces\u001b[0;34m(self, model, guide, args, kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_particles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyro/infer/trace_elbo.py\u001b[0m in \u001b[0;36m_get_trace\u001b[0;34m(self, model, guide, args, kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \"\"\"\n\u001b[1;32m     52\u001b[0m         model_trace, guide_trace = get_importance_trace(\n\u001b[0;32m---> 53\u001b[0;31m             \"flat\", self.max_plate_nesting, model, guide, args, kwargs)\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_validation_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mcheck_if_enumerated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguide_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyro/infer/enum.py\u001b[0m in \u001b[0;36mget_importance_trace\u001b[0;34m(graph_type, max_plate_nesting, model, guide, args, kwargs, detach)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mmodel_trace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprune_subsample_sites\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mmodel_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mguide_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_score_parts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_validation_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyro/poutine/trace_struct.py\u001b[0m in \u001b[0;36mcompute_log_prob\u001b[0;34m(self, site_filter)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"log_prob\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                         \u001b[0mlog_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fn\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"kwargs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyro/distributions/torch.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0menumerate_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_pmf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlog_pmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 [0, 1, 2]])\n\u001b[1;32m     51\u001b[0m     \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (23) must match the size of tensor b (512) at non-singleton dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFXjjVatECEB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_samples = 10\n",
        "\n",
        "def predict(vgg_input, w2v_input):\n",
        "  \n",
        "    sampled_models = [guide(None, None, None) for _ in range(num_samples)]\n",
        "    yhats = [model(vgg_input, w2v_input).data for model in sampled_models]\n",
        "    mean = torch.mean(torch.stack(yhats), 0).cpu()\n",
        "    predictions = np.argmax(mean.numpy(), axis=1)\n",
        "\n",
        "    yhats_uncertainity = [F.log_softmax(model(vgg_input, w2v_input).data, 1).cpu().numpy() for model in sampled_models]\n",
        "    uncertainity = np.asarray(yhats_uncertainity)\n",
        "\n",
        "    return (predictions, uncertainity)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9YJOrgyVEwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vgg_input = torch.tensor(dataset['vgg16_embeddings'][0:2]).cuda()\n",
        "w2v_input = torch.tensor(dataset['w2v_embeddings'][0:2]).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}